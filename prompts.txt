You said:
zwt@zwt-MS-7D86:~/CS598-DL4H-Final-Project/mmugl-main$ conda env create -f conda_linux_x86_environment.yml
Retrieving notices: ...working... done
Collecting package metadata (repodata.json): - WARNING conda.models.version:get_matcher(548): Using .* with relational operator is superfluous and deprecated and will be removed in a future version of conda. Your spec was 1.8.0.*, but conda is ignoring the .* and treating it as 1.8.0
WARNING conda.models.version:get_matcher(548): Using .* with relational operator is superfluous and deprecated and will be removed in a future version of conda. Your spec was 1.9.0.*, but conda is ignoring the .* and treating it as 1.9.0
WARNING conda.models.version:get_matcher(548): Using .* with relational operator is superfluous and deprecated and will be removed in a future version of conda. Your spec was 1.6.0.*, but conda is ignoring the .* and treating it as 1.6.0
WARNING conda.models.version:get_matcher(548): Using .* with relational operator is superfluous and deprecated and will be removed in a future version of conda. Your spec was 1.7.1.*, but conda is ignoring the .* and treating it as 1.7.1
done
Solving environment: failed

ResolvePackageNotFound: 
  - torchvision==0.2.2=py_3


Trying to setup on my ubuntu machine to replicate the paper Multi-Modal Graph Learning over
UMLS Knowledge Graphs
ChatGPT said:
You're trying to replicate a project using an old conda environment that includes torchvision==0.2.2=py_3, which is no longer available in current conda channels (or may never have been available under that specific build tag).

Here's how to resolve the ResolvePackageNotFound: torchvision==0.2.2=py_3 issue:

✅ Step-by-Step Fix
1. Edit conda_linux_x86_environment.yml
Open the file and locate the line:

  - torchvision==0.2.2=py_3
Replace it with a more installable version. If you want compatibility with older torch versions like 1.0, use:

  - torchvision==0.2.2
Or better, remove it entirely from the dependencies section and install it manually later.

2. Install Manually After Environment Creation
After removing that line:

conda env create -f conda_linux_x86_environment.yml
Then manually install a compatible PyTorch and torchvision combo, for example:

conda activate <env-name>
conda install pytorch==1.0.0 torchvision==0.2.2 -c pytorch
Replace <env-name> with the environment name from the YAML.

🔎 Optional: Use mamba (Faster)
Install mamba:

conda install mamba -n base -c conda-forge
Then retry:

mamba env create -f conda_linux_x86_environment.yml
Would you like help modifying the YAML file directly or checking other potentially deprecated packages in it?

You said:
zwt@zwt-MS-7D86:~/CS598-DL4H-Final-Project/mmugl-main$ conda env create -f conda_linux_x86_environment.yml
Collecting package metadata (repodata.json): | WARNING conda.models.version:get_matcher(548): Using .* with relational operator is superfluous and deprecated and will be removed in a future version of conda. Your spec was 1.7.1.*, but conda is ignoring the .* and treating it as 1.7.1
WARNING conda.models.version:get_matcher(548): Using .* with relational operator is superfluous and deprecated and will be removed in a future version of conda. Your spec was 1.9.0.*, but conda is ignoring the .* and treating it as 1.9.0
WARNING conda.models.version:get_matcher(548): Using .* with relational operator is superfluous and deprecated and will be removed in a future version of conda. Your spec was 1.6.0.*, but conda is ignoring the .* and treating it as 1.6.0
WARNING conda.models.version:get_matcher(548): Using .* with relational operator is superfluous and deprecated and will be removed in a future version of conda. Your spec was 1.8.0.*, but conda is ignoring the .* and treating it as 1.8.0
done
Solving environment: failed

ResolvePackageNotFound: 
  - torchvision=0.2.2
ChatGPT said:
The new error confirms that torchvision=0.2.2 itself—not just the py_3 build—is no longer available in public conda channels (likely because it's tied to the deprecated PyTorch 1.0 era).

✅ Fix Options for torchvision=0.2.2 Not Found
Option 1: Use a Compatible Version of PyTorch & TorchVision
You'll likely want to install:

conda install pytorch=1.0.0 torchvision=0.2.1 cpuonly -c pytorch
Or use:

conda install pytorch=1.0.0 torchvision=0.2.2 -c pytorch
…but do this after removing torchvision=0.2.2 from the conda_linux_x86_environment.yml and successfully creating the environment.

Option 2: Fix the .yml and Recreate Environment
Open conda_linux_x86_environment.yml in an editor.

Remove or comment out this line:

  - torchvision=0.2.2
Re-run:

conda env create -f conda_linux_x86_environment.yml
After it’s created:

conda activate <env-name>
conda install pytorch=1.0.0 torchvision=0.2.2 -c pytorch
If you want GPU support, use:

conda install pytorch=1.0.0 torchvision=0.2.2 cudatoolkit=10.0 -c pytorch
Would you like me to help sanitize the full .yml file for modern compatibility or locate an alternative conda or pip package source?

You said:
zwt@zwt-MS-7D86:~/CS598-DL4H-Final-Project/mmugl-main$ conda env create -f conda_linux_x86_environment.yml
Collecting package metadata (repodata.json): / WARNING conda.models.version:get_matcher(548): Using .* with relational operator is superfluous and deprecated and will be removed in a future version of conda. Your spec was 1.9.0.*, but conda is ignoring the .* and treating it as 1.9.0
WARNING conda.models.version:get_matcher(548): Using .* with relational operator is superfluous and deprecated and will be removed in a future version of conda. Your spec was 1.8.0.*, but conda is ignoring the .* and treating it as 1.8.0
WARNING conda.models.version:get_matcher(548): Using .* with relational operator is superfluous and deprecated and will be removed in a future version of conda. Your spec was 1.7.1.*, but conda is ignoring the .* and treating it as 1.7.1
WARNING conda.models.version:get_matcher(548): Using .* with relational operator is superfluous and deprecated and will be removed in a future version of conda. Your spec was 1.6.0.*, but conda is ignoring the .* and treating it as 1.6.0
done
Solving environment: done


==> WARNING: A newer version of conda exists. <==
  current version: 23.5.2
  latest version: 25.3.1

Please update conda by running

    $ conda update -n base -c defaults conda

Or to minimize the number of packages updated during conda update use

     conda install conda=25.3.1



Downloading and Extracting Packages
bokeh-2.4.3          | 7.7 MB    | ###################################################################### | 100%^[[B^[[B
dask-core-2022.2.1   | 747 KB    | ###################################################################### | 100% 
colorama-0.4.5       | 27 KB     | ###################################################################### | 100% 
cython-blis-0.7.7    | 2.0 MB    | ###################################################################### | 100% 
tensorboard-2.9.0    | 5.5 MB    | ###################################################################### | 100% 
soupsieve-2.3.1      | 34 KB     | ###################################################################### | 100% 
hdf5-1.12.1          | 4.1 MB    | ###################################################################### | 100% 
prometheus_client-0. | 90 KB     | ###################################################################### | 100% 
fdasrsf-2.3.12       | 467 KB    | ###################################################################### | 100% 
libffi-3.3           | 50 KB     | ###################################################################### | 100% 
glog-0.6.0           | 112 KB    | ###################################################################### | 100% 
send2trash-1.8.0     | 19 KB     | ###################################################################### | 100% 
scikit-image-0.19.2  | 9.9 MB    | ###################################################################### | 100% 
holoviews-1.15.0     | 4.0 MB    | ###################################################################### | 100% 
libevent-2.1.10      | 1.1 MB    | ###################################################################### | 100% 
imagecodecs-2021.8.2 | 7.2 MB    | ###################################################################### | 100% 
imageio-2.19.3       | 3.2 MB    | ###################################################################### | 100% 
graphviz-2.50.0      | 2.5 MB    | ###################################################################### | 100% 
tsnecuda-3.0.1       | 3.7 MB    | ###################################################################### | 100% 
packaging-21.3       | 36 KB     | ###################################################################### | 100% 
zlib-1.2.12          | 106 KB    | ###################################################################### | 100% 
numpy-1.21.5         | 10 KB     | ###################################################################### | 100% 
websocket-client-0.5 | 66 KB     | ###################################################################### | 100% 
typing-extensions-4. | 9 KB      | ###################################################################### | 100% 
xarray-0.20.1        | 563 KB    | ###################################################################### | 100% 
pyarrow-8.0.0        | 2.6 MB    | ###################################################################### | 100% 
libbrotlicommon-1.0. | 70 KB     | ###################################################################### | 100% 
openssl-1.1.1s       | 1.9 MB    | ###################################################################### | 100% 
ninja-1.10.2         | 8 KB      | ###################################################################### | 100% 
ipykernel-6.9.1      | 199 KB    | ###################################################################### | 100% 
pure_eval-0.2.2      | 14 KB     | ###################################################################### | 100% 
 ... (more hidden) ...




ChatGPT said:
Great — that log means your conda env create command succeeded in solving the environment and it's now successfully downloading and extracting packages.

So far, everything looks good. You're past the critical package resolution errors like ResolvePackageNotFound, and it's now just building the environment.

✅ Next Steps
Once installation completes:

Activate the environment:

conda activate <env-name>
Replace <env-name> with the name defined in the .yml file (usually at the top under name:).

Check/install missing PyTorch packages:
If you removed torchvision from the YAML, install manually:

conda install pytorch=1.0.0 torchvision=0.2.2 -c pytorch
Run your project/test the setup:
Try running any provided training or inference script to verify the environment is working as expected.

Would you like help with verifying your installation by running a script from the repo or making sure CUDA is available for PyTorch?

You said:
It was hanging after that
ChatGPT said:
If the environment creation hangs during the "Downloading and Extracting Packages" phase, here are a few practical things to do:

🛠️ 1. Check Disk Space
Run:

df -h
If your root partition (/) or conda environment location (typically under ~/anaconda3 or ~/.conda) is low on space, Conda can silently hang.

🛠️ 2. Use mamba for Faster, Reliable Installs
If you haven’t already:

conda install -n base -c conda-forge mamba
Then retry with:

mamba env create -f conda_linux_x86_environment.yml
This avoids slow dependency resolution and download issues.

🛠️ 3. Re-run with Verbosity
Sometimes it's not really "hanging" but just downloading slowly. To confirm what's going on:

conda config --set verbosity 3
conda env create -f conda_linux_x86_environment.yml
🛠️ 4. Retry From Scratch (Optional)
If it’s still stuck:

conda env remove -n <env-name>
conda clean --all
conda env create -f conda_linux_x86_environment.yml
Add --debug to see where it's failing.

Would you like me to help convert the .yml to a minimal pip+conda install script instead to avoid all the locking/dependency bloat?

You said:
Transaction finished


To activate this environment, use:

    mamba activate mmugl

Or to execute a single command in this environment, use:

    mamba run -n mmugl mycommand

warning  libmamba You are using 'pip' as an additional package manager.
    Be aware that packages installed with 'pip' are managed independently from 'conda-forge' channel.

Installing pip packages: blis==0.7.8, catalogue==2.0.8, cmake==3.24.0, coloredlogs==15.0.1, dill==0.3.5.1, gin-config==0.5.0, h5py==3.1.0, humanfriendly==10.0, iniconfig==1.1.1, leveldb==0.201, multiprocess==0.70.13, negspacy==1.0.3, nltk==3.7, nopdb==0.2.0, pathos==0.2.7, pathy==0.6.2, pluggy==0.13.1, pox==0.3.1, ppft==1.7.6.5, py==1.11.0, pydantic==1.9.2, pytest==6.2.3, quickumls==1.4.0.post1, quickumls-simstring==1.1.5.post1, spacy==3.4.1, spacy-loggers==1.0.3, srsly==2.4.4, thinc==8.1.0, torchinfo==1.7.1, typer==0.4.2, unidecode==1.3.4, unqlite==0.9.3, wasabi==0.10.1
zwt@zwt-MS-7D86:~/CS598-DL4H-Final-Project/mmugl-main$ mamba activate mmugl

'mamba' is running as a subprocess and can't modify the parent shell.
Thus you must initialize your shell before using activate and deactivate.

To initialize the current bash shell, run:
    $ eval "$(mamba shell hook --shell bash)"
and then activate or deactivate with:
    $ mamba activate
To automatically initialize all future (bash) shells, run:
    $ mamba shell init --shell bash --root-prefix=~/.local/share/mamba
If your shell was already initialized, reinitialize your shell with:
    $ mamba shell reinit --shell bash
Otherwise, this may be an issue. In the meantime you can run commands. See:
    $ mamba run --help

Supported shells are {bash, zsh, csh, posix, xonsh, cmd.exe, powershell, fish, nu}.
critical libmamba Shell not initialized
zwt@zwt-MS-7D86:~/CS598-DL4H-Final-Project/mmugl-main$ 
ChatGPT said:
You're almost done — the environment was successfully created, but mamba activate mmugl didn't work because your shell isn't initialized for mamba.

✅ Quick Fix (one-time for current terminal)
To activate the environment right now, just run:

eval "$(mamba shell hook --shell bash)"
mamba activate mmugl
✅ Permanent Fix (initialize for future terminals)
Run this once:

mamba shell init --shell bash --root-prefix=~/.local/share/mamba
Then restart your terminal (or source your .bashrc):

source ~/.bashrc
From then on, you can just use:

mamba activate mmugl
Let me know if you'd like to verify the install by running a specific script from the repo.

You said:
Do i still need to reinstall the package I removed?
ChatGPT said:
Yes — you need to reinstall torchvision==0.2.2 manually, since you previously removed it from the .yml file to allow the environment to build.

Here’s how to do it properly, now that your mmugl environment is active:

✅ Install torchvision==0.2.2 with PyTorch 1.0.0 (CPU-only)
conda install pytorch=1.0.0 torchvision=0.2.2 cpuonly -c pytorch
This ensures you match the expected version range used in the original codebase.

🔍 Optional: Verify Install
After installing:

python -c "import torchvision; print(torchvision.__version__)"
Should print:

0.2.2
Let me know if you want to use GPU support or if you'd rather upgrade to newer torch versions and patch the code (not usually recommended for reproduction).

You said:
Can I do this while in the active environment
ChatGPT said:
Yes, you should absolutely do this while the mmugl environment is active — that's the correct and intended way to install the missing package.

Once you're inside the environment ((mmugl) should appear in your terminal prompt), just run:

conda install pytorch=1.0.0 torchvision=0.2.2 cpuonly -c pytorch
This will install torch and torchvision into that environment specifically, keeping your base system clean and ensuring compatibility with the rest of your project dependencies.

Let me know if the environment uses CUDA — I can give you the GPU install command instead.

You said:
(mmugl) zwt@zwt-MS-7D86:~/CS598-DL4H-Final-Project/mmugl-main$ conda install pytorch=1.0.0 torchvision=0.2.2 cpuonly -c pytorch
Collecting package metadata (current_repodata.json): done
Solving environment: unsuccessful initial attempt using frozen solve. Retrying with flexible solve.
Collecting package metadata (repodata.json): done
Solving environment: unsuccessful initial attempt using frozen solve. Retrying with flexible solve.

PackagesNotFoundError: The following packages are not available from current channels:

  - pytorch=1.0.0
  - torchvision=0.2.2

Current channels:

  - https://conda.anaconda.org/pytorch/linux-64
  - https://conda.anaconda.org/pytorch/noarch
  - https://repo.anaconda.com/pkgs/main/linux-64
  - https://repo.anaconda.com/pkgs/main/noarch
  - https://repo.anaconda.com/pkgs/r/linux-64
  - https://repo.anaconda.com/pkgs/r/noarch

To search for alternate channels that may provide the conda package you're
looking for, navigate to

    https://anaconda.org

and use the search bar at the top of the page.
ChatGPT said:
The error indicates that pytorch=1.0.0 and torchvision=0.2.2 are no longer available via the standard pytorch channel — likely because they're very old versions.

✅ Fix: Use the pytorch channel's historical label
You can install these older versions by explicitly adding the pytorch channel's label/legacy (or label/pytorch-nightly), like so:

conda install pytorch=1.0.0 torchvision=0.2.2 cpuonly -c pytorch -c defaults -c conda-forge -c pytorch/label/legacy
This tells conda to search older archived versions that aren't listed in the main pytorch channel anymore.

🔁 If that still doesn’t work:
You can fall back to installing via pip (still while inside the mmugl env):

pip install torch==1.0.0 torchvision==0.2.2
This will bypass conda but still install the needed versions. It’s often more lenient for old packages.

Let me know if you're using GPU (so I can adjust the install accordingly), or want to upgrade PyTorch instead and patch the codebase.

You said:
Maybe a newer version of torchvision may work? What does it even do?
ChatGPT said:
Great question — let's break it down:

🔍 What is torchvision?
torchvision is a companion library to PyTorch that provides:

Common computer vision datasets
(like CIFAR10, ImageNet, MNIST — with convenient download/load code)

Image transformations
(e.g., resizing, cropping, normalization — via transforms.Compose([...]))

Pretrained vision models
(like ResNet, VGG, AlexNet — useful for transfer learning)

Utilities for image processing
(e.g., utils.make_grid to display image batches, save_image to save tensors as images)

🔁 Can you use a newer version?
Yes — if the code doesn't depend on deprecated or removed functionality, a newer torchvision version (like 0.9, 0.12, even 0.15) will often work.

But here’s the catch:

Old code (written for 0.2.2) might use APIs that were renamed or removed.

So after upgrading, you may need to patch a few imports or usages.

✅ Recommended Strategy
Install a newer (but still stable) version:

conda install pytorch=1.9.0 torchvision=0.10.0 cpuonly -c pytorch
Or for pip:

pip install torch==1.9.0 torchvision==0.10.0
Run the project. If you hit an error like:

AttributeError: module 'torchvision.transforms' has no attribute 'ToPILImage'
then you know you need to patch those lines.

Would you like me to help you identify which part of the repo actually uses torchvision, so we can assess how risky it is to upgrade?

You said:
I am trying to replicate a paper. Though I am running into issues with getting the data preprocessing script provided by the original author to run.

python dev/text_concept_extraction.py --output data --name base --workers 4 --mimic_path ../MIMIC-III --quickumls_path ../UMLS --window 6 --threshold 0.9 --similarity jaccard
zwt-MS-7D86 root[81279] INFO Processing MIMIC data
zwt-MS-7D86 root[81279] INFO [NOTE] processing note-events table
zwt-MS-7D86 root[81279] INFO [NOTE] processed table: (2042451, 5)
zwt-MS-7D86 root[81279] INFO [QuickUMLS] ../UMLS
zwt-MS-7D86 root[81279] INFO [QuickUMLS] jaccard@0.9, w: 6
zwt-MS-7D86 root[81279] INFO [UMLS] parse 2042451 entries on 4 workers
[WARNING] This installation was created with QuickUMLS v.1.3 or earlier, which does not support multiple database backends. For now, I'll assume that leveldb was used as default, implicit assumption will change in future versions of QuickUMLS. More info here: https://github.com/Georgetown-IR-Lab/QuickUMLS/wiki/Migration-QuickUMLS-1.3-to-1.4
[WARNING] This installation was created with QuickUMLS v.1.3 or earlier, which does not support multiple database backends. For now, I'll assume that leveldb was used as default, implicit assumption will change in future versions of QuickUMLS. More info here: https://github.com/Georgetown-IR-Lab/QuickUMLS/wiki/Migration-QuickUMLS-1.3-to-1.4
[WARNING] This installation was created with QuickUMLS v.1.3 or earlier, which does not support multiple database backends. For now, I'll assume that leveldb was used as default, implicit assumption will change in future versions of QuickUMLS. More info here: https://github.com/Georgetown-IR-Lab/QuickUMLS/wiki/Migration-QuickUMLS-1.3-to-1.4
[WARNING] This installation was created with QuickUMLS v.1.3 or earlier, which does not support multiple database backends. For now, I'll assume that leveldb was used as default, implicit assumption will change in future versions of QuickUMLS. More info here: https://github.com/Georgetown-IR-Lab/QuickUMLS/wiki/Migration-QuickUMLS-1.3-to-1.4
multiprocessing.pool.RemoteTraceback: 
"""
Traceback (most recent call last):
  File "/home/zwt/miniconda3/envs/mmugl/lib/python3.8/site-packages/quickumls/core.py", line 150, in __init__
    self.nlp = spacy.load(spacy_lang)
  File "/home/zwt/miniconda3/envs/mmugl/lib/python3.8/site-packages/spacy/__init__.py", line 54, in load
    return util.load_model(
  File "/home/zwt/miniconda3/envs/mmugl/lib/python3.8/site-packages/spacy/util.py", line 435, in load_model
    raise IOError(Errors.E941.format(name=name, full=OLD_MODEL_SHORTCUTS[name]))  # type: ignore[index]
OSError: [E941] Can't find model 'en'. It looks like you're trying to load a model from a shortcut, which is obsolete as of spaCy v3.0. To load the model, use its full name instead:

nlp = spacy.load("en_core_web_sm")

For more details on the available models, see the models directory: https://spacy.io/models. If you want to create a blank model, use spacy.blank: nlp = spacy.blank("en")

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/zwt/miniconda3/envs/mmugl/lib/python3.8/multiprocessing/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/home/zwt/miniconda3/envs/mmugl/lib/python3.8/multiprocessing/pool.py", line 48, in mapstar
    return list(map(*args))
  File "/home/zwt/CS598-DL4H-Final-Project/mmugl-main/kg/kg/data/quickumls.py", line 191, in map_quickumls_from_text_spacy
    nlp.add_pipe("QUExtractor", settings)
  File "/home/zwt/miniconda3/envs/mmugl/lib/python3.8/site-packages/spacy/language.py", line 795, in add_pipe
    pipe_component = self.create_pipe(
  File "/home/zwt/miniconda3/envs/mmugl/lib/python3.8/site-packages/spacy/language.py", line 674, in create_pipe
    resolved = registry.resolve(cfg, validate=validate)
  File "/home/zwt/miniconda3/envs/mmugl/lib/python3.8/site-packages/thinc/config.py", line 747, in resolve
    resolved, _ = cls._make(
  File "/home/zwt/miniconda3/envs/mmugl/lib/python3.8/site-packages/thinc/config.py", line 796, in _make
    filled, _, resolved = cls._fill(
  File "/home/zwt/miniconda3/envs/mmugl/lib/python3.8/site-packages/thinc/config.py", line 868, in _fill
    getter_result = getter(*args, **kwargs)
  File "/home/zwt/CS598-DL4H-Final-Project/mmugl-main/kg/kg/data/quickumls.py", line 170, in create_QUExtractor
    return QUExtractor(nlp, name)
  File "/home/zwt/CS598-DL4H-Final-Project/mmugl-main/kg/kg/data/quickumls.py", line 112, in __init__
    self.quickumls = QuickUMLS(
  File "/home/zwt/miniconda3/envs/mmugl/lib/python3.8/site-packages/quickumls/core.py", line 160, in __init__
    raise OSError(msg)
OSError: Model for language "ENG" is not downloaded. Please run "python -m spacy download en" before launching QuickUMLS
"""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "dev/text_concept_extraction.py", line 206, in <module>
    main()
  File "dev/text_concept_extraction.py", line 149, in main
    chunk_results = pool.map(map_quickumls_from_text_spacy,
  File "/home/zwt/miniconda3/envs/mmugl/lib/python3.8/multiprocessing/pool.py", line 364, in map
    return self._map_async(func, iterable, mapstar, chunksize).get()
  File "/home/zwt/miniconda3/envs/mmugl/lib/python3.8/multiprocessing/pool.py", line 771, in get
    raise self._value
OSError: Model for language "ENG" is not downloaded. Please run "python -m spacy download en" before launching QuickUMLS
ChatGPT said:
You're very close to having everything working — this error is due to spaCy expecting a language model that hasn't been downloaded yet.

✅ Fix
Run this inside your mmugl environment:

python -m spacy download en_core_web_sm
This will install the small English model, which is what QuickUMLS and spaCy are trying to load.

Then try your command again:

python dev/text_concept_extraction.py --output data --name base --workers 4 --mimic_path ../MIMIC-III --quickumls_path ../UMLS --window 6 --threshold 0.9 --similarity jaccard
🔍 What happened?
Older versions of spaCy used "en" as a shortcut for "en_core_web_sm".

As of spaCy >= 3.0, those shortcuts were removed, and spacy.load("en") will fail unless you explicitly download the correct model.

Let me know if you want to suppress the QuickUMLS v1.3 warnings too, or upgrade the pipeline to a newer version of spaCy and QuickUMLS.

You said:
(mmugl) zwt@zwt-MS-7D86:~/CS598-DL4H-Final-Project/mmugl-main$ python dev/text_concept_extraction.py --output data --name base --workers 4 --mimic_path ../MIMIC-III --quickumls_path ../UMLS --window 6 --threshold 0.9 --similarity jaccard
zwt-MS-7D86 root[81327] INFO Processing MIMIC data
zwt-MS-7D86 root[81327] INFO [NOTE] processing note-events table
zwt-MS-7D86 root[81327] INFO [NOTE] processed table: (2042451, 5)
zwt-MS-7D86 root[81327] INFO [QuickUMLS] ../UMLS
zwt-MS-7D86 root[81327] INFO [QuickUMLS] jaccard@0.9, w: 6
zwt-MS-7D86 root[81327] INFO [UMLS] parse 2042451 entries on 4 workers
[WARNING] This installation was created with QuickUMLS v.1.3 or earlier, which does not support multiple database backends. For now, I'll assume that leveldb was used as default, implicit assumption will change in future versions of QuickUMLS. More info here: https://github.com/Georgetown-IR-Lab/QuickUMLS/wiki/Migration-QuickUMLS-1.3-to-1.4
[WARNING] This installation was created with QuickUMLS v.1.3 or earlier, which does not support multiple database backends. For now, I'll assume that leveldb was used as default, implicit assumption will change in future versions of QuickUMLS. More info here: https://github.com/Georgetown-IR-Lab/QuickUMLS/wiki/Migration-QuickUMLS-1.3-to-1.4
[WARNING] This installation was created with QuickUMLS v.1.3 or earlier, which does not support multiple database backends. For now, I'll assume that leveldb was used as default, implicit assumption will change in future versions of QuickUMLS. More info here: https://github.com/Georgetown-IR-Lab/QuickUMLS/wiki/Migration-QuickUMLS-1.3-to-1.4
[WARNING] This installation was created with QuickUMLS v.1.3 or earlier, which does not support multiple database backends. For now, I'll assume that leveldb was used as default, implicit assumption will change in future versions of QuickUMLS. More info here: https://github.com/Georgetown-IR-Lab/QuickUMLS/wiki/Migration-QuickUMLS-1.3-to-1.4
multiprocessing.pool.RemoteTraceback: 
"""
Traceback (most recent call last):
  File "/home/zwt/miniconda3/envs/mmugl/lib/python3.8/site-packages/quickumls/core.py", line 150, in __init__
    self.nlp = spacy.load(spacy_lang)
  File "/home/zwt/miniconda3/envs/mmugl/lib/python3.8/site-packages/spacy/__init__.py", line 54, in load
    return util.load_model(
  File "/home/zwt/miniconda3/envs/mmugl/lib/python3.8/site-packages/spacy/util.py", line 435, in load_model
    raise IOError(Errors.E941.format(name=name, full=OLD_MODEL_SHORTCUTS[name]))  # type: ignore[index]
OSError: [E941] Can't find model 'en'. It looks like you're trying to load a model from a shortcut, which is obsolete as of spaCy v3.0. To load the model, use its full name instead:

nlp = spacy.load("en_core_web_sm")

For more details on the available models, see the models directory: https://spacy.io/models. If you want to create a blank model, use spacy.blank: nlp = spacy.blank("en")

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/zwt/miniconda3/envs/mmugl/lib/python3.8/multiprocessing/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/home/zwt/miniconda3/envs/mmugl/lib/python3.8/multiprocessing/pool.py", line 48, in mapstar
    return list(map(*args))
  File "/home/zwt/CS598-DL4H-Final-Project/mmugl-main/kg/kg/data/quickumls.py", line 191, in map_quickumls_from_text_spacy
    nlp.add_pipe("QUExtractor", settings)
  File "/home/zwt/miniconda3/envs/mmugl/lib/python3.8/site-packages/spacy/language.py", line 795, in add_pipe
    pipe_component = self.create_pipe(
  File "/home/zwt/miniconda3/envs/mmugl/lib/python3.8/site-packages/spacy/language.py", line 674, in create_pipe
    resolved = registry.resolve(cfg, validate=validate)
  File "/home/zwt/miniconda3/envs/mmugl/lib/python3.8/site-packages/thinc/config.py", line 747, in resolve
    resolved, _ = cls._make(
  File "/home/zwt/miniconda3/envs/mmugl/lib/python3.8/site-packages/thinc/config.py", line 796, in _make
    filled, _, resolved = cls._fill(
  File "/home/zwt/miniconda3/envs/mmugl/lib/python3.8/site-packages/thinc/config.py", line 868, in _fill
    getter_result = getter(*args, **kwargs)
  File "/home/zwt/CS598-DL4H-Final-Project/mmugl-main/kg/kg/data/quickumls.py", line 170, in create_QUExtractor
    return QUExtractor(nlp, name)
  File "/home/zwt/CS598-DL4H-Final-Project/mmugl-main/kg/kg/data/quickumls.py", line 112, in __init__
    self.quickumls = QuickUMLS(
  File "/home/zwt/miniconda3/envs/mmugl/lib/python3.8/site-packages/quickumls/core.py", line 160, in __init__
    raise OSError(msg)
OSError: Model for language "ENG" is not downloaded. Please run "python -m spacy download en" before launching QuickUMLS
"""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "dev/text_concept_extraction.py", line 206, in <module>
    main()
  File "dev/text_concept_extraction.py", line 149, in main
    chunk_results = pool.map(map_quickumls_from_text_spacy,
  File "/home/zwt/miniconda3/envs/mmugl/lib/python3.8/multiprocessing/pool.py", line 364, in map
    return self._map_async(func, iterable, mapstar, chunksize).get()
  File "/home/zwt/miniconda3/envs/mmugl/lib/python3.8/multiprocessing/pool.py", line 771, in get
    raise self._value
OSError: Model for language "ENG" is not downloaded. Please run "python -m spacy download en" before launching QuickUMLS
(mmugl) zwt@zwt-MS-7D86:~/CS598-DL4H-Final-Project/mmugl-main$ 
ChatGPT said:
You're still seeing the same error because the code is trying to load the deprecated shortcut "en" rather than the correct model name ("en_core_web_sm"), and it doesn’t fall back even if you downloaded the model.

✅ What’s going on?
QuickUMLS (or your wrapper) is calling:

spacy.load("en")
But in spaCy v3+, "en" is no longer valid — they removed these shortcuts.

🛠️ Fix Options
Option 1 (Quick & Dirty Patch)
If you want to fix this immediately for your local run:

Open this file:

mmugl-main/kg/kg/data/quickumls.py
Find the line in __init__ where it does:

self.nlp = spacy.load(spacy_lang)
Just force it to load the real model name:

Replace that line with:

self.nlp = spacy.load("en_core_web_sm")
Or, better, change the default spacy_lang from "en" to "en_core_web_sm" earlier in that file.

Save and rerun your script.

Option 2 (Patch it via symlink)
If you don’t want to edit source code, you can trick spaCy into accepting "en":

python -m spacy link en_core_web_sm en
This creates a shortcut so spacy.load("en") works again.

✅ Recommended
Do Option 1 for clarity (explicit is better than implicit), especially since you're replicating research code and may want it to be portable and reproducible.

Want me to walk you through editing the source file if you share the relevant lines?

You said:
# ===============================================
#
# Utility around QuickUMLS
#
# ===============================================
import logging
from collections import namedtuple
from typing import Any, Dict, List, Sequence, Tuple

import numpy as np
import pandas as pd
import spacy
from negspacy.negation import Negex
from quickumls import QuickUMLS
from spacy.language import Language
from spacy.tokens import Span, Token
from tqdm import tqdm

# KG
from kg.utils.constants import QUICKUMLS_ACCEPTED_SEMTYPES

QUSettings = namedtuple("QUSettings", ["quickumls_path", "threshold", "similarity", "window"])

default_qu_settigs = QUSettings(
    quickumls_path="please/provide/default/path/to/quickumls",
    threshold=0.9,
    similarity="jaccard",
    window=6,
)


def map_quickumls_from_text(data: Tuple) -> Sequence[Tuple[Any, ...]]:
    """
    For each row in data map the text notes to
    UMLS CUIs using QuickUMLS

    Parameter
    ---------
    data: namedtuple
        data.worker_id: int
            parallel worker id
        data.chunk_df: pd.DataFrame
            chunk of the noteevents dataframe
        data.settings: Dict[str, Any]
            quickumls settings
    """
    worker_id, chunk_df, settings = data

    matcher = QuickUMLS(
        settings.quickumls_path,
        overlapping_criteria="score",
        threshold=settings.threshold,
        similarity_name=settings.similarity,
        window=settings.window,
        accepted_semtypes=QUICKUMLS_ACCEPTED_SEMTYPES,
    )

    # get iterator over chunk
    iterator = chunk_df.iterrows()
    if worker_id == 0:
        iterator = tqdm(iterator, total=len(chunk_df))

    results = []
    for _, row in iterator:

        patient_id = row["SUBJECT_ID"]
        visit_id = row["HADM_ID"]
        category = row["CATEGORY"].strip()

        # get text and preprocess
        text = row["TEXT"]
        text = text.replace("\n", " ").replace("\r", "")  # remove linebreaks

        matches = matcher.match(text, best_match=True, ignore_syntax=False)

        document = set()
        cuis = set()
        for candidates in matches:

            # get best match
            match = candidates[0]

            # get attributes
            cui = match["cui"]
            similarity = match["similarity"]

            if cui not in cuis:
                cuis.add(cui)
                document.add((cui, similarity))

        results.append((patient_id, visit_id, category, document))

    return results


# ----------------------------------------------
#
# Spacy-based utilities
#
# ----------------------------------------------
class QUExtractor(object):
    """
    Spacy Compatible QuickUMLS Extractor Class

    Adapted from work by *Anonym*: *URL*
    specifically: *URL*
    """

    def __init__(self, nlp, qu_settings):

        # Create QuickUMLS Object
        self.quickumls = QuickUMLS(
            qu_settings.quickumls_path,
            overlapping_criteria="score",
            threshold=qu_settings.threshold,
            similarity_name=qu_settings.similarity,
            window=qu_settings.window,
            accepted_semtypes=QUICKUMLS_ACCEPTED_SEMTYPES,
        )

        # Save spacy pipeline
        self.nlp = nlp

        # let's extend this with some proprties that we want
        Span.set_extension("similarity", default=-1.0, force=True)
        Span.set_extension("semtypes", default=-1.0, force=True)
        Span.set_extension("cui", default="", force=True)
        Span.set_extension("ngram", default="", force=True)

    def __call__(self, doc):

        # Match UMLS vocabulary to document with ngrams and matches
        matches = self.quickumls._match(doc, best_match=True, ignore_syntax=False)

        # Convert QuickUMLS match objects into Spans

        # Integrate matches into document span for entity recognition
        for match in matches:

            # take first n_gram match
            ngram_match_dict = match[0]

            start_char_idx = int(ngram_match_dict["start"])
            end_char_idx = int(ngram_match_dict["end"])

            cui = ngram_match_dict["cui"]

            # char_span() creates a Span from these character indices
            # UMLS CUI should work well as the label here
            span = doc.char_span(start_char_idx, end_char_idx, label="UMLS", kb_id=str(cui))
            # add some custom metadata to the spans
            # if a span is already part of a different
            # recovered entity we skip it
            span._.similarity = ngram_match_dict["similarity"]
            span._.ngram = ngram_match_dict["ngram"]
            span._.semtypes = ngram_match_dict["semtypes"]
            span._.cui = cui
            try:
                if str(span) not in str(doc.ents):
                    doc.ents = list(doc.ents) + [span]
            except ValueError as e:
                logging.info(f"Could not add span: {e}")
                pass

        return doc


@Language.factory("QUExtractor")
def create_QUExtractor(nlp, name):
    return QUExtractor(nlp, name)


def map_quickumls_from_text_spacy(data: Tuple) -> Sequence[Tuple[Any, ...]]:
    """
    For each row in data map the text notes to
    UMLS CUIs using QuickUMLS

    Parameter
    ---------
    data: namedtuple
        data.worker_id: int
            parallel worker id
        data.chunk_df: pd.DataFrame
            chunk of the noteevents dataframe
        data.settings: Dict[str, Any]
            quickumls settings
    """
    worker_id, chunk_df, settings = data

    nlp = spacy.load("en_core_web_sm")
    nlp.add_pipe("QUExtractor", settings)
    nlp.add_pipe("negex", config={"ent_types": ["UMLS"]})

    # get iterator over chunk
    iterator = chunk_df.iterrows()
    if worker_id == 0:
        iterator = tqdm(iterator, total=len(chunk_df))

    results = []
    for _, row in iterator:

        patient_id = row["SUBJECT_ID"]
        visit_id = row["HADM_ID"]
        category = row["CATEGORY"].strip()

        # get text and preprocess
        text = row["TEXT"]
        text = text.replace("\n", " ").replace("\r", "")  # remove linebreaks
        matches = nlp(text)

        document = set()
        cuis = set()
        for candidates in matches.ents:

            # get attributes
            cui = candidates._.cui
            similarity = candidates._.similarity
            negation = -1 if candidates._.negex else 1

            if cui not in cuis and cui != "":
                cuis.add(cui)
                document.add((cui, similarity, negation))

        results.append((patient_id, visit_id, category, document))

    return results
ChatGPT said:
Thanks for sharing the source — this clears it up.

✅ Good news: your quickumls.py already uses the correct model name:

nlp = spacy.load("en_core_web_sm")
❌ Bad news: the traceback shows the error is not coming from this file — it's coming from the QuickUMLS library itself, which still tries to load spacy.load("en") inside its own internals.

🛠️ You have two solid options:
✅ Option 1: Add a Shortcut for "en" via spaCy CLI
Run this in your terminal:

python -m spacy link en_core_web_sm en
This creates a symlink so that calls to spacy.load("en") will resolve to the already-installed "en_core_web_sm" model. This is the simplest fix, especially when a third-party package (like quickumls) is using hardcoded "en".

🔧 Option 2: Patch QuickUMLS Source (Harder to maintain)
Open this file (likely inside your env):

~/miniconda3/envs/mmugl/lib/python3.8/site-packages/quickumls/core.py
Find this line (around line 150):

self.nlp = spacy.load(spacy_lang)
Replace it with:

if spacy_lang == "en":
    spacy_lang = "en_core_web_sm"
self.nlp = spacy.load(spacy_lang)
This prevents future crashes even if spacy_lang="en" is passed in.

🎯 Recommendation
Start with Option 1 (spacy link), since it's easy and doesn’t modify any source files:

python -m spacy link en_core_web_sm en
Then rerun your script.

Let me know if you want to silence the QuickUMLS v1.3 warnings too.

You said:
(mmugl) zwt@zwt-MS-7D86:~/CS598-DL4H-Final-Project/mmugl-main$ python -m spacy link en_core_web_sm en
DeprecationWarning: The command 'link' is deprecated.
⚠ As of spaCy v3.0, model symlinks are not supported anymore. You can
load trained pipeline packages using their full names or from a directory
path.
ChatGPT said:
Could you confirm whether you'd prefer to patch the QuickUMLS source code to fix the use of spacy.load('en'), or whether you'd like me to help downgrade to a spaCy version where the spacy link workaround still works?

You said:
if spacy_lang == "en":
    spacy_lang = "en_core_web_sm"
self.nlp = spacy.load(spacy_lang)
ChatGPT said:
Perfect — that’s exactly the patch needed to get around the deprecated spacy.load('en') usage in QuickUMLS. Now QuickUMLS should successfully load the modern spaCy model without throwing that [E941] error.

You can now rerun your pipeline:

python dev/text_concept_extraction.py --output data --name base --workers 4 --mimic_path ../MIMIC-III --quickumls_path ../UMLS --window 6 --threshold 0.9 --similarity jaccard
Let me know if you want help verifying outputs or prepping the next stage of

Could you please confirm which version of QuickUMLS you're using (e.g., 1.3 or 1.4), and whether you installed it from pip or from source? This will help determine why the _match method is missing — it may have been renamed or removed in your version.

You said:
How do I check
ChatGPT said:
You can check the version of QuickUMLS and how it was installed in a couple of ways:

✅ Option 1: Check via Python (inside your mmugl environment)
Run Python:

python
Then:

import quickumls
print(quickumls.__version__)
print(quickumls.__file__)
This will tell you:

The installed version (e.g., 1.3.0, 1.4.0)

The exact path of the installed package (so you can inspect or modify it)

✅ Option 2: Use pip
Still in your mmugl environment:

pip show quickumls
This will print:

Version

Installation path

Whether it came from PyPI or a Git repo

Let me know the output and I can help resolve the _match issue or determine if a method was renamed in your version.

You said:
mmugl) zwt@zwt-MS-7D86:~/CS598-DL4H-Final-Project/mmugl-main$ pip show quickumls
Name: quickumls
Version: 1.4.0.post1
Summary: QuickUMLS is a tool for fast, unsupervised biomedical concept extraction from medical text
Home-page: https://github.com/Georgetown-IR-Lab/QuickUMLS
Author: Luca Soldaini
Author-email: luca@ir.cs.georgetown.edu
License: MIT
Location: /home/zwt/miniconda3/envs/mmugl/lib/python3.8/site-packages
Requires: leveldb, nltk, numpy, quickumls-simstring, spacy, unidecode, unqlite
Required-by: 
ChatGPT said:
Thanks — you're using QuickUMLS v1.4.0.post1, which is the latest stable version, installed via pip.

This version changed the internal API — the method _match no longer exists (it was private anyway), and that’s likely causing the crash in your QUExtractor.

✅ Fix: Replace self.quickumls._match(...) with self.quickumls.match(...)
In your kg/kg/data/quickumls.py, update this line inside the __call__ method of QUExtractor:

❌ Old (broken in v1.4):
matches = self.quickumls._match(doc, best_match=True, ignore_syntax=False)
✅ New (compatible with v1.4):
matches = self.quickumls.match(doc.text, best_match=True, ignore_syntax=False)
Explanation:

self.quickumls.match(...) expects raw text, not a Doc object, so we use doc.text.

Optional: Verify it works
Once you make this change, rerun:

python dev/text_concept_extraction.py --output data --name base --workers 4 --mimic_path ../MIMIC-III --quickumls_path ../UMLS --window 6 --threshold 0.9 --similarity jaccard
Let me know if you’d like to test a smaller subset of MIMIC first to save time while debugging.

You said:
4 --mimic_path ../MIMIC-III --quickumls_path ../QuickUMLS --window 6 --threshold 0.9 --similarity jaccard
zwt-MS-7D86 root[81635] INFO Processing MIMIC data
zwt-MS-7D86 root[81635] INFO [NOTE] processing note-events table
zwt-MS-7D86 root[81635] INFO [NOTE] processed table: (2042451, 5)
zwt-MS-7D86 root[81635] INFO [QuickUMLS] ../QuickUMLS
zwt-MS-7D86 root[81635] INFO [QuickUMLS] jaccard@0.9, w: 6
zwt-MS-7D86 root[81635] INFO [UMLS] parse 2042451 entries on 4 workers
  0%|                                                                                                        | 0/510613 [00:00<?, ?it/s]
multiprocessing.pool.RemoteTraceback: 
"""
Traceback (most recent call last):
  File "/home/zwt/miniconda3/envs/mmugl/lib/python3.8/multiprocessing/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/home/zwt/miniconda3/envs/mmugl/lib/python3.8/multiprocessing/pool.py", line 48, in mapstar
    return list(map(*args))
  File "/home/zwt/CS598-DL4H-Final-Project/mmugl-main/kg/kg/data/quickumls.py", line 209, in map_quickumls_from_text_spacy
    matches = nlp(text)
  File "/home/zwt/miniconda3/envs/mmugl/lib/python3.8/site-packages/spacy/language.py", line 1025, in __call__
    error_handler(name, proc, [doc], e)
  File "/home/zwt/miniconda3/envs/mmugl/lib/python3.8/site-packages/spacy/util.py", line 1658, in raise_error
    raise e
  File "/home/zwt/miniconda3/envs/mmugl/lib/python3.8/site-packages/spacy/language.py", line 1020, in __call__
    doc = proc(doc, **component_cfg.get(name, {}))  # type: ignore[call-arg]
  File "/home/zwt/CS598-DL4H-Final-Project/mmugl-main/kg/kg/data/quickumls.py", line 133, in __call__
    matches = self.quickumls._match(doc.text, best_match=True, ignore_syntax=False)
AttributeError: 'QuickUMLS' object has no attribute '_match'
"""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "dev/text_concept_extraction.py", line 206, in <module>
    main()
  File "dev/text_concept_extraction.py", line 149, in main
    chunk_results = pool.map(map_quickumls_from_text_spacy,
  File "/home/zwt/miniconda3/envs/mmugl/lib/python3.8/multiprocessing/pool.py", line 364, in map
    return self._map_async(func, iterable, mapstar, chunksize).get()
  File "/home/zwt/miniconda3/envs/mmugl/lib/python3.8/multiprocessing/pool.py", line 771, in get
    raise self._value
AttributeError: 'QuickUMLS' object has no attribute '_match'
ChatGPT said:
Thanks — that confirms the issue is exactly what we anticipated:

🔥 You're still calling self.quickumls._match(...), but in QuickUMLS v1.4+, _match no longer exists (it was a private method in earlier versions).

✅ Final Fix (please apply this now):
In your file kg/kg/data/quickumls.py, update the __call__ method of the QUExtractor class.

🔁 Replace this line (currently line 133):
matches = self.quickumls._match(doc.text, best_match=True, ignore_syntax=False)
✅ With this instead:
matches = self.quickumls.match(doc.text, best_match=True, ignore_syntax=False)
📌 Why it works
match(...) is the public method you should be using.

It takes in raw text, which doc.text provides.

match(...) returns the same format of candidate matches you're already looping over.

Once you've made that change, re-run your script:

python dev/text_concept_extraction.py --output data --name base --workers 4 --mimic_path ../MIMIC-III --quickumls_path ../QuickUMLS --window 6 --threshold 0.9 --similarity jaccard
Let me know if you'd like help testing on a smaller sample to speed up iteration.

You said:
#!/usr/bin/env python3
# ===============================================
#
# Extract Medical Concepts from Clinical Reports
# in MIMIC-III with QuickUMLS
#
# ===============================================
from fileinput import filename
from json import load
import logging
import coloredlogs
import argparse
import pickle
import time

import os
import subprocess
from os import path
from typing import Dict, Any
from random import choice, sample
from multiprocessing import Pool
from itertools import repeat

# Tools
import networkx as nx
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from tqdm import tqdm, trange
from spacy.tokens import Span

# QuickUMLS
from quickumls import QuickUMLS
# from quickumls.constants import ACCEPTED_SEMTYPES

# Custom KG
from kg.data.umls import exclude_vocabularies, load_mrconso, match_code_set_to_cuis
from kg.data.processing import read_set_codes, load_noteevents_table
from kg.data.graph import build_atc_tree, build_icd9_tree
from kg.utils.constants import QUICKUMLS_ACCEPTED_SEMTYPES
from kg.data.quickumls import QUSettings, map_quickumls_from_text, map_quickumls_from_text_spacy
import kg.data.quickumls
from kg.data.eicu import process_note_table_eICU

# Globals
LOGGING_LEVEL = "INFO"


def parse_arguments() -> argparse.Namespace:

    # get parser
    parser = argparse.ArgumentParser()

    # General options
    parser.add_argument("-o", "--output", required=True, type=str,
        help="Output folder")
    parser.add_argument("-n", "--name", default="base", type=str,
        help="Name, will be part of the output filename")
    parser.add_argument("-w", "--workers", default=1, type=int,
        help="Number of parallel worker processes to use")
    parser.add_argument("--test", default=-1, type=int,
        help="Slice of data for testing")
    parser.add_argument("--use_scratch", default=None, type=str,
        help="Use local scratch for QuickUMLS files")

    parser_data = parser.add_argument_group("Data")
    parser_data.add_argument("--mimic_path", required=True, type=str,
        help="Path to MIMIC-III data")
    parser_data.add_argument("--eicu_path", default=None, type=str,
        help="Path to eICU hdf5 files")

    # Grah related arguments
    parser_quick = parser.add_argument_group("QuickUMLS")
    parser_quick.add_argument("--quickumls_path", required=True, type=str,
        help="Path to QuickUMLS files")
    parser_quick.add_argument("--window", default=6, type=int,
        help="QuickUMLS window size")
    parser_quick.add_argument("-t", "--threshold", default=0.9, type=float,
        help="QuickUMLS threshold value")
    parser_quick.add_argument("-s", "--similarity", default='jaccard', type=str,
        help="QuickUMLS similarity metric: {jaccard, cosine, dice, overlap}")

    args = parser.parse_args()
    return args




def main():
    """Graph Extraciton Script procedure"""
    t0_full = time.time()

    # Parse CMD arguments
    args = parse_arguments()
    processing_eICU_dataset = args.eicu_path is not None

    # set QU path
    quickumls_path = args.quickumls_path

    # configure local scratch
    if args.use_scratch is not None:

        tmpdir = args.use_scratch
        logging.info(f"Using local scratch at {tmpdir}")

        process = subprocess.run(
            ['rsync', '-aq', args.quickumls_path,  tmpdir],
            stdout=subprocess.PIPE)

        if process.returncode != 0:
            logging.error(f"Rsync from: {args.quickumls_path} to local scratch failed: {process.returncode}")
            exit()
        else:
            logging.info(f"Copied QuickUMLS to local scratch")
            quickumls_path = tmpdir


    # get noteevents filename
    if not processing_eICU_dataset:
        logging.info(f"Processing MIMIC data")
        noteevents_file = path.join(args.mimic_path, 'NOTEEVENTS.csv')
        noteevents_df = load_noteevents_table(noteevents_file, drop_error=True)
    else:
        logging.info(f"Processing eICU data")
        noteevents_file = path.join(args.eicu_path, 'note.h5')
        patients_file = path.join(args.eicu_path, 'patient.h5')
        noteevents_df = process_note_table_eICU(noteevents_file, patients_file)
        
    if args.test > 0:
        noteevents_df = noteevents_df[:args.test]
        logging.warning(f"[TESTING] data shape: {noteevents_df.shape}")

    # split
    df_split = np.array_split(noteevents_df, args.workers)

    # worker pools
    pool = Pool(args.workers)

    # QuickUMLS settings
    logging.info(f"[QuickUMLS] {quickumls_path}")
    logging.info(f"[QuickUMLS] {args.similarity}@{args.threshold}, w: {args.window}")
    settings = QUSettings(quickumls_path, args.threshold,
        args.similarity, args.window)

    # map using parallel pool
    logging.info(f"[UMLS] parse {len(noteevents_df)} entries on {args.workers} workers")
    t0 = time.time()
    chunk_results = pool.map(map_quickumls_from_text_spacy,
        zip(range(args.workers), df_split, repeat(settings, args.workers)))
    t1 = time.time()
    logging.info(f"[TIME] took {((t1 - t0) / 60):.1f} minutes")

    logging.info(f"[UMLS] merging chunks")
    t0 = time.time()
    results = {}
    for chunk in chunk_results:
        for row in tqdm(chunk):

            patient_id = row[0]
            visit_id = row[1]
            category = row[2]
            cuis = row[3]

            if patient_id not in results:
                results[patient_id] = {}

            results_patient = results[patient_id]
            if visit_id not in results_patient:
                results_patient[visit_id] = {}

            results_visit = results_patient[visit_id]
            if category not in results_visit:
                results_visit[category] = []

            results_category = results_visit[category]
            results_category.append(cuis)

    t1 = time.time()
    logging.info(f"[TIME] took {((t1 - t0) / 60):.1f} minutes")

    filename = "noteevents"
    if processing_eICU_dataset:
        filename += "_eICU"
    filename += f"_{args.name}_{args.similarity}_{args.threshold}_w{args.window}.pkl"
    file_path = path.join(args.output, filename)
    logging.info(f"Store graph and maps to: {file_path}")
    with open(file_path, 'wb') as handle:
        pickle.dump(results, handle, protocol=pickle.HIGHEST_PROTOCOL)

    t1_full = time.time()
    logging.info(40*"=")
    logging.info(f"[TIME] took {((t1_full - t0_full) / 60):.1f} minutes")
    logging.info("Done")
    logging.info(40*"=")



if __name__ == "__main__":
    
    # set logging
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s | %(message)s')
    coloredlogs.install(level=LOGGING_LEVEL)

    # run script
    main()


If I want the preprocessing to be done faster should I limit the data in NOTEEVENTS?
ChatGPT said:
Yes — if you want faster preprocessing for debugging, development, or testing, limiting the number of rows from NOTEEVENTS.csv is the correct and recommended approach.

✅ How to Limit the Data for Faster Runs
You already have the option built-in:

--test <N>
This slices the first N rows from the NOTEEVENTS.csv file:

Example (use only first 10,000 rows):
python dev/text_concept_extraction.py \
  --output data \
  --name test_run \
  --workers 4 \
  --mimic_path ../MIMIC-III \
  --quickumls_path ../QuickUMLS \
  --window 6 \
  --threshold 0.9 \
  --similarity jaccard \
  --test 10000
This will dramatically reduce runtime (from hours to minutes), and allow you to:

Validate QuickUMLS is working

Debug multiprocessing or model extraction

Confirm output structure

📌 Tip: Parallelism vs. Data Size
Also remember:

--workers controls parallelism. On 4-core CPUs, --workers 4 is fine.

Limiting rows with --test is the best way to reduce processing time early on.

Would you like a line of code to preview how many clinical notes are in the dataset per category or per patient?

You said:
UMLS Knowledge Graph
The extracted knowledge graph is based on the medical concepts extracted from the dataset. The script umls_graph_extraction.py in ./dev is used to create the graph:

python umls_graph_extraction.py \
    --output $PROJECT/data/umls_graph.pkl \
    --workers 4 \
    --text_concepts $PROJECT/data/noteevents_base_jaccard_0.9_w6.pkl \
    --use_categories \
    --icd_codes $PROJECT/data/disease_codes_all.txt \
    --atc_codes $PROJECT/data/med_codes_all.txt \
    --build_code_trees \
    --umls_path $PATH_TO_UMLS_DATA \
    --grow_hops 0 \
    --reduce_vocabularies \
    --add_sab \
    --add_sapbert
This should store the extracted graph as a networkx object in $PROJECT/data/umls_graph.pkl.
ChatGPT said:
Thanks for sharing — you're now at the graph construction step, which builds the UMLS knowledge graph from previously extracted concept mentions (noteevents_base_jaccard_0.9_w6.pkl).

Here's a breakdown of what the command does, and how you can run it correctly and efficiently:

✅ Minimal Working Command (Adjusted)
Run this from your project root, assuming your noteevents_* pickle exists:

python dev/umls_graph_extraction.py \
  --output data/umls_graph.pkl \
  --workers 4 \
  --text_concepts data/noteevents_base_jaccard_0.9_w6.pkl \
  --use_categories \
  --icd_codes data/disease_codes_all.txt \
  --atc_codes data/med_codes_all.txt \
  --build_code_trees \
  --umls_path ../UMLS \
  --grow_hops 0 \
  --reduce_vocabularies \
  --add_sab \
  --add_sapbert
🔍 What Each Argument Does
Argument	Purpose
--output	Path to store the final NetworkX graph (.pkl)
--text_concepts	Preprocessed file from text_concept_extraction.py
--icd_codes, --atc_codes	Optional vocabularies (e.g., for diseases and medications)
--umls_path	Path to UMLS/QuickUMLS vocab resources
--use_categories	Organize concepts by note category (e.g., discharge summary, radiology)
--grow_hops	Expand the graph via neighbors (0 = no growth)
--reduce_vocabularies	Filter nodes to selected vocabularies
--add_sab	Add UMLS source vocabularies as edge/node attributes
--add_sapbert	Optionally annotate with SapBERT embeddings (requires embeddings to exist)
🧠 Performance Tip
If you're still testing or debugging, consider pointing --text_concepts to a smaller file (like one made with --test 10000) to make graph construction much faster.

✅ Output
Final graph stored at data/umls_graph.pkl

You can later load and inspect it using:

import pickle
import networkx as nx

with open("data/umls_graph.pkl", "rb") as f:
    G = pickle.load(f)

print(nx.info(G))
Would you like help visualizing the graph or exploring statistics like node degrees or concept types after it’s built?

You said:
(mmugl) zwt@zwt-MS-7D86:~/CS598-DL4H-Final-Project/mmugl-main$ python dev/umls_graph_extraction.py   --output data/umls_graph.pkl   --workers 4   --text_concepts data/noteevents_base_jaccard_0.9_w6.pkl   --use_categories   --icd_codes data/disease_codes_all.txt   --atc_codes data/med_codes_all.txt   --build_code_trees   --umls_path ../UMLS   --grow_hops 0   --reduce_vocabularies   --add_sab   --add_sapbert
zwt-MS-7D86 root[82938] INFO Built full ICD tree: 8092
zwt-MS-7D86 root[82938] INFO Built full ATC tree: 595
zwt-MS-7D86 root[82938] INFO [UMLS] loaded MRCONSO: (14722785, 8)
zwt-MS-7D86 root[82938] INFO [UMLS] loaded MRREL: (54025710, 6)
zwt-MS-7D86 root[82938] INFO Reduce vocabulary with curated list
zwt-MS-7D86 root[82938] INFO [UMLS] Keep vocabularies: ['ICD9CM', 'ATC', 'BI', 'NCI_CDISC', 'CCC', 'CCS', 'RAM', 'CCPSS', 'CPT', 'CSP', 'DRUGBANK', 'DXP', 'FMA', 'ICNP', 'ICF', 'MeSH', 'NIC', 'NOC', 'PCDS', 'RXNORM', 'SNOMEDCT_US', 'WHO']
zwt-MS-7D86 root[82938] INFO [UMLS]	dropped 7275341 entries
zwt-MS-7D86 root[82938] INFO [UMLS] Keep vocabularies: ['ICD9CM', 'ATC', 'BI', 'NCI_CDISC', 'CCC', 'CCS', 'RAM', 'CCPSS', 'CPT', 'CSP', 'DRUGBANK', 'DXP', 'FMA', 'ICNP', 'ICF', 'MeSH', 'NIC', 'NOC', 'PCDS', 'RXNORM', 'SNOMEDCT_US', 'WHO']
zwt-MS-7D86 root[82938] INFO [UMLS]	dropped 46846408 entries
zwt-MS-7D86 root[82938] INFO [UMLS] Exclude vocabularies: ['DMDICD10', 'ICD10PCS', 'ICD10AE', 'ICD10AM', 'ICD10AMAE', 'ICD10DUT']
zwt-MS-7D86 root[82938] INFO [UMLS]	dropped 0 entries
zwt-MS-7D86 root[82938] INFO [UMLS] Exclude vocabularies: ['DMDICD10', 'ICD10PCS', 'ICD10AE', 'ICD10AM', 'ICD10AMAE', 'ICD10DUT']
zwt-MS-7D86 root[82938] INFO [UMLS]	dropped 0 entries
zwt-MS-7D86 root[82938] INFO [UMLS] Matching codes to ICD9CM
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8092/8092 [00:04<00:00, 1763.60it/s]
zwt-MS-7D86 root[82938] INFO [UMLS]	Matched 8080/8092
zwt-MS-7D86 root[82938] INFO [UMLS]	Unmatched codes: ['icd9_root', 'V01-V09', '320-327', '42682', 'E849', 'E846-E849', '610-611', '630-639', '61172', '5130', '7242', '71970']
zwt-MS-7D86 root[82938] INFO [UMLS]	2nd pass unmatched: icd9_roo
zwt-MS-7D86 root[82938] INFO [UMLS]	2nd pass unmatched: V01-V0
zwt-MS-7D86 root[82938] INFO [UMLS]	2nd pass unmatched: 320-32
zwt-MS-7D86 root[82938] INFO [UMLS]	2nd pass unmatched: E84
zwt-MS-7D86 root[82938] INFO [UMLS]	2nd pass unmatched: E846-E84
zwt-MS-7D86 root[82938] INFO [UMLS]	2nd pass unmatched: 610-61
zwt-MS-7D86 root[82938] INFO [UMLS]	2nd pass unmatched: 630-63
zwt-MS-7D86 root[82938] INFO [UMLS]	Final matched 8085/8092
zwt-MS-7D86 root[82938] INFO [UMLS]	Mapping size: 8085
zwt-MS-7D86 root[82938] INFO [UMLS] Matching codes to ATC
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 595/595 [00:00<00:00, 7755.91it/s]
zwt-MS-7D86 root[82938] INFO [UMLS]	Matched 592/595
zwt-MS-7D86 root[82938] INFO [UMLS]	Unmatched codes: ['atc_root', 'L01XE', 'L01XC']
zwt-MS-7D86 root[82938] INFO [UMLS]	2nd pass unmatched: atc_roo
zwt-MS-7D86 root[82938] INFO [UMLS]	Final matched 594/595
zwt-MS-7D86 root[82938] INFO [UMLS]	Mapping size: 594
zwt-MS-7D86 root[82938] INFO Load CUIs from text notes
zwt-MS-7D86 root[82938] INFO [UMLS] loading concept-text file: noteevents_base_jaccard_0.9_w6.pkl
Traceback (most recent call last):
  File "dev/umls_graph_extraction.py", line 227, in <module>
    main()
  File "dev/umls_graph_extraction.py", line 168, in main
    text_cuis = get_cuis_from_text_concept_raw_data(
  File "/home/zwt/CS598-DL4H-Final-Project/mmugl-main/kg/kg/data/umls.py", line 872, in get_cuis_from_text_concept_raw_data
    with open(filepath, "rb") as file:
FileNotFoundError: [Errno 2] No such file or directory: 'data/noteevents_base_jaccard_0.9_w6.pkl'
ChatGPT said:
You're nearly done — the error is very clear:

FileNotFoundError: [Errno 2] No such file or directory: 'data/noteevents_base_jaccard_0.9_w6.pkl'

✅ Cause
You're passing this path in your script:

--text_concepts data/noteevents_base_jaccard_0.9_w6.pkl
But that file doesn't exist in the data/ folder — either:

It was created in a different directory, or

The earlier text_concept_extraction.py run used a different output name or folder.

🛠️ Fix
Option 1: Check where the actual .pkl file was saved.

Run:

find . -name "noteevents_base_jaccard_0.9_w6.pkl"
Then adjust your --text_concepts path accordingly.

Option 2: If you ran with --output data earlier, make sure the file is really there:

ls -lh data/noteevents_base_jaccard_0.9_w6.pkl
If it's not, rerun the concept extraction step:

python dev/text_concept_extraction.py \
  --output data \
  --name base \
  --workers 4 \
  --mimic_path ../MIMIC-III \
  --quickumls_path ../QuickUMLS \
  --window 6 \
  --threshold 0.9 \
  --similarity jaccard
Then try the graph script again.

Would you like help summarizing the final output structure of the graph after it’s generated?

You said:
mmugl-main/data/noteevents_test_run_jaccard_0.9_w6.pkl
ChatGPT said:
Perfect — your extracted concept file is named:

data/noteevents_test_run_jaccard_0.9_w6.pkl
✅ Updated Command
Just update the --text_concepts flag in your graph extraction command to match:

python dev/umls_graph_extraction.py \
  --output data/umls_graph.pkl \
  --workers 4 \
  --text_concepts data/noteevents_test_run_jaccard_0.9_w6.pkl \
  --use_categories \
  --icd_codes data/disease_codes_all.txt \
  --atc_codes data/med_codes_all.txt \
  --build_code_trees \
  --umls_path ../UMLS \
  --grow_hops 0 \
  --reduce_vocabularies \
  --add_sab \
  --add_sapbert
That should run without error and generate the knowledge graph as data/umls_graph.pkl.

Would you like to see how to inspect that graph once it’s built (e.g., node types, sizes, degrees)?

You said:
This should store the extracted graph as a networkx object in $PROJECT/data/umls_graph.pkl.

Training
For training we use the script run_training_concepts.py in ./dev. There are many options and hyperparameters. A recommended configuration can be found /dev/examples.

You can run training for a diagnosis downstream task by copying the example diagnosis_training_example.sh to the ./dev directory, editing the shell variables at the top of the script with the right paths and then running the following command. Pretraining is performed within the same script ensure the flag --pretrain is set. All of the following examples train MMUGL as referred to in our paper (including medical concepts from text i.e. clinical reports).



python run_training_concepts.py \
    --name diagnosis \
    --logdir $PROJECT/dev/runs \
    --data_dir $PROJECT/data \
    --code_mappings $PROJECT/data/code_mappings \
    --epochs 1000 \
    --num_workers 4 \
    --grad_acc 4 \
    --batch_size 16 \
    --down_batch_size 2 \
    --down_grad_acc 16 \
    --down_learning_rate 0.0001 \
    --down_es 25 \
    --embedding_dim 255 \
    --graph_num_layers 3 \
    --graph_num_filters 2 \
    --gnn_operator SAGEConv \
    --attention_heads 2 \
    --feedforward_dim 512 \
    --num_blocks 1 \
    --agg_mask_loss 0.25 \
    --verbose \
    --mlp_dim 128 \
    --mlp_num_layers 2 \
    --freeze_embeddings \
    --store_pretrainmodel \
    --store_downmodel \
    --pretrain \
    --pretrain_es loss_total \
    --pretrain_patience 12 \
    --split_mode cgl \
    --icd_codes $PROJECT/data/disease_codes_all.txt \
    --down_icd_codes $PROJECT/data/cgl_mimic3_disease_codes.txt \
    --atc_codes $PROJECT/data/med_codes_all.txt \
    --diagnosis_csv $MIMIC_DIR/DIAGNOSES_ICD.csv \
    --prescription_csv $MIMIC_DIR/PRESCRIPTIONS.csv \
    --admission_csv $MIMIC_DIR/ADMISSIONS.csv \
    --umls_graph $UMLS_GRAPH \
    --notes_concepts $NOTES_CONCEPTS \
    --max_visit_text_length 1279 \
    --downstream_cgl \
    --cgl_num_contexts 4 \
    --cgl_task diagnosis \
    --down_mlp_dim 0 \
    --down_mlp_num_layers 0 \
    --random_state 1111
ChatGPT said:
You're now ready for the training stage of MMUGL, which builds on everything you've preprocessed:

✅ What You Need to Edit
Update the following variables in your shell or command:

🔁 Replace:
Placeholder	With...
$PROJECT	The path to your project directory, e.g., /home/zwt/CS598-DL4H-Final-Project/mmugl-main
$MIMIC_DIR	The path to your MIMIC-III CSVs (ADMISSIONS.csv, etc.)
$UMLS_GRAPH	Path to the output from your last step, e.g., data/umls_graph.pkl
$NOTES_CONCEPTS	Your .pkl file from concept extraction, e.g., data/noteevents_test_run_jaccard_0.9_w6.pkl
🧪 Suggested Minimal Debug Run
To test if training initializes correctly before committing to 1000 epochs, you can do a dry-run-style test with --epochs 2 and smaller batches:

python dev/run_training_concepts.py \
  --name diagnosis_test \
  --logdir dev/runs \
  --data_dir data \
  --code_mappings data/code_mappings \
  --epochs 2 \
  --num_workers 2 \
  --grad_acc 1 \
  --batch_size 4 \
  --down_batch_size 2 \
  --down_grad_acc 2 \
  --down_learning_rate 0.0001 \
  --down_es 5 \
  --embedding_dim 128 \
  --graph_num_layers 1 \
  --graph_num_filters 1 \
  --gnn_operator SAGEConv \
  --attention_heads 1 \
  --feedforward_dim 128 \
  --num_blocks 1 \
  --agg_mask_loss 0.25 \
  --mlp_dim 64 \
  --mlp_num_layers 1 \
  --freeze_embeddings \
  --store_pretrainmodel \
  --store_downmodel \
  --pretrain \
  --pretrain_es loss_total \
  --pretrain_patience 3 \
  --split_mode cgl \
  --icd_codes data/disease_codes_all.txt \
  --down_icd_codes data/cgl_mimic3_disease_codes.txt \
  --atc_codes data/med_codes_all.txt \
  --diagnosis_csv ../MIMIC-III/DIAGNOSES_ICD.csv \
  --prescription_csv ../MIMIC-III/PRESCRIPTIONS.csv \
  --admission_csv ../MIMIC-III/ADMISSIONS.csv \
  --umls_graph data/umls_graph.pkl \
  --notes_concepts data/noteevents_test_run_jaccard_0.9_w6.pkl \
  --max_visit_text_length 1279 \
  --downstream_cgl \
  --cgl_num_contexts 4 \
  --cgl_task diagnosis \
  --down_mlp_dim 0 \
  --down_mlp_num_layers 0 \
  --random_state 1111 \
  --verbose
✅ Tips Before Running Full Training
Make sure data/code_mappings exists (used for mapping ICD codes to CUIs)

Verify GPU usage (nvidia-smi) if you want faster performance

Logs and TensorBoard summaries go to dev/runs/diagnosis_test/

Would you like help interpreting logs, checking for GPU usage, or identifying expected outputs from training?

You said:
I need help fixing code that is mean to do model training. Here is the issue I am encountering
(mmugl) zwt@zwt-MS-7D86:~/CS598-DL4H-Final-Project/mmugl-main$ python dev/run_training_concepts.py \
  --name diagnosis_test \
  --logdir dev/runs \
  --data_dir data \
  --code_mappings data/code_mappings \
  --epochs 2 \
  --num_workers 2 \
  --grad_acc 1 \
  --batch_size 4 \
  --down_batch_size 2 \
  --down_grad_acc 2 \
  --down_learning_rate 0.0001 \
  --down_es 5 \
  --embedding_dim 128 \
  --graph_num_layers 1 \
  --graph_num_filters 1 \
  --gnn_operator SAGEConv \
  --attention_heads 1 \
  --feedforward_dim 128 \
  --num_blocks 1 \
  --agg_mask_loss 0.25 \
  --mlp_dim 64 \
  --mlp_num_layers 1 \
  --freeze_embeddings \
  --store_pretrainmodel \
  --store_downmodel \
  --pretrain \
  --pretrain_es loss_total \
  --pretrain_patience 3 \
  --split_mode cgl \
  --icd_codes data/disease_codes_all.txt \
  --down_icd_codes data/cgl_mimic3_disease_codes.txt \
  --atc_codes data/med_codes_all.txt \
  --diagnosis_csv ../MIMIC-III/DIAGNOSES_ICD.csv \
  --prescription_csv ../MIMIC-III/PRESCRIPTIONS.csv \
  --admission_csv ../MIMIC-III/ADMISSIONS.csv \
  --umls_graph data/umls_graph.pkl \
  --notes_concepts data/noteevents_test_run_jaccard_0.9_w6.pkl \
  --max_visit_text_length 1279 \
  --downstream_cgl \
  --cgl_num_contexts 4 \
  --cgl_task diagnosis \
  --down_mlp_dim 0 \
  --down_mlp_num_layers 0 \
  --random_state 1111 \
  --verbose
zwt-MS-7D86 root[83135] INFO ========================================
zwt-MS-7D86 root[83135] INFO Start Training script
zwt-MS-7D86 root[83135] INFO Host: zwt-MS-7D86
zwt-MS-7D86 root[83135] INFO Torch device: NVIDIA GeForce RTX 4090
zwt-MS-7D86 root[83135] INFO GPU Memory: 23.5 GB
zwt-MS-7D86 root[83135] INFO Workers: 2
zwt-MS-7D86 root[83135] INFO ========================================
Global seed set to 1111
zwt-MS-7D86 root[83135] INFO Load UMLS graph from: data/umls_graph.pkl
zwt-MS-7D86 root[83135] INFO [ICD] Processing diagnosis table
zwt-MS-7D86 root[83135] INFO [ICD] Table raw shape: (651047, 5)
zwt-MS-7D86 root[83135] INFO [ICD] Table processed shape: (650940, 3)
zwt-MS-7D86 root[83135] INFO [SPLIT cgl] train suggested: 6000
zwt-MS-7D86 root[83135] INFO [SPLIT cgl] train loaded: 45171
zwt-MS-7D86 root[83135] INFO [SPLIT cgl] intersect: 6000
zwt-MS-7D86 root[83135] INFO [SPLIT] Created data splits, train: 45171, val: 125
zwt-MS-7D86 root[83135] INFO Parsed 6984 diagnosis codes from provided file
zwt-MS-7D86 root[83135] INFO Parsed 348 prescription codes from provided file
zwt-MS-7D86 root[83135] INFO [DATA]----- TRAIN -----
zwt-MS-7D86 root[83135] INFO [ICD] Processing diagnosis table
zwt-MS-7D86 root[83135] INFO [ICD] Table raw shape: (651047, 5)
zwt-MS-7D86 root[83135] INFO [ICD] Table processed shape: (650940, 3)
zwt-MS-7D86 root[83135] INFO [ICD] Filtered patient ids, filtered shape: (609110, 3)
zwt-MS-7D86 root[83135] INFO [CHECK 1] patient ids: 45171
zwt-MS-7D86 root[83135] INFO [ICD] Remove non-vocabulary (size: 6984) codes, post: (609110, 3)
zwt-MS-7D86 root[83135] INFO [CHECK 2] patient ids: 45171
zwt-MS-7D86 root[83135] INFO [ATC] Processing prescriptions table
/home/zwt/CS598-DL4H-Final-Project/mmugl-main/kg/kg/data/processing.py:187: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.
  med_pd = pd.read_csv(file, dtype=ndc_types)
zwt-MS-7D86 root[83135] INFO [ATC] Table raw shape: (4156450, 19)
zwt-MS-7D86 root[83135] INFO [ATC] Preprocessed shape: (2987420, 5)
Traceback (most recent call last):
  File "/home/zwt/CS598-DL4H-Final-Project/mmugl-main/kg/kg/data/datasets.py", line 369, in __init__
    self.med_pd = pd.read_pickle(
  File "/home/zwt/miniconda3/envs/mmugl/lib/python3.8/site-packages/pandas/io/pickle.py", line 187, in read_pickle
    with get_handle(
  File "/home/zwt/miniconda3/envs/mmugl/lib/python3.8/site-packages/pandas/io/common.py", line 795, in get_handle
    handle = open(handle, ioargs.mode)
FileNotFoundError: [Errno 2] No such file or directory: 'data/code_mappings/preprocessed_mimic_tables/prescriptions_atc_preprocessed.pkl'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "dev/run_training_concepts.py", line 822, in <module>
    main()
  File "dev/run_training_concepts.py", line 336, in main
    train_dataset = CodeDataset(
  File "/home/zwt/CS598-DL4H-Final-Project/mmugl-main/kg/kg/data/datasets.py", line 389, in __init__
    self.med_pd = map_ndc_atc4_codes(
  File "/home/zwt/CS598-DL4H-Final-Project/mmugl-main/kg/kg/data/processing.py", line 249, in map_ndc_atc4_codes
    rxnorm2atc = pd.read_csv(rxnorm2atc_file)
  File "/home/zwt/miniconda3/envs/mmugl/lib/python3.8/site-packages/pandas/util/_decorators.py", line 311, in wrapper
    return func(*args, **kwargs)
  File "/home/zwt/miniconda3/envs/mmugl/lib/python3.8/site-packages/pandas/io/parsers/readers.py", line 678, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "/home/zwt/miniconda3/envs/mmugl/lib/python3.8/site-packages/pandas/io/parsers/readers.py", line 575, in _read
    parser = TextFileReader(filepath_or_buffer, **kwds)
  File "/home/zwt/miniconda3/envs/mmugl/lib/python3.8/site-packages/pandas/io/parsers/readers.py", line 932, in __init__
    self._engine = self._make_engine(f, self.engine)
  File "/home/zwt/miniconda3/envs/mmugl/lib/python3.8/site-packages/pandas/io/parsers/readers.py", line 1216, in _make_engine
    self.handles = get_handle(  # type: ignore[call-overload]
  File "/home/zwt/miniconda3/envs/mmugl/lib/python3.8/site-packages/pandas/io/common.py", line 786, in get_handle
    handle = open(
FileNotFoundError: [Errno 2] No such file or directory: 'data/code_mappings/ndc2atc_level4.csv'
ChatGPT said:
You're running the training correctly, and the logs confirm your environment and data are in good shape so far. The training is now failing because of two missing input files required for mapping medication codes:

❌ Missing Files
data/code_mappings/preprocessed_mimic_tables/prescriptions_atc_preprocessed.pkl

data/code_mappings/ndc2atc_level4.csv

These are needed for:

Mapping raw NDC codes in prescriptions to ATC codes.

Enabling the model to associate medications with standard vocabularies in the UMLS graph.

✅ Fix Options
Option A: Generate Missing Files
Check if the repo has a preprocessing script (likely in dev/) to generate those files. Look for something like:

preprocess_mimic.py

generate_code_mappings.py

Or a shell script that runs end-to-end preprocessing

If not obvious, you can generate ndc2atc_level4.csv yourself if you have access to RxNorm-to-ATC mapping data.

Option B: Minimal Run Without Medications
If you're testing and want to skip drug info entirely for now, try removing these arguments:

--atc_codes data/med_codes_all.txt
--prescription_csv ../MIMIC-III/PRESCRIPTIONS.csv
That should disable all ATC-related processing. However, make sure the code gracefully handles the absence of meds (sometimes not the case).

Option C: Manually Stub Missing Files (for testing)
Create dummy versions just to get past file checks:

mkdir -p data/code_mappings/preprocessed_mimic_tables

# Empty dummy DataFrame for pickle
python -c "import pandas as pd; pd.DataFrame().to_pickle('data/code_mappings/preprocessed_mimic_tables/prescriptions_atc_preprocessed.pkl')"

# Dummy CSV for ndc2atc
echo "ndc,atc4" > data/code_mappings/ndc2atc_level4.csv
⚠️ This will let the code proceed but won’t give meaningful model performance — just useful for pipeline validation.

✅ Next Steps
Would you like me to help:

Recreate a small ndc2atc_level4.csv based on public mappings?

Check whether the repo has a script to generate the .pkl and .csv files?

Guide you on temporarily modifying the code to bypass medication handling?

Let me know your preference.

You said:
I am running into the following issue in my training loop:

(mmugl) zwt@zwt-MS-7D86:~/CS598-DL4H-Final-Project/mmugl-main$ python dev/run_training_concepts.py   --name diagnosis_test   --logdir dev/runs   --data_dir data   --code_mappings data/code_mappings   --epochs 2   --num_workers 2   --grad_acc 1   --batch_size 4   --down_batch_size 2   --down_grad_acc 2   --down_learning_rate 0.0001   --down_es 5   --embedding_dim 128   --graph_num_layers 1   --graph_num_filters 1   --gnn_operator SAGEConv   --attention_heads 1   --feedforward_dim 128   --num_blocks 1   --agg_mask_loss 0.25   --mlp_dim 64   --mlp_num_layers 1   --freeze_embeddings   --store_pretrainmodel   --store_downmodel   --pretrain   --pretrain_es loss_total   --pretrain_patience 3   --split_mode cgl   --icd_codes data/disease_codes_all.txt   --down_icd_codes data/cgl_mimic3_disease_codes.txt   --atc_codes data/med_codes_all.txt   --diagnosis_csv ../MIMIC-III/DIAGNOSES_ICD.csv   --prescription_csv ../MIMIC-III/PRESCRIPTIONS.csv   --admission_csv ../MIMIC-III/ADMISSIONS.csv   --umls_graph data/umls_graph.pkl   --notes_concepts data/noteevents_test_run_jaccard_0.9_w6.pkl   --max_visit_text_length 1279   --downstream_cgl   --cgl_num_contexts 4   --cgl_task diagnosis   --down_mlp_dim 0   --down_mlp_num_layers 0   --random_state 1111   --verbose
2025-05-07 01:27:54 zwt-MS-7D86 root[83504] INFO 

Traceback (most recent call last):
  File "dev/run_training_concepts.py", line 822, in <module>
    main()
  File "dev/run_training_concepts.py", line 336, in main
    train_dataset = CodeDataset(
  File "/home/zwt/CS598-DL4H-Final-Project/mmugl-main/kg/kg/data/datasets.py", line 486, in __init__
    (tokenized_concepts, max_visit_text_length,) = load_and_preprocess_notes_concepts(
  File "/home/zwt/CS598-DL4H-Final-Project/mmugl-main/kg/kg/data/processing.py", line 764, in load_and_preprocess_notes_concepts
    umls_tokenizer.convert_tokens_to_ids(flat_text_codes), dtype=np.int64
  File "/home/zwt/CS598-DL4H-Final-Project/mmugl-main/kg/kg/data/vocabulary.py", line 86, in convert_tokens_to_ids
    return list(map(lambda token: self.vocabulary.word2idx[token], tokens))
  File "/home/zwt/CS598-DL4H-Final-Project/mmugl-main/kg/kg/data/vocabulary.py", line 86, in <lambda>
    return list(map(lambda token: self.vocabulary.word2idx[token], tokens))
KeyError: 'C0232257'
ChatGPT said:
You're very close — the pipeline is working through each stage, and now it's hitting a vocabulary mismatch error:

❌ KeyError: 'C0180153'
This means the UMLS CUI C0180153 (a medical concept from your notes) is not present in the word2idx vocabulary, which is used to map concepts into token IDs.

✅ What This Tells Us
Your concept extraction pipeline (noteevents_*.pkl) successfully extracted real CUIs.

But the vocabulary used in training (likely built from the UMLS graph) does not include all CUIs found in the notes.

This is common when:

Your text extraction is more comprehensive than your curated UMLS vocabulary.

You applied --reduce_vocabularies, which prunes the UMLS CUIs.

Some concepts in notes are not connected to ICD/ATC codes and were dropped during graph construction.

🔧 Options to Fix It
Option 1: Allow Unknown CUIs (Recommended for Debugging)
Modify the convert_tokens_to_ids function in vocabulary.py to use a default ID (e.g., <unk>) when a token is not found:

def convert_tokens_to_ids(self, tokens):
    return [self.vocabulary.word2idx.get(token, self.vocabulary.word2idx["<unk>"]) for token in tokens]
✅ This will prevent crashing and allow training to continue, with unknown concepts mapped to a special token.

Option 2: Rebuild Graph with Larger Vocabulary
Remove the --reduce_vocabularies flag when running umls_graph_extraction.py. This keeps all CUIs from the original UMLS dump:

python dev/umls_graph_extraction.py \
  ... \
  --no-reduce_vocabularies
⚠️ Larger graph = longer training and memory usage. Use only if you want high coverage.

Option 3: Filter Notes CUIs Before Tokenization
Update load_and_preprocess_notes_concepts() (in processing.py, line ~764) to skip CUIs not found in word2idx.

filtered_tokens = [token for token in flat_text_codes if token in umls_tokenizer.vocabulary.word2idx]
This drops unmatched CUIs instead of crashing. It’s useful when you want to keep your vocab tight but still process notes.

✅ What You Should Do Right Now
The quickest fix is Option 1: patch the convert_tokens_to_ids() to use get() with a fallback:

lambda token: self.vocabulary.word2idx.get(token, self.vocabulary.word2idx["<unk>"])
Let me know if you want the exact lines to patch or help locating the <unk> token setup (usually added in vocabulary init).

You said:
(mmugl) zwt@zwt-MS-7D86:~/CS598-DL4H-Final-Project/mmugl-main$ python dev/run_training_concepts.py   --name diagnosis_test   --logdir dev/runs   --data_dir data   --code_mappings data/code_mappings   --epochs 2   --num_workers 2   --grad_acc 1   --batch_size 4   --down_batch_size 2   --down_grad_acc 2   --down_learning_rate 0.0001   --down_es 5   --embedding_dim 128   --graph_num_layers 1   --graph_num_filters 1   --gnn_operator SAGEConv   --attention_heads 1   --feedforward_dim 128   --num_blocks 1   --agg_mask_loss 0.25   --mlp_dim 64   --mlp_num_layers 1   --freeze_embeddings   --store_pretrainmodel   --store_downmodel   --pretrain   --pretrain_es loss_total   --pretrain_patience 3   --split_mode cgl   --icd_codes data/disease_codes_all.txt   --down_icd_codes data/cgl_mimic3_disease_codes.txt   --atc_codes data/med_codes_all.txt   --diagnosis_csv ../MIMIC-III/DIAGNOSES_ICD.csv   --prescription_csv ../MIMIC-III/PRESCRIPTIONS.csv   --admission_csv ../MIMIC-III/ADMISSIONS.csv   --umls_graph data/umls_graph.pkl   --notes_concepts data/noteevents_test_run_jaccard_0.9_w6.pkl   --max_visit_text_length 1279   --downstream_cgl   --cgl_num_contexts 4   --cgl_task diagnosis   --down_mlp_dim 0   --down_mlp_num_layers 0   --random_state 1111   --verbose
zwt-MS-7D86 root[83504] INFO ========================================
zwt-MS-7D86 root[83504] INFO Start Training script
zwt-MS-7D86 root[83504] INFO Host: zwt-MS-7D86
zwt-MS-7D86 root[83504] INFO Torch device: NVIDIA GeForce RTX 4090
zwt-MS-7D86 root[83504] INFO GPU Memory: 23.5 GB
zwt-MS-7D86 root[83504] INFO Workers: 2
zwt-MS-7D86 root[83504] INFO ========================================
Global seed set to 1111
zwt-MS-7D86 root[83504] INFO Load UMLS graph from: data/umls_graph.pkl
zwt-MS-7D86 root[83504] INFO [ICD] Processing diagnosis table
zwt-MS-7D86 root[83504] INFO [ICD] Table raw shape: (651047, 5)
zwt-MS-7D86 root[83504] INFO [ICD] Table processed shape: (650940, 3)
zwt-MS-7D86 root[83504] INFO [SPLIT cgl] train suggested: 6000
zwt-MS-7D86 root[83504] INFO [SPLIT cgl] train loaded: 45171
zwt-MS-7D86 root[83504] INFO [SPLIT cgl] intersect: 6000
zwt-MS-7D86 root[83504] INFO [SPLIT] Created data splits, train: 45171, val: 125
zwt-MS-7D86 root[83504] INFO Parsed 6984 diagnosis codes from provided file
zwt-MS-7D86 root[83504] INFO Parsed 348 prescription codes from provided file
zwt-MS-7D86 root[83504] INFO [DATA]----- TRAIN -----
zwt-MS-7D86 root[83504] INFO [ICD] Processing diagnosis table
zwt-MS-7D86 root[83504] INFO [ICD] Table raw shape: (651047, 5)
zwt-MS-7D86 root[83504] INFO [ICD] Table processed shape: (650940, 3)
zwt-MS-7D86 root[83504] INFO [ICD] Filtered patient ids, filtered shape: (609110, 3)
zwt-MS-7D86 root[83504] INFO [CHECK 1] patient ids: 45171
zwt-MS-7D86 root[83504] INFO [ICD] Remove non-vocabulary (size: 6984) codes, post: (609110, 3)
zwt-MS-7D86 root[83504] INFO [CHECK 2] patient ids: 45171
zwt-MS-7D86 root[83504] INFO [ATC] Processing prescriptions table
/home/zwt/CS598-DL4H-Final-Project/mmugl-main/kg/kg/data/processing.py:187: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.
  med_pd = pd.read_csv(file, dtype=ndc_types)
zwt-MS-7D86 root[83504] INFO [ATC] Table raw shape: (4156450, 19)
zwt-MS-7D86 root[83504] INFO [ATC] Preprocessed shape: (2987420, 5)
zwt-MS-7D86 root[83504] INFO [ATC] Performed NDC -> RxNorm mapping, shape (2987420, 6)
zwt-MS-7D86 root[83504] INFO [ATC] Performed NDC -> ATC4 mapping, final shape (1988696, 5)
zwt-MS-7D86 root[83504] INFO Match (SUBJECT_ID, HADM_ID), diagnosis: (559813, 3), med: (1858182, 5)
zwt-MS-7D86 root[83504] INFO [CHECK 3] patient ids: 37819
zwt-MS-7D86 root[83504] INFO Grouping codes ICD9_CODE for (patients, admission), post shape (46958, 3)
zwt-MS-7D86 root[83504] INFO Grouping codes ATC4 for (patients, admission), post shape (46958, 3)
zwt-MS-7D86 root[83504] INFO [CHECK 4] patient ids: 37819
zwt-MS-7D86 root[83504] INFO Filtered patients to ICD9_CODE count range (1, inf), filtered shape (46958, 3)
zwt-MS-7D86 root[83504] INFO Filtered patients to ATC4 count range (1, inf), filtered shape (46958, 3)
zwt-MS-7D86 root[83504] INFO [CHECK 5] patient ids: 37819
zwt-MS-7D86 root[83504] INFO Merged diagnosis and prescription codes, shape: (46958, 4)
zwt-MS-7D86 root[83504] INFO [CHECK 6] patient ids: 37819
zwt-MS-7D86 root[83504] INFO Max #codes per visit, ICD: 39, ATC: 78
zwt-MS-7D86 root[83504] INFO  > configured: 63, final: 64
zwt-MS-7D86 root[83504] INFO [UMLS] UMLS graph data on input
zwt-MS-7D86 root[83504] INFO [CONCEPTS] Loading note concepts from: data/noteevents_test_run_jaccard_0.9_w6.pkl
zwt-MS-7D86 root[83504] INFO [CONCEPTS] 36 patients
zwt-MS-7D86 root[83504] INFO [CONCEPTS] maximum visit text length: 1279
zwt-MS-7D86 root[83504] INFO [CONCEPTS] Flattening and tokenizing notes text concepts
  0%|                                                                                                                                                                 | 0/36 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "dev/run_training_concepts.py", line 822, in <module>
    main()
  File "dev/run_training_concepts.py", line 336, in main
    train_dataset = CodeDataset(
  File "/home/zwt/CS598-DL4H-Final-Project/mmugl-main/kg/kg/data/datasets.py", line 486, in __init__
    (tokenized_concepts, max_visit_text_length,) = load_and_preprocess_notes_concepts(
  File "/home/zwt/CS598-DL4H-Final-Project/mmugl-main/kg/kg/data/processing.py", line 764, in load_and_preprocess_notes_concepts
    umls_tokenizer.convert_tokens_to_ids(flat_text_codes), dtype=np.int64
  File "/home/zwt/CS598-DL4H-Final-Project/mmugl-main/kg/kg/data/vocabulary.py", line 86, in convert_tokens_to_ids
    return list(map(lambda token: self.vocabulary.word2idx[token], tokens))
  File "/home/zwt/CS598-DL4H-Final-Project/mmugl-main/kg/kg/data/vocabulary.py", line 86, in <lambda>
    return list(map(lambda token: self.vocabulary.word2idx[token], tokens))
KeyError: 'C0232257'
ChatGPT said:
You're running into the same root issue as before: the tokenizer is attempting to map the extracted concept 'C0232257' to an index, but it's missing from the vocabulary (word2idx), leading to a crash.

✅ Immediate Fix: Add Safe Lookup with <unk> Token
Patch the line in vocabulary.py, likely around line 86, from:

return list(map(lambda token: self.vocabulary.word2idx[token], tokens))
To:

return list(map(lambda token: self.vocabulary.word2idx.get(token, self.vocabulary.word2idx.get("<unk>", 0)), tokens))
This tries to use an <unk> token index for unknown CUIs.

If <unk> is not in your vocabulary, it'll fall back to index 0.

🔍 Optional: Confirm <unk> Exists in Your Vocab
Check where the vocabulary is created (probably in the file that builds vocabulary.word2idx) and make sure it includes:

self.word2idx["<unk>"] = 0
If it doesn’t, just add it before training begins.

✅ Why This Is the Right Fix
You’re using CUIs extracted from notes.

But not all CUIs are in the UMLS subgraph (especially if you use --reduce_vocabularies).

This patch allows the model to proceed while treating unknown CUIs as a generic placeholder — standard NLP practice.

Let me know if you'd like help locating where the vocabulary is built so you can confirm <unk> is defined.

You said:
zwt-MS-7D86 root[83533] INFO Best F1 0.124, AuROC: 0.940, t: 0.01
zwt-MS-7D86 root[83533] INFO ----- Sample prediction -----
zwt-MS-7D86 root[83533] INFO Predict: 00845 03842 0388 0389 04104 04111 0413 0414 0417 04185 042 07054 07070 1120 1122 1629 1970 1977 1983 1985 2449 25000 25002 25040 25050 25060 25080 2639 2720 2724 2749 2760 2761 2762 2763 2765 27651 27652 2766 2767 2768 27800 27801 2800 2809 2841 2851 28521 28522 28529 2859 2866 2869 2875 28860 29181 2930 29410 2948 29680 30000 3004 30391 3051 311 32723 3310 3314 3320 33829 3453 34590 3481 34830 34839 3485 34982 3572 36201 3659 3970 4019 40390 40391 41071 412 41400 41401 4148 41519 4168 4239 4240 4241 4254 4271 42731 42732 4275 42789 4280 42821 42822 42823 42830 42832 42833 431 43310 4373 43811 43889 44024 4414 4439 45341 4538 45829 4589 45981 4821 48241 486 49121 4928 49320 49322 49390 496 5070 5119 5180 5185 51881 51882 51883 51884 51889 51919 53081 53550 5363 5533 5569 5570 5601 56210 56400 570 5712 5715 5723 5728 5770 5771 5781 5789 58381 5845 5849 5853 5856 5859 5990 5997 60000 6826 70703 70705 70714 70722 71590 7242 73300 7746 78039 7850 78551 78552 7863 78720 78791 78820 78959 7907 79092 79902 99591 99592 99662 99674 99681 9971 99811 99812 9982 99831 99859 E8780 E8781 E8782 E8788 E8798 E8889 E9320 E9331 E9342 V090 V1005 V1011 V103 V1046 V1052 V1082 V1251 V1254 V153 V1581 V1582 V422 V433 V4364 V4365 V440 V441 V4501 V4502 V4511 V4581 V4582 V4589 V4611 V462 V4986 V5861 V5865 V5867 V667 V707 V8741
zwt-MS-7D86 root[83533] INFO Target : 1977 1985 25000 2720 37900 4019 42731 49121 51881 5990 V1011 V103
zwt-MS-7D86 root[83533] INFO -----------------------------
zwt-MS-7D86 root[83533] INFO ========================================
zwt-MS-7D86 root[83533] INFO Finished Downstream Training
zwt-MS-7D86 root[83533] INFO Val F1      : 0.133
zwt-MS-7D86 root[83533] INFO Val F1 (inflated): 0.114
zwt-MS-7D86 root[83533] INFO Val AuPRC   : 0.940
zwt-MS-7D86 root[83533] INFO Val Thresh  : 0.01
zwt-MS-7D86 root[83533] INFO Val R@20    : 0.26
zwt-MS-7D86 root[83533] INFO Val R@40    : 0.36
zwt-MS-7D86 root[83533] INFO 
zwt-MS-7D86 root[83533] INFO Test F1      : 0.124
zwt-MS-7D86 root[83533] INFO Test F1 (inflated): 0.107
zwt-MS-7D86 root[83533] INFO Test AuPRC   : 0.940
zwt-MS-7D86 root[83533] INFO Test Thresh  : 0.01
zwt-MS-7D86 root[83533] INFO Test R@20    : 0.26
zwt-MS-7D86 root[83533] INFO Test R@40    : 0.37
zwt-MS-7D86 root[83533] INFO ========================================
(mmugl) zwt@zwt-MS-7D86:~/CS598-DL4H-Final-Project/mmugl-main$ 

How are these results?
ChatGPT said:
Great — your training ran successfully! Let's break down the evaluation metrics:

📊 Evaluation Summary
🔹 Validation (Val) Metrics
F1 Score: 0.133

F1 (inflated): 0.114

Area under Precision-Recall Curve (AuPRC): 0.940

Top-20 Recall (R@20): 0.26

Top-40 Recall (R@40): 0.36

🔹 Test Metrics
F1 Score: 0.124

F1 (inflated): 0.107

AuPRC: 0.940

R@20: 0.26

R@40: 0.37

🧠 Interpretation
Metric	Meaning
F1 Score	Harmonic mean of precision and recall. Your scores (~0.12) are modest.
F1 (inflated)	May refer to micro/macro averaging adjustments. Slightly lower here.
AuPRC = 0.94	Very good. Indicates high precision over a range of recall values.
Recall@20/40	Reasonable — you're retrieving ~26–37% of true labels in top 20/40.
✅ What’s Good
High AuPRC (0.94) is excellent, showing your model distinguishes well between relevant and irrelevant diagnoses.

The infrastructure is working correctly: graph, embeddings, notes, and label supervision all loaded and processed.

🚧 What Can Be Improved
F1 scores (~0.12) are relatively low. This is common with long-tailed medical code distributions and partial coverage of CUIs.

You only trained for 2 epochs — this is extremely short.

With --pretrain_patience 3, you're barely scratching the surface.

🔁 Next Step Recommendations
✅ Increase --epochs to 50 or more, and allow full --pretrain_patience.

🧪 Use more note concepts (your test run had just 36 patients).

🔍 Analyze which CUIs aren't in vocab (like 'C0232257') and decide to:

Grow the vocabulary

Or map them to <unk> consistently

🧠 Try different GNNs like GATConv, TransformerConv, or more layers.

Would you like help visualizing the learned embeddings or plotting the PR curves?

You said:
Is 50 epochs fine? Would more be suggested or overkill?
ChatGPT said:
Training for 50 epochs is a solid and commonly used starting point — not overkill in your case. Here's how to think about it:

🎯 Why 50 Epochs Is Reasonable
You're using a graph neural network (GNN) with pretraining, which often requires more epochs to converge than vanilla MLPs or shallow models.

Your current results at 2 epochs already show promise (e.g., high AuPRC), which implies more training can definitely refine the F1 and recall.

The script uses early stopping via --pretrain_patience, so it will automatically halt if there's no improvement — reducing overfitting risk.

📈 Suggested Strategy
Epochs	Purpose	Notes
10	Quick smoke test (what you just did)	Done
50	Standard medium-run	✅ Good next step
100–200	Long-run tuning (only if needed)	Only if you're getting diminishing returns at 50
If training time or memory becomes an issue, you can:

Use --checkpoint_epochs to monitor training mid-run.

Tune --pretrain_es (early stopping metric) or --pretrain_patience to exit earlier.

✅ Bottom Line:
Start with 50 epochs. If your validation F1/AuPRC flattens early, early stopping will save you. If not, it's likely needed to reach your model's full potential.

Would you like a command template with 50 epochs configured and logging/output directories cleaned up?

(mmugl) zwt@zwt-MS-7D86:~/CS598-DL4H-Final-Project/mmugl-main$ python dev/run_training_concepts.py   --name diagnosis_test   --logdir dev/runs   --data_dir data   --code_mappings data/code_mappings   --epochs 80   --num_workers 2   --grad_acc 1   --batch_size 4   --down_batch_size 2   --down_grad_acc 2   --down_learning_rate 0.0001   --down_es 5   --embedding_dim 128   --graph_num_layers 1   --graph_num_filters 1   --gnn_operator SAGEConv   --attention_heads 1   --feedforward_dim 128   --num_blocks 1   --agg_mask_loss 0.25   --mlp_dim 64   --mlp_num_layers 1   --freeze_embeddings   --store_pretrainmodel   --store_downmodel   --pretrain   --pretrain_es loss_total   --pretrain_patience 3   --split_mode cgl   --icd_codes data/disease_codes_all.txt   --down_icd_codes data/cgl_mimic3_disease_codes.txt   --atc_codes data/med_codes_all.txt   --diagnosis_csv ../MIMIC-III/DIAGNOSES_ICD.csv   --prescription_csv ../MIMIC-III/PRESCRIPTIONS.csv   --admission_csv ../MIMIC-III/ADMISSIONS.csv   --umls_graph data/umls_graph.pkl   --notes_concepts data/noteevents_test_run_jaccard_0.9_w6.pkl   --max_visit_text_length 1279   --downstream_cgl   --cgl_num_contexts 4   --cgl_task diagnosis   --down_mlp_dim 0   --down_mlp_num_layers 0   --random_state 1111   --verbose
zwt-MS-7D86 root[86735] INFO ========================================
zwt-MS-7D86 root[86735] INFO Start Training script
zwt-MS-7D86 root[86735] INFO Host: zwt-MS-7D86
zwt-MS-7D86 root[86735] INFO Torch device: NVIDIA GeForce RTX 4090
zwt-MS-7D86 root[86735] INFO GPU Memory: 23.5 GB
zwt-MS-7D86 root[86735] INFO Workers: 2
zwt-MS-7D86 root[86735] INFO ========================================
Global seed set to 1111
zwt-MS-7D86 root[86735] INFO Load UMLS graph from: data/umls_graph.pkl
zwt-MS-7D86 root[86735] INFO [ICD] Processing diagnosis table
zwt-MS-7D86 root[86735] INFO [ICD] Table raw shape: (651047, 5)
zwt-MS-7D86 root[86735] INFO [ICD] Table processed shape: (650940, 3)
zwt-MS-7D86 root[86735] INFO [SPLIT cgl] train suggested: 6000
zwt-MS-7D86 root[86735] INFO [SPLIT cgl] train loaded: 45171
zwt-MS-7D86 root[86735] INFO [SPLIT cgl] intersect: 6000
zwt-MS-7D86 root[86735] INFO [SPLIT] Created data splits, train: 45171, val: 125
zwt-MS-7D86 root[86735] INFO Parsed 6984 diagnosis codes from provided file
zwt-MS-7D86 root[86735] INFO Parsed 348 prescription codes from provided file
zwt-MS-7D86 root[86735] INFO [DATA]----- TRAIN -----
zwt-MS-7D86 root[86735] INFO [ICD] Processing diagnosis table
zwt-MS-7D86 root[86735] INFO [ICD] Table raw shape: (651047, 5)
zwt-MS-7D86 root[86735] INFO [ICD] Table processed shape: (650940, 3)
zwt-MS-7D86 root[86735] INFO [ICD] Filtered patient ids, filtered shape: (609110, 3)
zwt-MS-7D86 root[86735] INFO [CHECK 1] patient ids: 45171
zwt-MS-7D86 root[86735] INFO [ICD] Remove non-vocabulary (size: 6984) codes, post: (609110, 3)
zwt-MS-7D86 root[86735] INFO [CHECK 2] patient ids: 45171
zwt-MS-7D86 root[86735] INFO [ATC] Processing prescriptions table
/home/zwt/CS598-DL4H-Final-Project/mmugl-main/kg/kg/data/processing.py:187: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.
  med_pd = pd.read_csv(file, dtype=ndc_types)
zwt-MS-7D86 root[86735] INFO [ATC] Table raw shape: (4156450, 19)
zwt-MS-7D86 root[86735] INFO [ATC] Preprocessed shape: (2987420, 5)
zwt-MS-7D86 root[86735] INFO [ATC] Performed NDC -> RxNorm mapping, shape (2987420, 6)
zwt-MS-7D86 root[86735] INFO [ATC] Performed NDC -> ATC4 mapping, final shape (1988696, 5)
zwt-MS-7D86 root[86735] INFO Match (SUBJECT_ID, HADM_ID), diagnosis: (559813, 3), med: (1858182, 5)
zwt-MS-7D86 root[86735] INFO [CHECK 3] patient ids: 37819
zwt-MS-7D86 root[86735] INFO Grouping codes ICD9_CODE for (patients, admission), post shape (46958, 3)
zwt-MS-7D86 root[86735] INFO Grouping codes ATC4 for (patients, admission), post shape (46958, 3)
zwt-MS-7D86 root[86735] INFO [CHECK 4] patient ids: 37819
zwt-MS-7D86 root[86735] INFO Filtered patients to ICD9_CODE count range (1, inf), filtered shape (46958, 3)
zwt-MS-7D86 root[86735] INFO Filtered patients to ATC4 count range (1, inf), filtered shape (46958, 3)
zwt-MS-7D86 root[86735] INFO [CHECK 5] patient ids: 37819
zwt-MS-7D86 root[86735] INFO Merged diagnosis and prescription codes, shape: (46958, 4)
zwt-MS-7D86 root[86735] INFO [CHECK 6] patient ids: 37819
zwt-MS-7D86 root[86735] INFO Max #codes per visit, ICD: 39, ATC: 78
zwt-MS-7D86 root[86735] INFO  > configured: 63, final: 64
zwt-MS-7D86 root[86735] INFO [UMLS] UMLS graph data on input
zwt-MS-7D86 root[86735] INFO [CONCEPTS] Loading note concepts from: data/noteevents_test_run_jaccard_0.9_w6.pkl
zwt-MS-7D86 root[86735] INFO [CONCEPTS] 774 patients
zwt-MS-7D86 root[86735] INFO [CONCEPTS] maximum visit text length: 1279
zwt-MS-7D86 root[86735] INFO [CONCEPTS] Flattening and tokenizing notes text concepts
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 774/774 [00:00<00:00, 2628.85it/s]
zwt-MS-7D86 root[86735] INFO [CONCEPTS] Pretokenized notes concepts for 774 patients
zwt-MS-7D86 root[86735] INFO [DATASET] output text targets: False
zwt-MS-7D86 root[86735] INFO [DATA]----- VAL -----
zwt-MS-7D86 root[86735] INFO [ICD] Processing diagnosis table
zwt-MS-7D86 root[86735] INFO [ICD] Table raw shape: (651047, 5)
zwt-MS-7D86 root[86735] INFO [ICD] Table processed shape: (650940, 3)
zwt-MS-7D86 root[86735] INFO [ICD] Filtered patient ids, filtered shape: (3942, 3)
zwt-MS-7D86 root[86735] INFO [CHECK 1] patient ids: 125
zwt-MS-7D86 root[86735] INFO [ICD] Remove non-vocabulary (size: 6984) codes, post: (3942, 3)
zwt-MS-7D86 root[86735] INFO [CHECK 2] patient ids: 125
zwt-MS-7D86 root[86735] INFO [ATC] Processing prescriptions table
/home/zwt/CS598-DL4H-Final-Project/mmugl-main/kg/kg/data/processing.py:187: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.
  med_pd = pd.read_csv(file, dtype=ndc_types)
zwt-MS-7D86 root[86735] INFO [ATC] Table raw shape: (4156450, 19)
zwt-MS-7D86 root[86735] INFO [ATC] Preprocessed shape: (2987420, 5)
zwt-MS-7D86 root[86735] INFO [ATC] Performed NDC -> RxNorm mapping, shape (2987420, 6)
zwt-MS-7D86 root[86735] INFO [ATC] Performed NDC -> ATC4 mapping, final shape (1988696, 5)
zwt-MS-7D86 root[86735] INFO Match (SUBJECT_ID, HADM_ID), diagnosis: (3702, 3), med: (12325, 5)
zwt-MS-7D86 root[86735] INFO [CHECK 3] patient ids: 119
zwt-MS-7D86 root[86735] INFO Grouping codes ICD9_CODE for (patients, admission), post shape (289, 3)
zwt-MS-7D86 root[86735] INFO Grouping codes ATC4 for (patients, admission), post shape (289, 3)
zwt-MS-7D86 root[86735] INFO [CHECK 4] patient ids: 119
zwt-MS-7D86 root[86735] INFO Filtered patients to ICD9_CODE count range (1, inf), filtered shape (289, 3)
zwt-MS-7D86 root[86735] INFO Filtered patients to ATC4 count range (1, inf), filtered shape (289, 3)
zwt-MS-7D86 root[86735] INFO [CHECK 5] patient ids: 119
zwt-MS-7D86 root[86735] INFO Merged diagnosis and prescription codes, shape: (289, 4)
zwt-MS-7D86 root[86735] INFO [CHECK 6] patient ids: 119
zwt-MS-7D86 root[86735] INFO Use existing tokenizer
zwt-MS-7D86 root[86735] INFO Max #codes per visit, ICD: 39, ATC: 48
zwt-MS-7D86 root[86735] INFO  > configured: 63, final: 64
zwt-MS-7D86 root[86735] INFO [UMLS] UMLS graph data on input
zwt-MS-7D86 root[86735] INFO [CONCEPTS] Loading note concepts from: data/noteevents_test_run_jaccard_0.9_w6.pkl
zwt-MS-7D86 root[86735] INFO [CONCEPTS] 774 patients
zwt-MS-7D86 root[86735] INFO [CONCEPTS] maximum visit text length: 1279
zwt-MS-7D86 root[86735] INFO [CONCEPTS] Flattening and tokenizing notes text concepts
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 774/774 [00:00<00:00, 2551.82it/s]
zwt-MS-7D86 root[86735] INFO [CONCEPTS] Pretokenized notes concepts for 774 patients
zwt-MS-7D86 root[86735] INFO [DATASET] output text targets: False
zwt-MS-7D86 root[86735] INFO [NN] Training with text: True, targets: False
zwt-MS-7D86 root[86735] INFO [NN] cache graph forward pass: True
zwt-MS-7D86 root[86735] INFO [NN] UMLS graph embedding module
zwt-MS-7D86 root[86735] INFO [UMLS] loaded graph data, hops: 0
zwt-MS-7D86 root[86735] INFO [GRAPH] using SapBert embeddings as initialization
zwt-MS-7D86 root[86735] INFO [UMLS] use provided CUI vocabulary: 23864
zwt-MS-7D86 root[86735] INFO [UMLS] loaded 99064 edges
zwt-MS-7D86 root[86735] INFO [UMLS] Built SapBERT feature matrix: torch.Size([23864, 768])
zwt-MS-7D86 root[86735] INFO UMLSGraphEmbedding has 23864 nodes
zwt-MS-7D86 root[86735] INFO [GRAPH] Node aggregation: sum
zwt-MS-7D86 root[86735] INFO Built: UMLSGNN 1 layers, 1 filters: SAGEConv:128
zwt-MS-7D86 root[86735] INFO [EMBEDDING] Final embedding size: (3, 23861) = 23864
zwt-MS-7D86 root[86735] INFO [INIT] DONT initialize co node embeddings
zwt-MS-7D86 root[86735] INFO [NN] Initializing heads for aggregation loss
zwt-MS-7D86 root[86735] INFO ----- Model -----
zwt-MS-7D86 root[86735] INFO Embedding dimension: 128
zwt-MS-7D86 root[86735] INFO GNN operator: SAGEConv
zwt-MS-7D86 root[86735] INFO Graph layers: 1, staged: False
zwt-MS-7D86 root[86735] INFO Transformer heads: 1, mlp: 128, blocks: 1
zwt-MS-7D86 root[86735] INFO MLP dim: 64, layers: 1
zwt-MS-7D86 root[86735] INFO Masked loss alpha: 0.25
zwt-MS-7D86 root[86735] INFO Hungarian set loss alpha: 0.0
zwt-MS-7D86 root[86735] INFO Co-occurrence loss alpha: 0.0
zwt-MS-7D86 root[86735] INFO co-nodes latent space: False
zwt-MS-7D86 root[86735] INFO Decoder Network: False
zwt-MS-7D86 root[86735] INFO ----- ----- -----
zwt-MS-7D86 root[86735] INFO Tensorboard logs at: dev/runs/diagnosis_test
zwt-MS-7D86 root[86735] INFO ========================================
zwt-MS-7D86 root[86735] INFO Run Training for 80 epochs
zwt-MS-7D86 root[86735] INFO Num batches: 11740, size: 4
zwt-MS-7D86 root[86735] INFO Val batches: 73
zwt-MS-7D86 root[86735] INFO Learning rate: 0.0005
zwt-MS-7D86 root[86735] INFO ========================================
zwt-MS-7D86 root[86735] INFO [TRAIN] early stopping on val/loss_total, patience 3
zwt-MS-7D86 root[86735] INFO [TRAINER] Sampling 4 triplets per anchor id
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name           | Type                            | Params
-------------------------------------------------------------------
0 | pretrain_model | JointCodePretrainingTransformer | 20.3 M
1 | loss           | BCEWithLogitsLoss               | 0     
-------------------------------------------------------------------
1.7 M     Trainable params
18.5 M    Non-trainable params
20.3 M    Total params
81.085    Total estimated model params size (MB)
Sanity Checking: 0it [00:00, ?it/s]/home/zwt/miniconda3/envs/mmugl/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the num_workers argument (try 32 which is the number of cpus on this machine) in the DataLoader init to improve performance.
  rank_zero_warn(
/home/zwt/miniconda3/envs/mmugl/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the num_workers argument (try 32 which is the number of cpus on this machine) in the DataLoader init to improve performance.
  rank_zero_warn(
Epoch 37: 100%|████████████████████████████████████████████████████████| 11813/11813 [01:23<00:00, 141.46it/s, loss=0.175, v_num=2, train/batch_loss=0.250, train/loss=0.173, val/loss=0.187, val/loss_total=0.187]
zwt-MS-7D86 root[86735] INFO Load best validation loss model: dev/runs/tb/version_2/checkpoints/epoch=31-step=375680.ckpt                                                                      
zwt-MS-7D86 root[86735] INFO Missing keys: []
zwt-MS-7D86 root[86735] INFO Unexpected keys: []
zwt-MS-7D86 root[86735] INFO Store pretrain model to dev/runs/model
zwt-MS-7D86 root[86735] INFO Running final validation
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 73/73 [00:00<00:00, 175.14it/s]
zwt-MS-7D86 root[86735] INFO [d2d] jaccard: 0.289 f1: 0.421 prauc: 0.538 
zwt-MS-7D86 root[86735] INFO [p2d] jaccard: 0.073 f1: 0.123 prauc: 0.266 
zwt-MS-7D86 root[86735] INFO [p2p] jaccard: 0.778 f1: 0.871 prauc: 0.933 
zwt-MS-7D86 root[86735] INFO [d2p] jaccard: 0.423 f1: 0.583 prauc: 0.688 
zwt-MS-7D86 root[86735] INFO ========================================
zwt-MS-7D86 root[86735] INFO Finished Pretraining Training
zwt-MS-7D86 root[86735] INFO Trained for 38 epochs
zwt-MS-7D86 root[86735] INFO Val Jaccard       : 0.391
zwt-MS-7D86 root[86735] INFO Val F1            : 0.499
zwt-MS-7D86 root[86735] INFO Val AuPRC         : 0.606
zwt-MS-7D86 root[86735] INFO ========================================
zwt-MS-7D86 root[86735] INFO Layer: UMLSGraphEmbedding(
  (cui_feature_projector): Linear(in_features=768, out_features=128, bias=True)
  (gnn): UMLSGNN(
    (graph_convs): ModuleList(
      (0): ModuleList(
        (0): HeteroConv(num_relations=1)
      )
    )
    (pool_stacks): ModuleList()
  )
) frozen, (8 params frozen)
zwt-MS-7D86 root[86735] INFO Set downstream batchsize to 2
zwt-MS-7D86 root[86735] INFO 
zwt-MS-7D86 root[86735] INFO ========================================
zwt-MS-7D86 root[86735] INFO Performing downstream DISEASE training
zwt-MS-7D86 root[86735] INFO Task: diagnosis
zwt-MS-7D86 root[86735] INFO With text: True
zwt-MS-7D86 root[86735] INFO eICU: False
zwt-MS-7D86 root[86735] INFO ========================================
zwt-MS-7D86 root[86735] INFO [CGL] Parsed 4825 diagnosis codes from provided file
zwt-MS-7D86 root[86735] INFO [DATA]----- TRAIN -----
zwt-MS-7D86 root[86735] WARNING [DATASET] keeping no med patients: True
zwt-MS-7D86 root[86735] INFO [ICD] Processing diagnosis table
zwt-MS-7D86 root[86735] INFO [ICD] Table raw shape: (651047, 5)
zwt-MS-7D86 root[86735] INFO [ICD] Table processed shape: (650940, 3)
zwt-MS-7D86 root[86735] INFO [ICD] Filtered patient ids, filtered shape: (609110, 3)
zwt-MS-7D86 root[86735] INFO [CHECK 1] patient ids: 45171
zwt-MS-7D86 root[86735] INFO [ICD] Remove non-vocabulary (size: 6984) codes, post: (609110, 3)
zwt-MS-7D86 root[86735] INFO [CHECK 2] patient ids: 45171
zwt-MS-7D86 root[86735] INFO [ATC] Processing prescriptions table
/home/zwt/CS598-DL4H-Final-Project/mmugl-main/kg/kg/data/processing.py:187: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.
  med_pd = pd.read_csv(file, dtype=ndc_types)
zwt-MS-7D86 root[86735] INFO [ATC] Table raw shape: (4156450, 19)
zwt-MS-7D86 root[86735] INFO [ATC] Preprocessed shape: (2987420, 5)
zwt-MS-7D86 root[86735] INFO [ATC] Performed NDC -> RxNorm mapping, shape (2987420, 6)
zwt-MS-7D86 root[86735] INFO [ATC] Performed NDC -> ATC4 mapping, final shape (1988696, 5)
zwt-MS-7D86 root[86735] INFO Filtered visit range (1, inf), final shape: (1988696, 5)
zwt-MS-7D86 root[86735] INFO [ATC] Filtered patient ids, filtered shape: (1858217, 5)
zwt-MS-7D86 root[86735] INFO [CHECK 3] patient ids: 45171
zwt-MS-7D86 root[86735] INFO Grouping codes ICD9_CODE for (patients, admission), post shape (55610, 3)
zwt-MS-7D86 root[86735] INFO Grouping codes ATC4 for (patients, admission), post shape (46962, 3)
zwt-MS-7D86 root[86735] INFO [CHECK 4] patient ids: 45171
zwt-MS-7D86 root[86735] INFO Filtered patients to ICD9_CODE count range (1, inf), filtered shape (55610, 3)
zwt-MS-7D86 root[86735] INFO Filtered patients to ATC4 count range (1, inf), filtered shape (46962, 3)
zwt-MS-7D86 root[86735] INFO [CHECK 5] patient ids: 45171
zwt-MS-7D86 root[86735] INFO Merged diagnosis and prescription codes, shape: (55614, 4)
zwt-MS-7D86 root[86735] INFO [CHECK 6] patient ids: 45171
zwt-MS-7D86 root[86735] INFO [ATC] imputing patients without prescriptions
zwt-MS-7D86 root[86735] INFO Use existing tokenizer
zwt-MS-7D86 root[86735] INFO Max #codes per visit, ICD: 39, ATC: 78
zwt-MS-7D86 root[86735] INFO  > configured: 63, final: 64
zwt-MS-7D86 root[86735] INFO [UMLS] UMLS graph data on input
zwt-MS-7D86 root[86735] INFO [CONCEPTS] Loading note concepts from: data/noteevents_test_run_jaccard_0.9_w6.pkl
zwt-MS-7D86 root[86735] INFO [CONCEPTS] 774 patients
zwt-MS-7D86 root[86735] INFO [CONCEPTS] maximum visit text length: 1279
zwt-MS-7D86 root[86735] INFO [CONCEPTS] Flattening and tokenizing notes text concepts
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 774/774 [00:00<00:00, 2577.76it/s]
zwt-MS-7D86 root[86735] INFO [CONCEPTS] Pretokenized notes concepts for 774 patients
zwt-MS-7D86 root[86735] INFO [DATASET] output text targets: False
zwt-MS-7D86 root[86735] INFO [TARGET] Considering distinct target code set
zwt-MS-7D86 root[86735] INFO [TARGET] Instatiating new target tokenizer, d: 4825
zwt-MS-7D86 root[86735] INFO Reformating for visit sequence learning based on: ../MIMIC-III/ADMISSIONS.csv
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 45171/45171 [00:06<00:00, 7208.88it/s]
zwt-MS-7D86 root[86735] INFO Reformated data for visit sequence learning, num records: 6156
zwt-MS-7D86 root[86735] INFO Maximum number of visits: 42
zwt-MS-7D86 root[86735] INFO [DATA]----- VAL -----
zwt-MS-7D86 root[86735] WARNING [DATASET] keeping no med patients: True
zwt-MS-7D86 root[86735] INFO [ICD] Processing diagnosis table
zwt-MS-7D86 root[86735] INFO [ICD] Table raw shape: (651047, 5)
zwt-MS-7D86 root[86735] INFO [ICD] Table processed shape: (650940, 3)
zwt-MS-7D86 root[86735] INFO [ICD] Filtered patient ids, filtered shape: (3942, 3)
zwt-MS-7D86 root[86735] INFO [CHECK 1] patient ids: 125
zwt-MS-7D86 root[86735] INFO [ICD] Remove non-vocabulary (size: 6984) codes, post: (3942, 3)
zwt-MS-7D86 root[86735] INFO [CHECK 2] patient ids: 125
zwt-MS-7D86 root[86735] INFO [ATC] Processing prescriptions table
/home/zwt/CS598-DL4H-Final-Project/mmugl-main/kg/kg/data/processing.py:187: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.
  med_pd = pd.read_csv(file, dtype=ndc_types)
zwt-MS-7D86 root[86735] INFO [ATC] Table raw shape: (4156450, 19)
zwt-MS-7D86 root[86735] INFO [ATC] Preprocessed shape: (2987420, 5)
zwt-MS-7D86 root[86735] INFO [ATC] Performed NDC -> RxNorm mapping, shape (2987420, 6)
zwt-MS-7D86 root[86735] INFO [ATC] Performed NDC -> ATC4 mapping, final shape (1988696, 5)
zwt-MS-7D86 root[86735] INFO Filtered visit range (1, inf), final shape: (1988696, 5)
zwt-MS-7D86 root[86735] INFO [ATC] Filtered patient ids, filtered shape: (12325, 5)
zwt-MS-7D86 root[86735] INFO [CHECK 3] patient ids: 125
zwt-MS-7D86 root[86735] INFO Grouping codes ICD9_CODE for (patients, admission), post shape (318, 3)
zwt-MS-7D86 root[86735] INFO Grouping codes ATC4 for (patients, admission), post shape (289, 3)
zwt-MS-7D86 root[86735] INFO [CHECK 4] patient ids: 125
zwt-MS-7D86 root[86735] INFO Filtered patients to ICD9_CODE count range (1, inf), filtered shape (318, 3)
zwt-MS-7D86 root[86735] INFO Filtered patients to ATC4 count range (1, inf), filtered shape (289, 3)
zwt-MS-7D86 root[86735] INFO [CHECK 5] patient ids: 125
zwt-MS-7D86 root[86735] INFO Merged diagnosis and prescription codes, shape: (318, 4)
zwt-MS-7D86 root[86735] INFO [CHECK 6] patient ids: 125
zwt-MS-7D86 root[86735] INFO [ATC] imputing patients without prescriptions
zwt-MS-7D86 root[86735] INFO Use existing tokenizer
zwt-MS-7D86 root[86735] INFO Max #codes per visit, ICD: 39, ATC: 48
zwt-MS-7D86 root[86735] INFO  > configured: 63, final: 64
zwt-MS-7D86 root[86735] INFO [UMLS] UMLS graph data on input
zwt-MS-7D86 root[86735] INFO [CONCEPTS] Loading note concepts from: data/noteevents_test_run_jaccard_0.9_w6.pkl
zwt-MS-7D86 root[86735] INFO [CONCEPTS] 774 patients
zwt-MS-7D86 root[86735] INFO [CONCEPTS] maximum visit text length: 1279
zwt-MS-7D86 root[86735] INFO [CONCEPTS] Flattening and tokenizing notes text concepts
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 774/774 [00:00<00:00, 2605.34it/s]
zwt-MS-7D86 root[86735] INFO [CONCEPTS] Pretokenized notes concepts for 774 patients
zwt-MS-7D86 root[86735] INFO [DATASET] output text targets: False
zwt-MS-7D86 root[86735] INFO [TARGET] Use existing target tokenizer
zwt-MS-7D86 root[86735] INFO Reformating for visit sequence learning based on: ../MIMIC-III/ADMISSIONS.csv
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [00:00<00:00, 6153.54it/s]
zwt-MS-7D86 root[86735] INFO Reformated data for visit sequence learning, num records: 125
zwt-MS-7D86 root[86735] INFO Maximum number of visits: 42
zwt-MS-7D86 root[86735] INFO [DATA]----- TEST -----
zwt-MS-7D86 root[86735] WARNING [DATASET] keeping no med patients: True
zwt-MS-7D86 root[86735] INFO [ICD] Processing diagnosis table
zwt-MS-7D86 root[86735] INFO [ICD] Table raw shape: (651047, 5)
zwt-MS-7D86 root[86735] INFO [ICD] Table processed shape: (650940, 3)
zwt-MS-7D86 root[86735] INFO [ICD] Filtered patient ids, filtered shape: (37888, 3)
zwt-MS-7D86 root[86735] INFO [CHECK 1] patient ids: 1221
zwt-MS-7D86 root[86735] INFO [ICD] Remove non-vocabulary (size: 6984) codes, post: (37888, 3)
zwt-MS-7D86 root[86735] INFO [CHECK 2] patient ids: 1221
zwt-MS-7D86 root[86735] INFO [ATC] Processing prescriptions table
/home/zwt/CS598-DL4H-Final-Project/mmugl-main/kg/kg/data/processing.py:187: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.
  med_pd = pd.read_csv(file, dtype=ndc_types)
zwt-MS-7D86 root[86735] INFO [ATC] Table raw shape: (4156450, 19)
zwt-MS-7D86 root[86735] INFO [ATC] Preprocessed shape: (2987420, 5)
zwt-MS-7D86 root[86735] INFO [ATC] Performed NDC -> RxNorm mapping, shape (2987420, 6)
zwt-MS-7D86 root[86735] INFO [ATC] Performed NDC -> ATC4 mapping, final shape (1988696, 5)
zwt-MS-7D86 root[86735] INFO Filtered visit range (1, inf), final shape: (1988696, 5)
zwt-MS-7D86 root[86735] INFO [ATC] Filtered patient ids, filtered shape: (118153, 5)
zwt-MS-7D86 root[86735] INFO [CHECK 3] patient ids: 1221
zwt-MS-7D86 root[86735] INFO Grouping codes ICD9_CODE for (patients, admission), post shape (3001, 3)
zwt-MS-7D86 root[86735] INFO Grouping codes ATC4 for (patients, admission), post shape (2677, 3)
zwt-MS-7D86 root[86735] INFO [CHECK 4] patient ids: 1221
zwt-MS-7D86 root[86735] INFO Filtered patients to ICD9_CODE count range (1, inf), filtered shape (3001, 3)
zwt-MS-7D86 root[86735] INFO Filtered patients to ATC4 count range (1, inf), filtered shape (2677, 3)
zwt-MS-7D86 root[86735] INFO [CHECK 5] patient ids: 1221
zwt-MS-7D86 root[86735] INFO Merged diagnosis and prescription codes, shape: (3001, 4)
zwt-MS-7D86 root[86735] INFO [CHECK 6] patient ids: 1221
zwt-MS-7D86 root[86735] INFO [ATC] imputing patients without prescriptions
zwt-MS-7D86 root[86735] INFO Use existing tokenizer
zwt-MS-7D86 root[86735] INFO Max #codes per visit, ICD: 39, ATC: 58
zwt-MS-7D86 root[86735] INFO  > configured: 63, final: 64
zwt-MS-7D86 root[86735] INFO [UMLS] UMLS graph data on input
zwt-MS-7D86 root[86735] INFO [CONCEPTS] Loading note concepts from: data/noteevents_test_run_jaccard_0.9_w6.pkl
zwt-MS-7D86 root[86735] INFO [CONCEPTS] 774 patients
zwt-MS-7D86 root[86735] INFO [CONCEPTS] maximum visit text length: 1279
zwt-MS-7D86 root[86735] INFO [CONCEPTS] Flattening and tokenizing notes text concepts
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 774/774 [00:00<00:00, 2463.61it/s]
zwt-MS-7D86 root[86735] INFO [CONCEPTS] Pretokenized notes concepts for 774 patients
zwt-MS-7D86 root[86735] INFO [DATASET] output text targets: False
zwt-MS-7D86 root[86735] INFO [TARGET] Use existing target tokenizer
zwt-MS-7D86 root[86735] INFO Reformating for visit sequence learning based on: ../MIMIC-III/ADMISSIONS.csv
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1221/1221 [00:00<00:00, 6068.46it/s]
zwt-MS-7D86 root[86735] INFO Reformated data for visit sequence learning, num records: 1221
zwt-MS-7D86 root[86735] INFO Maximum number of visits: 42
zwt-MS-7D86 root[86735] INFO [NN] Training CGL with text: True
zwt-MS-7D86 root[86735] INFO [NN] CGL number of context vectors 4
zwt-MS-7D86 root[86735] INFO [NN] CGL MLP layers: [128, 4825]
zwt-MS-7D86 root[86735] INFO [NN] cache graph forward pass: True
zwt-MS-7D86 root[86735] INFO [CGL] task: diagnosis (binary: False), loss: BCEWithLogitsLoss()
zwt-MS-7D86 root[86735] INFO ========================================
zwt-MS-7D86 root[86735] INFO Run Training for 80 epochs
zwt-MS-7D86 root[86735] INFO Num batches: 3078, size: 2
zwt-MS-7D86 root[86735] INFO Val batches: 63
zwt-MS-7D86 root[86735] INFO Learning rate: 0.0001
zwt-MS-7D86 root[86735] INFO ========================================
zwt-MS-7D86 root[86735] INFO [CGL] downstream training patience: 5
zwt-MS-7D86 root[86735] INFO [TRAIN] gradient accumulation: 2
zwt-MS-7D86 root[86735] INFO 	-> accumulated batch size: 4
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/zwt/miniconda3/envs/mmugl/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:611: UserWarning: Checkpoint directory dev/runs/tb/version_2/checkpoints exists and is not empty.
  rank_zero_warn(f"Checkpoint directory {dirpath} exists and is not empty.")
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name      | Type               | Params
-------------------------------------------------
0 | cgl_model | CglDownstreamModel | 20.3 M
1 | loss_func | BCEWithLogitsLoss  | 0     
-------------------------------------------------
1.7 M     Trainable params
18.7 M    Non-trainable params
20.3 M    Total params
81.328    Total estimated model params size (MB)
Sanity Checking: 0it [00:00, ?it/s]/home/zwt/miniconda3/envs/mmugl/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the num_workers argument (try 32 which is the number of cpus on this machine) in the DataLoader init to improve performance.
  rank_zero_warn(
/home/zwt/miniconda3/envs/mmugl/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the num_workers argument (try 32 which is the number of cpus on this machine) in the DataLoader init to improve performance.
  rank_zero_warn(
Epoch 34:  71%|██████████████████████████████████████▉                | 2182/3078 [01:32<00:38, 23.57it/s, loss=0.0123, v_num=2, train/batch_loss=0.015, train/loss=0.00535, val/loss=0Epoch 34:  71%|███████████████████▏       | 2183/3078 [01:32<00:37, 23.57it/s, loss=0.0123, v_num=2, train/batch_loss=0.015, train/loss=0.00535, val/loss=0.0114, val_down/auroc=0.961]Epoch 34:  72%|█████████████████▉       | 2205/3078 [01:33<00:37, 23.57it/s, loss=0.0111, v_num=2, train/batch_loss=0.00451, train/loss=0.00535, val/loss=0.0114, vaEpoch 34:  72%|█████████████████▉       | 2205/3078 [01:33<00:37, 23.56it/s, loss=0.0109, v_num=2, tEpoch 34:  72%|▋| 2206/3078 [01:33<00:36, 23.57it/s, loss=0.0109, v_num=2, train/batch_loss=0.00437, traiEpoch 34:  72%|▋| 2206/3078 [01:33<00:37, 23.56it/s, loss=0.0112, v_num=2, train/batch_loss=0.0147, trainEpoch 35: 100%|█| 3141/3141 [02:06<00:00, 24.80it/s, loss=0.0107, v_num=2, train/batch_loss=0.00568, trai
zwt-MS-7D86 root[86735] INFO Load best validation loss model: dev/runs/tb/version_2/checkpoints/epoch=25-step=40014.ckpt
zwt-MS-7D86 root[86735] INFO Missing keys: []
zwt-MS-7D86 root[86735] INFO Unexpected keys: []
zwt-MS-7D86 root[86735] INFO Store cgl model to dev/runs/model
zwt-MS-7D86 root[86735] INFO Running final validation
zwt-MS-7D86 root[86735] INFO Validation set:
100%|████████████████████████████████████████████████████████████████████| 63/63 [00:01<00:00, 59.03it/s]
zwt-MS-7D86 root[86735] INFO Best F1 0.235, AuROC: 0.961, t: 0.13
zwt-MS-7D86 root[86735] INFO ----- Sample prediction -----
zwt-MS-7D86 root[86735] INFO Predict: 25000 2859 4019 41401 42731 4280 42830 486 49121 496 5070 51881 51884 53081 5849 5990 V4582
zwt-MS-7D86 root[86735] INFO Target : 135 2749 27652 2851 4241 4280 42821 49121 60000 99811 E8788 V5865
zwt-MS-7D86 root[86735] INFO -----------------------------
zwt-MS-7D86 root[86735] INFO Test set:
100%|██████████████████████████████████████████████████████████████████| 611/611 [00:08<00:00, 70.92it/s]
zwt-MS-7D86 root[86735] INFO Best F1 0.221, AuROC: 0.961, t: 0.10
zwt-MS-7D86 root[86735] INFO ----- Sample prediction -----
zwt-MS-7D86 root[86735] INFO Predict: 1628 1629 1983 2449 25000 2720 2724 2762 2859 4019 412 41401 42731 4280 486 49121 49322 496 5070 51881 51884 51919 53081 5849 5990 E9320 V1011 V103 V1582 V4582 V462 V5865
zwt-MS-7D86 root[86735] INFO Target : 1977 1985 25000 2720 37900 4019 42731 49121 51881 5990 V1011 V103
zwt-MS-7D86 root[86735] INFO -----------------------------
zwt-MS-7D86 root[86735] INFO ========================================
zwt-MS-7D86 root[86735] INFO Finished Downstream Training
zwt-MS-7D86 root[86735] INFO Val F1      : 0.235
zwt-MS-7D86 root[86735] INFO Val F1 (inflated): 0.238
zwt-MS-7D86 root[86735] INFO Val AuPRC   : 0.961
zwt-MS-7D86 root[86735] INFO Val Thresh  : 0.13
zwt-MS-7D86 root[86735] INFO Val R@20    : 0.38
zwt-MS-7D86 root[86735] INFO Val R@40    : 0.50
zwt-MS-7D86 root[86735] INFO 
zwt-MS-7D86 root[86735] INFO Test F1      : 0.221
zwt-MS-7D86 root[86735] INFO Test F1 (inflated): 0.232
zwt-MS-7D86 root[86735] INFO Test AuPRC   : 0.961
zwt-MS-7D86 root[86735] INFO Test Thresh  : 0.10
zwt-MS-7D86 root[86735] INFO Test R@20    : 0.39
zwt-MS-7D86 root[86735] INFO Test R@40    : 0.51
zwt-MS-7D86 root[86735] INFO ========================================
ChatGPT said:

Analyzed
Always show details

import re

# Simulated multiline log input
log_text = '''zwt-MS-7D86 root[86735] INFO Test R@40    : 0.51
zwt-MS-7D86 root[86735] INFO ========================================'''

# Remove timestamps and host identifiers using regex
cleaned_log = re.sub(r'^\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2} [^\s]+ root\[\d+\] INFO ', '', log_text, flags=re.MULTILINE)

cleaned_log
Result
'Test R@40    : 0.51\n========================================'
Here is a cleaned version of the log snippet without timestamps or host metadata:

Test R@40    : 0.51
========================================
Would you like me to clean the full block of logs you posted in the same way and return it as a downloadable file or formatted text? 

(mmugl) zwt@zwt-MS-7D86:~/CS598-DL4H-Final-Project/mmugl-main$ python dev/run_training_concepts.py   --name diagnosis_test   --logdir dev/runs   --data_dir data   --code_mappings data/code_mappings   --epochs 80   --num_workers 2   --grad_acc 1   --batch_size 4   --down_batch_size 2   --down_grad_acc 2   --down_learning_rate 0.0001   --down_es 5   --embedding_dim 128   --graph_num_layers 1   --graph_num_filters 1   --gnn_operator SAGEConv   --attention_heads 1   --feedforward_dim 128   --num_blocks 1   --agg_mask_loss 0.25   --mlp_dim 64   --mlp_num_layers 1   --freeze_embeddings   --store_pretrainmodel   --store_downmodel   --pretrain   --pretrain_es loss_total   --pretrain_patience 3   --split_mode cgl   --icd_codes data/disease_codes_all.txt   --down_icd_codes data/cgl_mimic3_disease_codes.txt   --atc_codes data/med_codes_all.txt   --diagnosis_csv ../MIMIC-III/DIAGNOSES_ICD.csv   --prescription_csv ../MIMIC-III/PRESCRIPTIONS.csv   --admission_csv ../MIMIC-III/ADMISSIONS.csv   --umls_graph data/umls_graph.pkl   --notes_concepts data/noteevents_test_run_jaccard_0.9_w6.pkl   --max_visit_text_length 1279   --downstream_cgl   --cgl_num_contexts 4   --cgl_task diagnosis   --down_mlp_dim 0   --down_mlp_num_layers 0   --random_state 1111   --verbose
zwt-MS-7D86 root[86735] INFO ========================================
zwt-MS-7D86 root[86735] INFO Start Training script
zwt-MS-7D86 root[86735] INFO Host: zwt-MS-7D86
zwt-MS-7D86 root[86735] INFO Torch device: NVIDIA GeForce RTX 4090
zwt-MS-7D86 root[86735] INFO GPU Memory: 23.5 GB
zwt-MS-7D86 root[86735] INFO Workers: 2
zwt-MS-7D86 root[86735] INFO ========================================
Global seed set to 1111
zwt-MS-7D86 root[86735] INFO Load UMLS graph from: data/umls_graph.pkl
zwt-MS-7D86 root[86735] INFO [ICD] Processing diagnosis table
zwt-MS-7D86 root[86735] INFO [ICD] Table raw shape: (651047, 5)
zwt-MS-7D86 root[86735] INFO [ICD] Table processed shape: (650940, 3)
zwt-MS-7D86 root[86735] INFO [SPLIT cgl] train suggested: 6000
zwt-MS-7D86 root[86735] INFO [SPLIT cgl] train loaded: 45171
zwt-MS-7D86 root[86735] INFO [SPLIT cgl] intersect: 6000
zwt-MS-7D86 root[86735] INFO [SPLIT] Created data splits, train: 45171, val: 125
zwt-MS-7D86 root[86735] INFO Parsed 6984 diagnosis codes from provided file
zwt-MS-7D86 root[86735] INFO Parsed 348 prescription codes from provided file
zwt-MS-7D86 root[86735] INFO [DATA]----- TRAIN -----
zwt-MS-7D86 root[86735] INFO [ICD] Processing diagnosis table
zwt-MS-7D86 root[86735] INFO [ICD] Table raw shape: (651047, 5)
zwt-MS-7D86 root[86735] INFO [ICD] Table processed shape: (650940, 3)
zwt-MS-7D86 root[86735] INFO [ICD] Filtered patient ids, filtered shape: (609110, 3)
zwt-MS-7D86 root[86735] INFO [CHECK 1] patient ids: 45171
zwt-MS-7D86 root[86735] INFO [ICD] Remove non-vocabulary (size: 6984) codes, post: (609110, 3)
zwt-MS-7D86 root[86735] INFO [CHECK 2] patient ids: 45171
zwt-MS-7D86 root[86735] INFO [ATC] Processing prescriptions table
/home/zwt/CS598-DL4H-Final-Project/mmugl-main/kg/kg/data/processing.py:187: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.
  med_pd = pd.read_csv(file, dtype=ndc_types)
zwt-MS-7D86 root[86735] INFO [ATC] Table raw shape: (4156450, 19)
zwt-MS-7D86 root[86735] INFO [ATC] Preprocessed shape: (2987420, 5)
zwt-MS-7D86 root[86735] INFO [ATC] Performed NDC -> RxNorm mapping, shape (2987420, 6)
zwt-MS-7D86 root[86735] INFO [ATC] Performed NDC -> ATC4 mapping, final shape (1988696, 5)
zwt-MS-7D86 root[86735] INFO Match (SUBJECT_ID, HADM_ID), diagnosis: (559813, 3), med: (1858182, 5)
zwt-MS-7D86 root[86735] INFO [CHECK 3] patient ids: 37819
zwt-MS-7D86 root[86735] INFO Grouping codes ICD9_CODE for (patients, admission), post shape (46958, 3)
zwt-MS-7D86 root[86735] INFO Grouping codes ATC4 for (patients, admission), post shape (46958, 3)
zwt-MS-7D86 root[86735] INFO [CHECK 4] patient ids: 37819
zwt-MS-7D86 root[86735] INFO Filtered patients to ICD9_CODE count range (1, inf), filtered shape (46958, 3)
zwt-MS-7D86 root[86735] INFO Filtered patients to ATC4 count range (1, inf), filtered shape (46958, 3)
zwt-MS-7D86 root[86735] INFO [CHECK 5] patient ids: 37819
zwt-MS-7D86 root[86735] INFO Merged diagnosis and prescription codes, shape: (46958, 4)
zwt-MS-7D86 root[86735] INFO [CHECK 6] patient ids: 37819
zwt-MS-7D86 root[86735] INFO Max #codes per visit, ICD: 39, ATC: 78
zwt-MS-7D86 root[86735] INFO  > configured: 63, final: 64
zwt-MS-7D86 root[86735] INFO [UMLS] UMLS graph data on input
zwt-MS-7D86 root[86735] INFO [CONCEPTS] Loading note concepts from: data/noteevents_test_run_jaccard_0.9_w6.pkl
zwt-MS-7D86 root[86735] INFO [CONCEPTS] 774 patients
zwt-MS-7D86 root[86735] INFO [CONCEPTS] maximum visit text length: 1279
zwt-MS-7D86 root[86735] INFO [CONCEPTS] Flattening and tokenizing notes text concepts
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 774/774 [00:00<00:00, 2628.85it/s]
zwt-MS-7D86 root[86735] INFO [CONCEPTS] Pretokenized notes concepts for 774 patients
zwt-MS-7D86 root[86735] INFO [DATASET] output text targets: False
zwt-MS-7D86 root[86735] INFO [DATA]----- VAL -----
zwt-MS-7D86 root[86735] INFO [ICD] Processing diagnosis table
zwt-MS-7D86 root[86735] INFO [ICD] Table raw shape: (651047, 5)
zwt-MS-7D86 root[86735] INFO [ICD] Table processed shape: (650940, 3)
zwt-MS-7D86 root[86735] INFO [ICD] Filtered patient ids, filtered shape: (3942, 3)
zwt-MS-7D86 root[86735] INFO [CHECK 1] patient ids: 125
zwt-MS-7D86 root[86735] INFO [ICD] Remove non-vocabulary (size: 6984) codes, post: (3942, 3)
zwt-MS-7D86 root[86735] INFO [CHECK 2] patient ids: 125
zwt-MS-7D86 root[86735] INFO [ATC] Processing prescriptions table
/home/zwt/CS598-DL4H-Final-Project/mmugl-main/kg/kg/data/processing.py:187: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.
  med_pd = pd.read_csv(file, dtype=ndc_types)
zwt-MS-7D86 root[86735] INFO [ATC] Table raw shape: (4156450, 19)
zwt-MS-7D86 root[86735] INFO [ATC] Preprocessed shape: (2987420, 5)
zwt-MS-7D86 root[86735] INFO [ATC] Performed NDC -> RxNorm mapping, shape (2987420, 6)
zwt-MS-7D86 root[86735] INFO [ATC] Performed NDC -> ATC4 mapping, final shape (1988696, 5)
zwt-MS-7D86 root[86735] INFO Match (SUBJECT_ID, HADM_ID), diagnosis: (3702, 3), med: (12325, 5)
zwt-MS-7D86 root[86735] INFO [CHECK 3] patient ids: 119
zwt-MS-7D86 root[86735] INFO Grouping codes ICD9_CODE for (patients, admission), post shape (289, 3)
zwt-MS-7D86 root[86735] INFO Grouping codes ATC4 for (patients, admission), post shape (289, 3)
zwt-MS-7D86 root[86735] INFO [CHECK 4] patient ids: 119
zwt-MS-7D86 root[86735] INFO Filtered patients to ICD9_CODE count range (1, inf), filtered shape (289, 3)
zwt-MS-7D86 root[86735] INFO Filtered patients to ATC4 count range (1, inf), filtered shape (289, 3)
zwt-MS-7D86 root[86735] INFO [CHECK 5] patient ids: 119
zwt-MS-7D86 root[86735] INFO Merged diagnosis and prescription codes, shape: (289, 4)
zwt-MS-7D86 root[86735] INFO [CHECK 6] patient ids: 119
zwt-MS-7D86 root[86735] INFO Use existing tokenizer
zwt-MS-7D86 root[86735] INFO Max #codes per visit, ICD: 39, ATC: 48
zwt-MS-7D86 root[86735] INFO  > configured: 63, final: 64
zwt-MS-7D86 root[86735] INFO [UMLS] UMLS graph data on input
zwt-MS-7D86 root[86735] INFO [CONCEPTS] Loading note concepts from: data/noteevents_test_run_jaccard_0.9_w6.pkl
zwt-MS-7D86 root[86735] INFO [CONCEPTS] 774 patients
zwt-MS-7D86 root[86735] INFO [CONCEPTS] maximum visit text length: 1279
zwt-MS-7D86 root[86735] INFO [CONCEPTS] Flattening and tokenizing notes text concepts
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 774/774 [00:00<00:00, 2551.82it/s]
zwt-MS-7D86 root[86735] INFO [CONCEPTS] Pretokenized notes concepts for 774 patients
zwt-MS-7D86 root[86735] INFO [DATASET] output text targets: False
zwt-MS-7D86 root[86735] INFO [NN] Training with text: True, targets: False
zwt-MS-7D86 root[86735] INFO [NN] cache graph forward pass: True
zwt-MS-7D86 root[86735] INFO [NN] UMLS graph embedding module
zwt-MS-7D86 root[86735] INFO [UMLS] loaded graph data, hops: 0
zwt-MS-7D86 root[86735] INFO [GRAPH] using SapBert embeddings as initialization
zwt-MS-7D86 root[86735] INFO [UMLS] use provided CUI vocabulary: 23864
zwt-MS-7D86 root[86735] INFO [UMLS] loaded 99064 edges
zwt-MS-7D86 root[86735] INFO [UMLS] Built SapBERT feature matrix: torch.Size([23864, 768])
zwt-MS-7D86 root[86735] INFO UMLSGraphEmbedding has 23864 nodes
zwt-MS-7D86 root[86735] INFO [GRAPH] Node aggregation: sum
zwt-MS-7D86 root[86735] INFO Built: UMLSGNN 1 layers, 1 filters: SAGEConv:128
zwt-MS-7D86 root[86735] INFO [EMBEDDING] Final embedding size: (3, 23861) = 23864
zwt-MS-7D86 root[86735] INFO [INIT] DONT initialize co node embeddings
zwt-MS-7D86 root[86735] INFO [NN] Initializing heads for aggregation loss
zwt-MS-7D86 root[86735] INFO ----- Model -----
zwt-MS-7D86 root[86735] INFO Embedding dimension: 128
zwt-MS-7D86 root[86735] INFO GNN operator: SAGEConv
zwt-MS-7D86 root[86735] INFO Graph layers: 1, staged: False
zwt-MS-7D86 root[86735] INFO Transformer heads: 1, mlp: 128, blocks: 1
zwt-MS-7D86 root[86735] INFO MLP dim: 64, layers: 1
zwt-MS-7D86 root[86735] INFO Masked loss alpha: 0.25
zwt-MS-7D86 root[86735] INFO Hungarian set loss alpha: 0.0
zwt-MS-7D86 root[86735] INFO Co-occurrence loss alpha: 0.0
zwt-MS-7D86 root[86735] INFO co-nodes latent space: False
zwt-MS-7D86 root[86735] INFO Decoder Network: False
zwt-MS-7D86 root[86735] INFO ----- ----- -----
zwt-MS-7D86 root[86735] INFO Tensorboard logs at: dev/runs/diagnosis_test
zwt-MS-7D86 root[86735] INFO ========================================
zwt-MS-7D86 root[86735] INFO Run Training for 80 epochs
zwt-MS-7D86 root[86735] INFO Num batches: 11740, size: 4
zwt-MS-7D86 root[86735] INFO Val batches: 73
zwt-MS-7D86 root[86735] INFO Learning rate: 0.0005
zwt-MS-7D86 root[86735] INFO ========================================
zwt-MS-7D86 root[86735] INFO [TRAIN] early stopping on val/loss_total, patience 3
zwt-MS-7D86 root[86735] INFO [TRAINER] Sampling 4 triplets per anchor id
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name           | Type                            | Params
-------------------------------------------------------------------
0 | pretrain_model | JointCodePretrainingTransformer | 20.3 M
1 | loss           | BCEWithLogitsLoss               | 0     
-------------------------------------------------------------------
1.7 M     Trainable params
18.5 M    Non-trainable params
20.3 M    Total params
81.085    Total estimated model params size (MB)
Sanity Checking: 0it [00:00, ?it/s]/home/zwt/miniconda3/envs/mmugl/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the num_workers argument (try 32 which is the number of cpus on this machine) in the DataLoader init to improve performance.
  rank_zero_warn(
/home/zwt/miniconda3/envs/mmugl/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the num_workers argument (try 32 which is the number of cpus on this machine) in the DataLoader init to improve performance.
  rank_zero_warn(
Epoch 37: 100%|████████████████████████████████████████████████████████| 11813/11813 [01:23<00:00, 141.46it/s, loss=0.175, v_num=2, train/batch_loss=0.250, train/loss=0.173, val/loss=0.187, val/loss_total=0.187]
zwt-MS-7D86 root[86735] INFO Load best validation loss model: dev/runs/tb/version_2/checkpoints/epoch=31-step=375680.ckpt                                                                      
zwt-MS-7D86 root[86735] INFO Missing keys: []
zwt-MS-7D86 root[86735] INFO Unexpected keys: []
zwt-MS-7D86 root[86735] INFO Store pretrain model to dev/runs/model
zwt-MS-7D86 root[86735] INFO Running final validation
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 73/73 [00:00<00:00, 175.14it/s]
zwt-MS-7D86 root[86735] INFO [d2d] jaccard: 0.289 f1: 0.421 prauc: 0.538 
zwt-MS-7D86 root[86735] INFO [p2d] jaccard: 0.073 f1: 0.123 prauc: 0.266 
zwt-MS-7D86 root[86735] INFO [p2p] jaccard: 0.778 f1: 0.871 prauc: 0.933 
zwt-MS-7D86 root[86735] INFO [d2p] jaccard: 0.423 f1: 0.583 prauc: 0.688 
zwt-MS-7D86 root[86735] INFO ========================================
zwt-MS-7D86 root[86735] INFO Finished Pretraining Training
zwt-MS-7D86 root[86735] INFO Trained for 38 epochs
zwt-MS-7D86 root[86735] INFO Val Jaccard       : 0.391
zwt-MS-7D86 root[86735] INFO Val F1            : 0.499
zwt-MS-7D86 root[86735] INFO Val AuPRC         : 0.606
zwt-MS-7D86 root[86735] INFO ========================================
zwt-MS-7D86 root[86735] INFO Layer: UMLSGraphEmbedding(
  (cui_feature_projector): Linear(in_features=768, out_features=128, bias=True)
  (gnn): UMLSGNN(
    (graph_convs): ModuleList(
      (0): ModuleList(
        (0): HeteroConv(num_relations=1)
      )
    )
    (pool_stacks): ModuleList()
  )
) frozen, (8 params frozen)
zwt-MS-7D86 root[86735] INFO Set downstream batchsize to 2
zwt-MS-7D86 root[86735] INFO 
zwt-MS-7D86 root[86735] INFO ========================================
zwt-MS-7D86 root[86735] INFO Performing downstream DISEASE training
zwt-MS-7D86 root[86735] INFO Task: diagnosis
zwt-MS-7D86 root[86735] INFO With text: True
zwt-MS-7D86 root[86735] INFO eICU: False
zwt-MS-7D86 root[86735] INFO ========================================
zwt-MS-7D86 root[86735] INFO [CGL] Parsed 4825 diagnosis codes from provided file
zwt-MS-7D86 root[86735] INFO [DATA]----- TRAIN -----
zwt-MS-7D86 root[86735] WARNING [DATASET] keeping no med patients: True
zwt-MS-7D86 root[86735] INFO [ICD] Processing diagnosis table
zwt-MS-7D86 root[86735] INFO [ICD] Table raw shape: (651047, 5)
zwt-MS-7D86 root[86735] INFO [ICD] Table processed shape: (650940, 3)
zwt-MS-7D86 root[86735] INFO [ICD] Filtered patient ids, filtered shape: (609110, 3)
zwt-MS-7D86 root[86735] INFO [CHECK 1] patient ids: 45171
zwt-MS-7D86 root[86735] INFO [ICD] Remove non-vocabulary (size: 6984) codes, post: (609110, 3)
zwt-MS-7D86 root[86735] INFO [CHECK 2] patient ids: 45171
zwt-MS-7D86 root[86735] INFO [ATC] Processing prescriptions table
/home/zwt/CS598-DL4H-Final-Project/mmugl-main/kg/kg/data/processing.py:187: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.
  med_pd = pd.read_csv(file, dtype=ndc_types)
zwt-MS-7D86 root[86735] INFO [ATC] Table raw shape: (4156450, 19)
zwt-MS-7D86 root[86735] INFO [ATC] Preprocessed shape: (2987420, 5)
zwt-MS-7D86 root[86735] INFO [ATC] Performed NDC -> RxNorm mapping, shape (2987420, 6)
zwt-MS-7D86 root[86735] INFO [ATC] Performed NDC -> ATC4 mapping, final shape (1988696, 5)
zwt-MS-7D86 root[86735] INFO Filtered visit range (1, inf), final shape: (1988696, 5)
zwt-MS-7D86 root[86735] INFO [ATC] Filtered patient ids, filtered shape: (1858217, 5)
zwt-MS-7D86 root[86735] INFO [CHECK 3] patient ids: 45171
zwt-MS-7D86 root[86735] INFO Grouping codes ICD9_CODE for (patients, admission), post shape (55610, 3)
zwt-MS-7D86 root[86735] INFO Grouping codes ATC4 for (patients, admission), post shape (46962, 3)
zwt-MS-7D86 root[86735] INFO [CHECK 4] patient ids: 45171
zwt-MS-7D86 root[86735] INFO Filtered patients to ICD9_CODE count range (1, inf), filtered shape (55610, 3)
zwt-MS-7D86 root[86735] INFO Filtered patients to ATC4 count range (1, inf), filtered shape (46962, 3)
zwt-MS-7D86 root[86735] INFO [CHECK 5] patient ids: 45171
zwt-MS-7D86 root[86735] INFO Merged diagnosis and prescription codes, shape: (55614, 4)
zwt-MS-7D86 root[86735] INFO [CHECK 6] patient ids: 45171
zwt-MS-7D86 root[86735] INFO [ATC] imputing patients without prescriptions
zwt-MS-7D86 root[86735] INFO Use existing tokenizer
zwt-MS-7D86 root[86735] INFO Max #codes per visit, ICD: 39, ATC: 78
zwt-MS-7D86 root[86735] INFO  > configured: 63, final: 64
zwt-MS-7D86 root[86735] INFO [UMLS] UMLS graph data on input
zwt-MS-7D86 root[86735] INFO [CONCEPTS] Loading note concepts from: data/noteevents_test_run_jaccard_0.9_w6.pkl
zwt-MS-7D86 root[86735] INFO [CONCEPTS] 774 patients
zwt-MS-7D86 root[86735] INFO [CONCEPTS] maximum visit text length: 1279
zwt-MS-7D86 root[86735] INFO [CONCEPTS] Flattening and tokenizing notes text concepts
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 774/774 [00:00<00:00, 2577.76it/s]
zwt-MS-7D86 root[86735] INFO [CONCEPTS] Pretokenized notes concepts for 774 patients
zwt-MS-7D86 root[86735] INFO [DATASET] output text targets: False
zwt-MS-7D86 root[86735] INFO [TARGET] Considering distinct target code set
zwt-MS-7D86 root[86735] INFO [TARGET] Instatiating new target tokenizer, d: 4825
zwt-MS-7D86 root[86735] INFO Reformating for visit sequence learning based on: ../MIMIC-III/ADMISSIONS.csv
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 45171/45171 [00:06<00:00, 7208.88it/s]
zwt-MS-7D86 root[86735] INFO Reformated data for visit sequence learning, num records: 6156
zwt-MS-7D86 root[86735] INFO Maximum number of visits: 42
zwt-MS-7D86 root[86735] INFO [DATA]----- VAL -----
zwt-MS-7D86 root[86735] WARNING [DATASET] keeping no med patients: True
zwt-MS-7D86 root[86735] INFO [ICD] Processing diagnosis table
zwt-MS-7D86 root[86735] INFO [ICD] Table raw shape: (651047, 5)
zwt-MS-7D86 root[86735] INFO [ICD] Table processed shape: (650940, 3)
zwt-MS-7D86 root[86735] INFO [ICD] Filtered patient ids, filtered shape: (3942, 3)
zwt-MS-7D86 root[86735] INFO [CHECK 1] patient ids: 125
zwt-MS-7D86 root[86735] INFO [ICD] Remove non-vocabulary (size: 6984) codes, post: (3942, 3)
zwt-MS-7D86 root[86735] INFO [CHECK 2] patient ids: 125
zwt-MS-7D86 root[86735] INFO [ATC] Processing prescriptions table
/home/zwt/CS598-DL4H-Final-Project/mmugl-main/kg/kg/data/processing.py:187: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.
  med_pd = pd.read_csv(file, dtype=ndc_types)
zwt-MS-7D86 root[86735] INFO [ATC] Table raw shape: (4156450, 19)
zwt-MS-7D86 root[86735] INFO [ATC] Preprocessed shape: (2987420, 5)
zwt-MS-7D86 root[86735] INFO [ATC] Performed NDC -> RxNorm mapping, shape (2987420, 6)
zwt-MS-7D86 root[86735] INFO [ATC] Performed NDC -> ATC4 mapping, final shape (1988696, 5)
zwt-MS-7D86 root[86735] INFO Filtered visit range (1, inf), final shape: (1988696, 5)
zwt-MS-7D86 root[86735] INFO [ATC] Filtered patient ids, filtered shape: (12325, 5)
zwt-MS-7D86 root[86735] INFO [CHECK 3] patient ids: 125
zwt-MS-7D86 root[86735] INFO Grouping codes ICD9_CODE for (patients, admission), post shape (318, 3)
zwt-MS-7D86 root[86735] INFO Grouping codes ATC4 for (patients, admission), post shape (289, 3)
zwt-MS-7D86 root[86735] INFO [CHECK 4] patient ids: 125
zwt-MS-7D86 root[86735] INFO Filtered patients to ICD9_CODE count range (1, inf), filtered shape (318, 3)
zwt-MS-7D86 root[86735] INFO Filtered patients to ATC4 count range (1, inf), filtered shape (289, 3)
zwt-MS-7D86 root[86735] INFO [CHECK 5] patient ids: 125
zwt-MS-7D86 root[86735] INFO Merged diagnosis and prescription codes, shape: (318, 4)
zwt-MS-7D86 root[86735] INFO [CHECK 6] patient ids: 125
zwt-MS-7D86 root[86735] INFO [ATC] imputing patients without prescriptions
zwt-MS-7D86 root[86735] INFO Use existing tokenizer
zwt-MS-7D86 root[86735] INFO Max #codes per visit, ICD: 39, ATC: 48
zwt-MS-7D86 root[86735] INFO  > configured: 63, final: 64
zwt-MS-7D86 root[86735] INFO [UMLS] UMLS graph data on input
zwt-MS-7D86 root[86735] INFO [CONCEPTS] Loading note concepts from: data/noteevents_test_run_jaccard_0.9_w6.pkl
zwt-MS-7D86 root[86735] INFO [CONCEPTS] 774 patients
zwt-MS-7D86 root[86735] INFO [CONCEPTS] maximum visit text length: 1279
zwt-MS-7D86 root[86735] INFO [CONCEPTS] Flattening and tokenizing notes text concepts
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 774/774 [00:00<00:00, 2605.34it/s]
zwt-MS-7D86 root[86735] INFO [CONCEPTS] Pretokenized notes concepts for 774 patients
zwt-MS-7D86 root[86735] INFO [DATASET] output text targets: False
zwt-MS-7D86 root[86735] INFO [TARGET] Use existing target tokenizer
zwt-MS-7D86 root[86735] INFO Reformating for visit sequence learning based on: ../MIMIC-III/ADMISSIONS.csv
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [00:00<00:00, 6153.54it/s]
zwt-MS-7D86 root[86735] INFO Reformated data for visit sequence learning, num records: 125
zwt-MS-7D86 root[86735] INFO Maximum number of visits: 42
zwt-MS-7D86 root[86735] INFO [DATA]----- TEST -----
zwt-MS-7D86 root[86735] WARNING [DATASET] keeping no med patients: True
zwt-MS-7D86 root[86735] INFO [ICD] Processing diagnosis table
zwt-MS-7D86 root[86735] INFO [ICD] Table raw shape: (651047, 5)
zwt-MS-7D86 root[86735] INFO [ICD] Table processed shape: (650940, 3)
zwt-MS-7D86 root[86735] INFO [ICD] Filtered patient ids, filtered shape: (37888, 3)
zwt-MS-7D86 root[86735] INFO [CHECK 1] patient ids: 1221
zwt-MS-7D86 root[86735] INFO [ICD] Remove non-vocabulary (size: 6984) codes, post: (37888, 3)
zwt-MS-7D86 root[86735] INFO [CHECK 2] patient ids: 1221
zwt-MS-7D86 root[86735] INFO [ATC] Processing prescriptions table
/home/zwt/CS598-DL4H-Final-Project/mmugl-main/kg/kg/data/processing.py:187: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.
  med_pd = pd.read_csv(file, dtype=ndc_types)
zwt-MS-7D86 root[86735] INFO [ATC] Table raw shape: (4156450, 19)
zwt-MS-7D86 root[86735] INFO [ATC] Preprocessed shape: (2987420, 5)
zwt-MS-7D86 root[86735] INFO [ATC] Performed NDC -> RxNorm mapping, shape (2987420, 6)
zwt-MS-7D86 root[86735] INFO [ATC] Performed NDC -> ATC4 mapping, final shape (1988696, 5)
zwt-MS-7D86 root[86735] INFO Filtered visit range (1, inf), final shape: (1988696, 5)
zwt-MS-7D86 root[86735] INFO [ATC] Filtered patient ids, filtered shape: (118153, 5)
zwt-MS-7D86 root[86735] INFO [CHECK 3] patient ids: 1221
zwt-MS-7D86 root[86735] INFO Grouping codes ICD9_CODE for (patients, admission), post shape (3001, 3)
zwt-MS-7D86 root[86735] INFO Grouping codes ATC4 for (patients, admission), post shape (2677, 3)
zwt-MS-7D86 root[86735] INFO [CHECK 4] patient ids: 1221
zwt-MS-7D86 root[86735] INFO Filtered patients to ICD9_CODE count range (1, inf), filtered shape (3001, 3)
zwt-MS-7D86 root[86735] INFO Filtered patients to ATC4 count range (1, inf), filtered shape (2677, 3)
zwt-MS-7D86 root[86735] INFO [CHECK 5] patient ids: 1221
zwt-MS-7D86 root[86735] INFO Merged diagnosis and prescription codes, shape: (3001, 4)
zwt-MS-7D86 root[86735] INFO [CHECK 6] patient ids: 1221
zwt-MS-7D86 root[86735] INFO [ATC] imputing patients without prescriptions
zwt-MS-7D86 root[86735] INFO Use existing tokenizer
zwt-MS-7D86 root[86735] INFO Max #codes per visit, ICD: 39, ATC: 58
zwt-MS-7D86 root[86735] INFO  > configured: 63, final: 64
zwt-MS-7D86 root[86735] INFO [UMLS] UMLS graph data on input
zwt-MS-7D86 root[86735] INFO [CONCEPTS] Loading note concepts from: data/noteevents_test_run_jaccard_0.9_w6.pkl
zwt-MS-7D86 root[86735] INFO [CONCEPTS] 774 patients
zwt-MS-7D86 root[86735] INFO [CONCEPTS] maximum visit text length: 1279
zwt-MS-7D86 root[86735] INFO [CONCEPTS] Flattening and tokenizing notes text concepts
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 774/774 [00:00<00:00, 2463.61it/s]
zwt-MS-7D86 root[86735] INFO [CONCEPTS] Pretokenized notes concepts for 774 patients
zwt-MS-7D86 root[86735] INFO [DATASET] output text targets: False
zwt-MS-7D86 root[86735] INFO [TARGET] Use existing target tokenizer
zwt-MS-7D86 root[86735] INFO Reformating for visit sequence learning based on: ../MIMIC-III/ADMISSIONS.csv
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1221/1221 [00:00<00:00, 6068.46it/s]
zwt-MS-7D86 root[86735] INFO Reformated data for visit sequence learning, num records: 1221
zwt-MS-7D86 root[86735] INFO Maximum number of visits: 42
zwt-MS-7D86 root[86735] INFO [NN] Training CGL with text: True
zwt-MS-7D86 root[86735] INFO [NN] CGL number of context vectors 4
zwt-MS-7D86 root[86735] INFO [NN] CGL MLP layers: [128, 4825]
zwt-MS-7D86 root[86735] INFO [NN] cache graph forward pass: True
zwt-MS-7D86 root[86735] INFO [CGL] task: diagnosis (binary: False), loss: BCEWithLogitsLoss()
zwt-MS-7D86 root[86735] INFO ========================================
zwt-MS-7D86 root[86735] INFO Run Training for 80 epochs
zwt-MS-7D86 root[86735] INFO Num batches: 3078, size: 2
zwt-MS-7D86 root[86735] INFO Val batches: 63
zwt-MS-7D86 root[86735] INFO Learning rate: 0.0001
zwt-MS-7D86 root[86735] INFO ========================================
zwt-MS-7D86 root[86735] INFO [CGL] downstream training patience: 5
zwt-MS-7D86 root[86735] INFO [TRAIN] gradient accumulation: 2
zwt-MS-7D86 root[86735] INFO 	-> accumulated batch size: 4
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/zwt/miniconda3/envs/mmugl/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:611: UserWarning: Checkpoint directory dev/runs/tb/version_2/checkpoints exists and is not empty.
  rank_zero_warn(f"Checkpoint directory {dirpath} exists and is not empty.")
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name      | Type               | Params
-------------------------------------------------
0 | cgl_model | CglDownstreamModel | 20.3 M
1 | loss_func | BCEWithLogitsLoss  | 0     
-------------------------------------------------
1.7 M     Trainable params
18.7 M    Non-trainable params
20.3 M    Total params
81.328    Total estimated model params size (MB)
Sanity Checking: 0it [00:00, ?it/s]/home/zwt/miniconda3/envs/mmugl/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the num_workers argument (try 32 which is the number of cpus on this machine) in the DataLoader init to improve performance.
  rank_zero_warn(
/home/zwt/miniconda3/envs/mmugl/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the num_workers argument (try 32 which is the number of cpus on this machine) in the DataLoader init to improve performance.
  rank_zero_warn(
Epoch 34:  71%|██████████████████████████████████████▉                | 2182/3078 [01:32<00:38, 23.57it/s, loss=0.0123, v_num=2, train/batch_loss=0.015, train/loss=0.00535, val/loss=0Epoch 34:  71%|███████████████████▏       | 2183/3078 [01:32<00:37, 23.57it/s, loss=0.0123, v_num=2, train/batch_loss=0.015, train/loss=0.00535, val/loss=0.0114, val_down/auroc=0.961]Epoch 34:  72%|█████████████████▉       | 2205/3078 [01:33<00:37, 23.57it/s, loss=0.0111, v_num=2, train/batch_loss=0.00451, train/loss=0.00535, val/loss=0.0114, vaEpoch 34:  72%|█████████████████▉       | 2205/3078 [01:33<00:37, 23.56it/s, loss=0.0109, v_num=2, tEpoch 34:  72%|▋| 2206/3078 [01:33<00:36, 23.57it/s, loss=0.0109, v_num=2, train/batch_loss=0.00437, traiEpoch 34:  72%|▋| 2206/3078 [01:33<00:37, 23.56it/s, loss=0.0112, v_num=2, train/batch_loss=0.0147, trainEpoch 35: 100%|█| 3141/3141 [02:06<00:00, 24.80it/s, loss=0.0107, v_num=2, train/batch_loss=0.00568, trai
zwt-MS-7D86 root[86735] INFO Load best validation loss model: dev/runs/tb/version_2/checkpoints/epoch=25-step=40014.ckpt
zwt-MS-7D86 root[86735] INFO Missing keys: []
zwt-MS-7D86 root[86735] INFO Unexpected keys: []
zwt-MS-7D86 root[86735] INFO Store cgl model to dev/runs/model
zwt-MS-7D86 root[86735] INFO Running final validation
zwt-MS-7D86 root[86735] INFO Validation set:
100%|████████████████████████████████████████████████████████████████████| 63/63 [00:01<00:00, 59.03it/s]
zwt-MS-7D86 root[86735] INFO Best F1 0.235, AuROC: 0.961, t: 0.13
zwt-MS-7D86 root[86735] INFO ----- Sample prediction -----
zwt-MS-7D86 root[86735] INFO Predict: 25000 2859 4019 41401 42731 4280 42830 486 49121 496 5070 51881 51884 53081 5849 5990 V4582
zwt-MS-7D86 root[86735] INFO Target : 135 2749 27652 2851 4241 4280 42821 49121 60000 99811 E8788 V5865
zwt-MS-7D86 root[86735] INFO -----------------------------
zwt-MS-7D86 root[86735] INFO Test set:
100%|██████████████████████████████████████████████████████████████████| 611/611 [00:08<00:00, 70.92it/s]
zwt-MS-7D86 root[86735] INFO Best F1 0.221, AuROC: 0.961, t: 0.10
zwt-MS-7D86 root[86735] INFO ----- Sample prediction -----
zwt-MS-7D86 root[86735] INFO Predict: 1628 1629 1983 2449 25000 2720 2724 2762 2859 4019 412 41401 42731 4280 486 49121 49322 496 5070 51881 51884 51919 53081 5849 5990 E9320 V1011 V103 V1582 V4582 V462 V5865
zwt-MS-7D86 root[86735] INFO Target : 1977 1985 25000 2720 37900 4019 42731 49121 51881 5990 V1011 V103
zwt-MS-7D86 root[86735] INFO -----------------------------
zwt-MS-7D86 root[86735] INFO ========================================
zwt-MS-7D86 root[86735] INFO Finished Downstream Training
zwt-MS-7D86 root[86735] INFO Val F1      : 0.235
zwt-MS-7D86 root[86735] INFO Val F1 (inflated): 0.238
zwt-MS-7D86 root[86735] INFO Val AuPRC   : 0.961
zwt-MS-7D86 root[86735] INFO Val Thresh  : 0.13
zwt-MS-7D86 root[86735] INFO Val R@20    : 0.38
zwt-MS-7D86 root[86735] INFO Val R@40    : 0.50
zwt-MS-7D86 root[86735] INFO 
zwt-MS-7D86 root[86735] INFO Test F1      : 0.221
zwt-MS-7D86 root[86735] INFO Test F1 (inflated): 0.232
zwt-MS-7D86 root[86735] INFO Test AuPRC   : 0.961
zwt-MS-7D86 root[86735] INFO Test Thresh  : 0.10
zwt-MS-7D86 root[86735] INFO Test R@20    : 0.39
zwt-MS-7D86 root[86735] INFO Test R@40    : 0.51
zwt-MS-7D86 root[86735] INFO ========================================

as well as my zwt-MS-...
ChatGPT said:

Analysis errored
Always show details

import re

# Simulated large log string (truncated for demonstration)
with open("/mnt/data/full_log.txt", "r") as f:
    raw_log = f.read()


You said:
training-50k-entries.txt
Document
Use the following file 
ChatGPT said:
Thanks for sharing the training log. Here's a full summary of the training process based on your training-50k-entries.txt file:

✅ Environment Setup & Pretraining
Run command: Executed with extensive flags for UMLS-based joint concept-aware pretraining and downstream disease prediction (--cgl_task diagnosis).

GPU: NVIDIA RTX 4090, 23.5 GB memory.

Data:

Diagnosis table shape: (651047, 5), processed to (650940, 3).

Prescription table (PRESCRIPTIONS.csv): (4156450, 19) → ATC-mapped: (1988696, 5).

UMLS Graph: 23,864 CUIs, 99,064 edges.

Notes concepts from noteevents_test_run_jaccard_0.9_w6.pkl: 774 patients, max length 1279.

Graph & Model Configuration:

GNN Layer: SAGEConv, 1 layer, 128-dim.

Text encoder: Transformer (1 head, 1 block).

Loss: BCEWithLogitsLoss, masked aggregation loss α = 0.25.

Pretraining Stats:

Epochs: Ran for 38/80 (early stopped).

Best model checkpoint: epoch=31-step=375680.ckpt.

Final Val metrics:

F1: 0.499

Jaccard: 0.391

AuPRC: 0.606

Pretrain task performance by head:

d2d: F1 0.421, AuPRC 0.538

p2d: F1 0.123, AuPRC 0.266

p2p: F1 0.871, AuPRC 0.933

d2p: F1 0.583, AuPRC 0.688

🔽 Downstream Fine-Tuning (CGL Diagnosis Task)
Context-Guided Learning (CGL):

Context vectors: 4

Target classes: 4825 ICD codes

Learning rate: 1e-4

Batch size: 2, grad accumulation = 2 (⇒ effective 4).

Training Stats:

Epochs: 35 (early stopped)

Best checkpoint: epoch=25-step=40014.ckpt

Validation Results:

F1: 0.235

AuROC: 0.961

Optimal Threshold: 0.13

Recall@20: 0.38

Recall@40: 0.50

Test Results:

F1: 0.221

F1 (inflated): 0.232

AuROC: 0.961

Threshold: 0.10

Recall@20: 0.39

Recall@40: 0.51

📌 Sample Predictions
Validation:

Predict: 25000 2859 4019 41401 42731 ...

Target: 135 2749 27652 2851 4241 ...

Test:

Predict: 1628 1629 1983 2449 25000 ...

Target: 1977 1985 25000 2720 37900 ...

Would you like help interpreting the quality of these results or improving them?

You said:
Just send a file that is trimmed
ChatGPT said:

Analyzed
Always show details

from pathlib import Path
import re

# Load the original file
input_path = Path("/mnt/data/training-50k-entries.txt")
output_path = Path("/mnt/data/training-log-anonymized.txt")

# Regex to remove timestamps and hostnames like 'zwt-MS-7D86 root[86735]'
timestamp_host_pattern = re.compile(r"^\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2} [\w\-]+ root\[\d+\] ")

# Also clean inline timestamps and hostname references
inline_timestamp_pattern = re.compile(r"\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}")
hostname_pattern = re.compile(r"zwt-MS-\w+")

# Read and clean each line
cleaned_lines = []
with input_path.open() as f:
    for line in f:
        line = timestamp_host_pattern.sub("", line)
        line = inline_timestamp_pattern.sub("", line)
        line = hostname_pattern.sub("", line)
        cleaned_lines.append(line)

# Write to the new file
output_path.write_text("".join(cleaned_lines))
output_path
Result
PosixPath('/mnt/data/training-log-anonymized.txt')
Here is the trimmed version of your log file with timestamps and hostnames removed:

📄 Download training-log-anonymized.txt 

You said:
training_and_eval_log.txt

ChatGPT said:

Analysis errored
Always show details

from pathlib import Path
import re

# Define paths
input_path = "/mnt/data/training_and_eval_log.txt"
output_path = "/mnt/data/training_and_eval_log_trimmed.txt"

# Read the original file
with open(input_path, "r", encoding="utf-8") as file:
    lines = file.readlines()

# Regex pattern to match date-time prefixes (e.g., zwt-MS-7D86 root[83533])
pattern = re.compile(r"^\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2} [\w\-]+ root\[\d+\] ")

# Remove matched patterns
trimmed_lines = [pattern.sub("", line) for line in lines]

# Save trimmed output
with open(output_path, "w", encoding="utf-8") as file:
    file.writelines(trimmed_lines)

output_path

Analyzed
Always show details

from pathlib import Path
import re

# Re-define file paths after environment reset
input_path = "/mnt/data/training_and_eval_log.txt"
output_path = "/mnt/data/training_and_eval_log_trimmed.txt"

# Read the original file
with open(input_path, "r", encoding="utf-8") as file:
    lines = file.readlines()

# Regex to match and remove datetime and host info prefix
pattern = re.compile(r"^\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2} [\w\-]+ root\[\d+\] ")

# Apply pattern and trim lines
trimmed_lines = [pattern.sub("", line) for line in lines]

# Write the trimmed content to a new file
with open(output_path, "w", encoding="utf-8") as file:
    file.writelines(trimmed_lines)

output_path
Result
'/mnt/data/training_and_eval_log_trimmed.txt'
The trimmed file with all dates, times, and hostnames removed is ready:

👉 Download trimmed log file 

You said:
Can you write a brief description and any relevant equations to explain focal loss and why it might be good for this project.
ChatGPT said:

Thought
ChatGPT said:
\section*{Focal Loss}

\paragraph{Background.}
In heavily \emph{imbalanced} classification settings—where the number of negative examples vastly exceeds the positives—standard cross‑entropy places most gradient weight on easy, majority‑class samples.  Learning then stagnates on the hard, minority‑class cases that matter most.  
Focal Loss (FL), introduced by \citet{lin2017focal}, down‑weights easy examples so the model focuses its capacity on the hard ones.

\paragraph{Binary focal loss.}
Let $y\in\{0,1\}$ be the ground‑truth label and $\hat{p}\in(0,1)$ the model’s raw probability for the positive class ($\hat{p}=\sigma(z)$ after a sigmoid).  
The cross‑entropy (CE) loss is
\[
\operatorname{CE}(y,\hat{p}) \;=\; -\,y\log\hat{p}\;-\;(1-y)\log(1-\hat{p}).
\]

Focal loss multiplies the CE term by a modulating factor that decays to \emph{zero} as the sample becomes easier:

\[
\boxed{\;
\operatorname{FL}(y,\hat{p};\alpha,\gamma)
\;=\;
-\,\alpha\,y\,(1-\hat{p})^{\gamma}\log\hat{p}
\;-\;
(1-\alpha)\,(1-y)\,\hat{p}^{\gamma}\log(1-\hat{p})
\;}
\]

\begin{itemize}
    \item $\gamma\ge 0$ (``focusing'' parameter) reduces the loss contribution from well‑classified examples; a higher $\gamma$ focuses more on hard cases.
    \item $\alpha\in[0,1]$ (optional balancing term) compensates for class‑prior skew, assigning larger weight to the minority class ($\alpha$ is typically set to its prior inverse).
\end{itemize}

For \emph{multi‑label} problems with $C$ independent logits, FL is summed over classes:
\[
\mathcal{L}_{\text{FL}} \;=\; \frac{1}{C}\sum_{c=1}^{C} \operatorname{FL}\bigl(y_{c},\hat{p}_{c};\alpha_{c},\gamma\bigr).
\]

\paragraph{Why it fits this project.}
Your diagnosis–prediction pipeline must label thousands of ICD codes from highly sparse labels:

\begin{enumerate}
    \item \textbf{Extreme label imbalance.}  Many disease codes appear in $<1\%$ of admissions.  FL explicitly counteracts this by diminishing gradients from overwhelming negative/majority instances.
    \item \textbf{Hard‑example emphasis.}  Clinically valuable errors are often the minority, ambiguous diagnoses.  The $(1-\hat{p})^{\gamma}$ term drives the optimiser to keep updating hard, informative samples.
    \item \textbf{Plug‑and‑play.}  FL is a drop‑in replacement for BCEWithLogitsLoss already used in your PyTorch Lightning setup; only two scalar hyper‑parameters need tuning.
\end{enumerate}

Empirically, substituting $\mathcal{L}_{\text{BCE}}\!\to\!\mathcal{L}_{\text{FL}}$ with $\gamma\in[1,3]$ and class‑level $\alpha$ (e.g.\ inverse frequency) often yields sizable gains in F1 and AUCPR on sparse, multi‑label medical datasets.

\bibliographystyle{unsrtnat}
\bibliography{references}
You said:
Can you provide a brief description of what each of these metrics mean
F1 
F1 Inflated 
AuRPC
Thresh
R@20
R@40
Give me an idea of what the full metric name is, what it measures, and how I should interpet it.

ChatGPT said:
Metric cheat‑sheet

Metric	What it measures	How to read it
Jaccard (a.k.a. Intersection‑over‑Union)	$$\text{Jaccard}=\frac{	Y_\text{true}\cap Y_\text{pred}
F1 score	Harmonic mean of Precision and Recall: 
F
1
=
2
  
P
R
P
+
R
F1= 
P+R
2PR
​
 . It balances false positives and false negatives. In multi‑label work this is often the micro F1 (computed on all label–instance pairs).	Peaks when the classifier has a good balance of precision and recall.
AuPRC (Area under the Precision–Recall Curve)	Integrates precision as recall sweeps from 0 → 1 across all threshold values. Robust to class imbalance; reflects ranking quality of the probabilities.	Values range 0–1; random ≈ prevalence of positives, perfect = 1.
F1 Inflated	Same formula as F1 but computed after forcing at least one positive prediction per record (e.g.\ adding the top‑ranked code if the model’s thresholding yielded none). This prevents degenerate “predict nothing” behaviour on sparse data and usually inflates the score slightly.	Higher than plain F1; shows performance assuming you always return ≥1 code.
AuRPC	Typo in many logs—should be AuPRC (same as above).	
Thresh	The probability cut‑off that maximises the chosen validation metric (often F1 or AuPRC). At inference, scores ≥ Thresh are converted to 1, below to 0.	A lower threshold means the model is more willing to emit positives.
R@20 / R@40 (Recall‑at‑k)	Fraction of true diagnosis codes that appear in the model’s top‑k highest‑scoring predictions: $$R@k=\frac{	Y_\text{true}\cap \text{Top}_k
Why multiple metrics?

Set‑based (Jaccard) rewards exact list overlap.

Threshold‑dependent (F1, Thresh, F1 Inflated) evaluate a concrete binary output.

Ranking‑based (AuPRC, R@k) assess the quality of the probability ordering independent of a fixed cut‑off—crucial for imbalanced, many‑label scenarios like ICD code prediction.

You said:
Just in bullet points. No need for table
ChatGPT said:
Jaccard (Intersection‑over‑Union) – overlap between predicted and true label sets: 
∣
Y
pred
∩
Y
true
∣
/
∣
Y
pred
∪
Y
true
∣
∣Y 
pred
​
 ∩Y 
true
​
 ∣/∣Y 
pred
​
 ∪Y 
true
​
 ∣.

F1 score – harmonic mean of precision and recall; balances false positives and false negatives.

AuPRC (Area under Precision–Recall Curve) – integrates precision across all recall levels; gauges quality of the probability ranking, robust to class imbalance.

F1 Inflated – F1 computed after forcing at least one positive prediction per record (adds the top‑ranked label if none passed the threshold), so the score is “inflated” relative to plain F1.

AuRPC – usually a misspelling of AuPRC; if reported, interpret as AuPRC.

Thresh – probability cut‑off chosen on the validation set (often the value that maximises F1 or AuPRC) for turning probabilities into binary predictions.

R@20 (Recall‑at‑20) – proportion of true labels that appear within the model’s 20 highest‑scored predictions.

R@40 (Recall‑at‑40) – same as above but for the top 40 predictions; will always be ≥ R@20.

You said:
I guess can you explain intuitively what each measures, briefly
ChatGPT said:
Jaccard (overlap) – “How much do my predicted labels and the true labels overlap, relative to everything either side mentioned?”

F1 score – “How well am I balancing catching the right labels (recall) without sprinkling in too many wrong ones (precision)?”

AuPRC (area under the precision‑recall curve) – “Across all decision thresholds, how good is my ranking of true labels above false ones, especially when positives are rare?”

F1 Inflated – “Same F1, but I make sure every patient gets at least one predicted label, so scores look a bit rosier when I would otherwise output nothing.”

Thresh – “The probability cut‑off I chose (from validation) to decide when a label is ‘on’ or ‘off’.”

R@20 (recall‑at‑20) – “If I hand you my top 20 guesses, what fraction of the real labels did I capture?”

R@40 (recall‑at‑40) – “Same idea, but now you get my top 40 guesses—so recall should be higher than at 20.”

You said:
Ok that seems intuitive. Can you combine with the slight formal definitions and an understanding of whether higher is better or worse.
ChatGPT said:
Jaccard index (a .k.a. Intersection‑over‑Union)

Formal: J = | ŷ ∩ y | / | ŷ ∪ y |, comparing the set of predicted labels ŷ to the gold labels y for a patient (and usually averaged).

Interpretation: Fraction of total mentioned codes that are correct.

Higher = better (1 = perfect overlap, 0 = none).

F1 score

Formal: F1 = 2·(Precision·Recall)/(Precision + Recall).

Interpretation: Balances “how many predicted codes were right” (precision) with “how many true codes were found” (recall).

Higher = better (1 = perfect, 0 = worst).

AuPRC (Area under the Precision‑Recall Curve)

Formal: Integral of precision as a function of recall when sweeping the decision threshold from 1 → 0.

Interpretation: Measures the model’s ability to rank true codes above false ones across all thresholds—robust to class imbalance.

Higher = better (1 = ideal ranking, baseline ≈ prevalence of positives).

F1 Inflated

Formal: Same F1 formula but after forcing the model to output at least one code per patient (common trick to avoid undefined F1 when there are zero positives).

Interpretation: Shows best‑case F1 if the system never “opts out.” Values are typically higher than raw F1.

Higher = better (compared within methods that use the same inflation rule).

Thresh (optimal decision threshold)

Formal: Probability cut‑off chosen on the validation set (e.g., maximizing F1 or AuPRC).

Interpretation: Below this, a predicted probability is turned “off”; above, it becomes a label.

Higher or lower isn’t intrinsically better—it just reflects where the trade‑off is made.

Recall‑at‑k (R@20, R@40)

Formal: R@k = | (top‑k predictions ∩ y) | / | y |.

Interpretation: “If clinicians only look at my first k suggested codes, what fraction of the true codes would they see?”

Higher = better; R@40 ≥ R@20 by definition.


You said:
zwt-MS-7D86 root[98760] INFO ========================================
zwt-MS-7D86 root[98760] INFO [TRAIN] early stopping on val/loss_total, patience 3
$$ UTILIZING FOCAL LOSS $$
zwt-MS-7D86 root[98760] INFO [TRAINER] Sampling 4 triplets per anchor id
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]

  | Name           | Type                            | Params
-------------------------------------------------------------------
0 | pretrain_model | JointCodePretrainingTransformer | 20.3 M
1 | loss           | FocalLoss                       | 0     
-------------------------------------------------------------------
1.7 M     Trainable params
18.5 M    Non-trainable params
20.3 M    Total params
81.085    Total estimated model params size (MB)
Sanity Checking: 0it [00:00, ?it/s]/home/zwt/miniconda3/envs/mmugl/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the num_workers argument (try 32 which is the number of cpus on this machine) in the DataLoader init to improve performance.
  rank_zero_warn(
/home/zwt/miniconda3/envs/mmugl/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the num_workers argument (try 32 which is the number of cpus on this machine) in the DataLoader init to improve performance.
  rank_zero_warn(
Epoch 0: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 11740/11740 [01:28<00:00, 131.93it/s, loss=0.0145, v_num=11, train/batch_loss=0.0102, train/loss=0.0168]
zwt-MS-7D86 root[98760] INFO Load best validation loss model: 
Traceback (most recent call last):
  File "dev/run_training_concepts_focal.py", line 870, in <module>
    main()
  File "dev/run_training_concepts_focal.py", line 579, in main
    best_model_state_dict = torch.load(checkpointing.best_model_path)["state_dict"]
  File "/home/zwt/miniconda3/envs/mmugl/lib/python3.8/site-packages/torch/serialization.py", line 594, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/home/zwt/miniconda3/envs/mmugl/lib/python3.8/site-packages/torch/serialization.py", line 230, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/zwt/miniconda3/envs/mmugl/lib/python3.8/site-packages/torch/serialization.py", line 211, in __init__
    super(_open_file, self).__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: ''
ChatGPT said:
The crash happens because checkpointing.best_model_path is an empty string, so torch.load('') can’t open anything and raises FileNotFoundError.

Why did best_model_path stay empty?
No checkpoint was ever written.
A ModelCheckpoint callback only fills best_model_path after it has saved at least one file whose monitored metric meets the saving criteria. If nothing was saved, the field is empty.

Common reasons nothing is saved

You’re monitoring the wrong metric name (e.g. "val/loss_total" vs "val_loss_total").

save_top_k is 0 or the metric is NaN/Inf, so every candidate is rejected.

The validation loop never ran (e.g. limit_val_batches=0, val_dataloader missing, etc.).

Training ended before the first validation checkpoint (e.g. very low max_epochs, or an early failure inside the loop).

How to fix
Verify the ModelCheckpoint configuration

checkpoint_cb = pl.callbacks.ModelCheckpoint(
    monitor="val/loss_total",   # exact key in validation logs
    mode="min",                 # "min" for losses, "max" for scores
    save_top_k=1,               # keep best model
    filename="{epoch}-{step}-{val_loss_total:.4f}",
)
Make sure validation really runs

Print a line in validation_step or set logger=True on the metric to confirm you see it in the progress bar.

Check that val_dataloader is returned and not empty.

Add a fallback so the code never tries to load an empty path

if checkpointing.best_model_path:
    best_state = torch.load(checkpointing.best_model_path)["state_dict"]
else:
    print("⚠️  No checkpoint saved – loading last model weights instead.")
    best_state = model.state_dict()          # or skip loading altogether
Optionally save the last epoch no matter what

checkpoint_cb = ModelCheckpoint(..., save_last=True)
After one successful validation epoch with a correctly monitored metric, best_model_path will hold the path to a .ckpt file and the torch.load call will succeed.

You said:
(mmugl) zwt@zwt-MS-7D86:~/CS598-DL4H-Final-Project/mmugl-main$ python dev/run_training_concepts_focal.py   --name diagnosis_test   --logdir dev/runs   --data_dir data   --code_mappings data/code_mappings   --epochs 1   --num_workers 2   --grad_acc 1   --batch_size 4   --down_batch_size 2   --down_grad_acc 2   --down_learning_rate 0.0001   --down_es 5   --embedding_dim 128   --graph_num_layers 1   --graph_num_filters 1   --gnn_operator SAGEConv   --attention_heads 1   --feedforward_dim 128   --num_blocks 1   --agg_mask_loss 0.25   --mlp_dim 64   --mlp_num_layers 1   --freeze_embeddings   --store_pretrainmodel   --store_downmodel   --pretrain   --pretrain_es loss_total   --pretrain_patience 3   --split_mode cgl   --icd_codes data/disease_codes_all.txt   --down_icd_codes data/cgl_mimic3_disease_codes.txt   --atc_codes data/med_codes_all.txt   --diagnosis_csv ../MIMIC-III/DIAGNOSES_ICD.csv   --prescription_csv ../MIMIC-III/PRESCRIPTIONS.csv   --admission_csv ../MIMIC-III/ADMISSIONS.csv   --umls_graph data/umls_graph.pkl   --notes_concepts data/noteevents_test_run_jaccard_0.9_w6.pkl   --max_visit_text_length 1279   --downstream_cgl   --cgl_num_contexts 4   --cgl_task diagnosis   --down_mlp_dim 0   --down_mlp_num_layers 0   --random_state 1111   --verbose --store_graphmodel --loss focal
ChatGPT said:

Thought for 10 seconds
Running with --loss focal and only one epoch will work as long as a checkpoint actually gets written during that single validation pass.
Here’s what to double‑check so you don’t hit the empty‑path error again:

Checkpoint callback

checkpoint_cb = pl.callbacks.ModelCheckpoint(
    monitor="val/loss_total",   # EXACT key produced in validation_step
    mode="min",
    save_top_k=1,               # keep best
    save_last=True,             # always writes 'last.ckpt'
    filename="{epoch}-{step}-{val_loss_total:.4f}",
)
Even with one epoch, save_last=True guarantees a file is written.

Validation metric name — confirm the progress bar shows something like
val/loss_total=… (spelled exactly the same).

Load step

if checkpointing.best_model_path:
    state_dict = torch.load(checkpointing.best_model_path)["state_dict"]
else:                           # safety net
    print("⚠️  No best checkpoint – loading last weights.")
    state_dict = torch.load(os.path.join(checkpoint_dir, "last.ckpt"))["state_dict"]
If those pieces are in place, the run you just launched should finish the single epoch, write last.ckpt (and a “best” file too, given one validation), and the torch.load call will succeed when it tries to reload the best model.

You said:
zwt-MS-7D86 root[99598] INFO 
zwt-MS-7D86 root[99598] INFO ========================================
zwt-MS-7D86 root[99598] INFO Performing downstream DISEASE training
zwt-MS-7D86 root[99598] INFO Task: diagnosis
zwt-MS-7D86 root[99598] INFO With text: True
zwt-MS-7D86 root[99598] INFO eICU: False
zwt-MS-7D86 root[99598] INFO ========================================
zwt-MS-7D86 root[99598] INFO [CGL] Parsed 4825 diagnosis codes from provided file
zwt-MS-7D86 root[99598] INFO [DATA]----- TRAIN -----
zwt-MS-7D86 root[99598] WARNING [DATASET] keeping no med patients: True
zwt-MS-7D86 root[99598] INFO [ICD] Processing diagnosis table
zwt-MS-7D86 root[99598] INFO [ICD] Table raw shape: (651047, 5)
zwt-MS-7D86 root[99598] INFO [ICD] Table processed shape: (650940, 3)
Traceback (most recent call last):
  File "/home/zwt/miniconda3/envs/mmugl/lib/python3.8/site-packages/pandas/core/indexes/base.py", line 3629, in get_loc
    return self._engine.get_loc(casted_key)
  File "pandas/_libs/index.pyx", line 136, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/index.pyx", line 163, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 5198, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 5206, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: None

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "dev/run_training_concepts_focal.py", line 870, in <module>
    main()
  File "dev/run_training_concepts_focal.py", line 750, in main
    downstream_cgl_acc_val, downstream_cgl_acc = train_cgl_downstream(
  File "/home/zwt/CS598-DL4H-Final-Project/mmugl-main/kg/kg/training/cgl.py", line 107, in train_cgl_downstream
    train_dataset = CglDatasetDownstream(
  File "/home/zwt/CS598-DL4H-Final-Project/mmugl-main/kg/kg/data/datasets.py", line 842, in __init__
    super().__init__(
  File "/home/zwt/CS598-DL4H-Final-Project/mmugl-main/kg/kg/data/datasets.py", line 358, in __init__
    self.diagnosis_pd = self.diagnosis_pd[self.diagnosis_pd.apply(filter_lambda, axis=1)]
  File "/home/zwt/miniconda3/envs/mmugl/lib/python3.8/site-packages/pandas/core/frame.py", line 8848, in apply
    return op.apply().__finalize__(self, method="apply")
  File "/home/zwt/miniconda3/envs/mmugl/lib/python3.8/site-packages/pandas/core/apply.py", line 733, in apply
    return self.apply_standard()
  File "/home/zwt/miniconda3/envs/mmugl/lib/python3.8/site-packages/pandas/core/apply.py", line 857, in apply_standard
    results, res_index = self.apply_series_generator()
  File "/home/zwt/miniconda3/envs/mmugl/lib/python3.8/site-packages/pandas/core/apply.py", line 873, in apply_series_generator
    results[i] = self.f(v)
  File "/home/zwt/CS598-DL4H-Final-Project/mmugl-main/kg/kg/data/datasets.py", line 357, in <lambda>
    filter_lambda = lambda x: x["SUBJECT_ID"] in patient_ids
  File "/home/zwt/miniconda3/envs/mmugl/lib/python3.8/site-packages/pandas/core/series.py", line 958, in __getitem__
    return self._get_value(key)
  File "/home/zwt/miniconda3/envs/mmugl/lib/python3.8/site-packages/pandas/core/series.py", line 1069, in _get_value
    loc = self.index.get_loc(label)
  File "/home/zwt/miniconda3/envs/mmugl/lib/python3.8/site-packages/pandas/core/indexes/base.py", line 3631, in get_loc
    raise KeyError(key) from err
KeyError: None
ChatGPT said:

Thought for 39 seconds
KeyError: None in a row‑wise .apply() almost always means the column you’re trying to read isn’t in the DataFrame at that moment.
In the downstream‐CGL loader the lambda expects a field called SUBJECT_ID, but after the earlier “group / merge / filter” pipeline that column is no longer present (or has become an index).

Why it happens
groupby(...).agg(...) or groupby(...).apply(...) returns a result whose grouping keys become the index unless you call .reset_index().

Later, when you pipe that result into the dataset constructor, the code still tries to access row["SUBJECT_ID"]; because the label is not a column any more, pandas looks for the key, gets None, and raises KeyError: None.

Quick ways to verify
print(diagnosis_pd.columns)        # should list 'SUBJECT_ID'
print(diagnosis_pd.head())
If you see SUBJECT_ID listed under the Index (left‑most bold column) rather than the header line, it became the index.

Two easy fixes
After every groupby that should keep those keys as columns, add reset_index():

diagnosis_pd = (
    diagnosis_pd
    .groupby(["SUBJECT_ID", "HADM_ID"], as_index=False)   # or .reset_index() later
    .agg({"ICD9_CODE": list})
)
Or, right before you build the dataset, force the keys back into columns:

if diagnosis_pd.index.name == "SUBJECT_ID" or "SUBJECT_ID" not in diagnosis_pd.columns:
    diagnosis_pd = diagnosis_pd.reset_index()

required = {"SUBJECT_ID", "HADM_ID", "ICD9_CODE"}
missing  = required.difference(diagnosis_pd.columns)
if missing:
    raise ValueError(f"Missing column(s) in diagnosis_pd: {missing}")
Bottom line
Bring SUBJECT_ID (and any other keys you still reference later) back as actual columns before you hand the frame to CglDatasetDownstream.
Once that column is visible again the lambda x["SUBJECT_ID"] in patient_ids will work and the pipeline can proceed to training with Focal Loss.

You said:
This is only happening in the script where I have the new focal loss. the original seems to be working
ChatGPT said:

Thought for 17 seconds
Because the only thing you changed is the script, the failure must come from something that the
“focal” version does before the call to CglDatasetDownstream.
The loader that crashes is identical in both runs, so the difference has to be in the
DataFrame that reaches it.

What the traceback tells us
The error happens inside

filter_lambda = lambda x: x["SUBJECT_ID"] in patient_ids
self.diagnosis_pd = self.diagnosis_pd[self.diagnosis_pd.apply(filter_lambda, axis=1)]
KeyError: None means x["SUBJECT_ID"] fails because the row (a Series)
has no label "SUBJECT_ID".

So, in the failing run SUBJECT_ID is no longer a column of
self.diagnosis_pd by the time that lambda executes.

Why it works in the original script
In the working script the same frame does contain that column, so the lambda
runs fine.
The only way the column can disappear is if, somewhere in the “focal” script,
you:

perform a groupby(...).agg(...) without as_index=False or
reset_index(), turning SUBJECT_ID into the index, or

drop / rename the column (e.g. by selecting a subset of columns) after
the equivalent point in the original pipeline.

How to confirm quickly
Add these two lines right before the failing constructor call
(CglDatasetDownstream) in the focal script:

print("↪ diagnosis_pd columns:", diagnosis_pd.columns.tolist())
print("↪ diagnosis_pd index  :", diagnosis_pd.index.names)
You will almost certainly see that "SUBJECT_ID" is not in
diagnosis_pd.columns but is listed in diagnosis_pd.index
(or is missing altogether).

Run the same print‑out in the original script—you should see the
column present there.

Two safe fixes
Force the keys back into columns before the dataset is built

if "SUBJECT_ID" not in diagnosis_pd.columns:
    diagnosis_pd = diagnosis_pd.reset_index()

required = {"SUBJECT_ID", "HADM_ID", "ICD9_CODE"}
missing  = required.difference(diagnosis_pd.columns)
if missing:
    raise ValueError(f"Missing column(s): {missing}")
Put this right before the CglDatasetDownstream call.
It guarantees the column is where the lambda expects it, no matter what you
did earlier.

Audit the place where you changed the pipeline

Look for a new or modified line that does something like

diagnosis_pd = diagnosis_pd.groupby([...]).agg(...  # as_index=True by default
and change it to

diagnosis_pd = (
    diagnosis_pd
    .groupby([...], as_index=False)  # <- KEEP keys as columns
    .agg(...)
)
or just append .reset_index().

Why Focal Loss itself isn’t the culprit
The only new thing you intended to add was a different loss function, but
focal loss does not touch the data pipeline.
The column drop must have happened because of an unrelated edit you made
while creating the new script (e.g. copy‑pasting and trimming code).

Next steps
Add the quick diagnostic prints: confirm the column really is gone.

Either patch the pipeline with reset_index() as shown, or
restore the column where it got lost.

Re‑run the focal‑loss script; it should move past this error and start training.

Let me know if you need help finding the exact line that strips the column, or
if another error pops up after this fix!

You said:
Can you help me consider any extensions that I can make for this paper? Some possible extensions may be new loss functions, new data to integrate into the model, other novel ideas that would improve the usefulness of the research.
ChatGPT said:
Here are several directions you might explore to extend your work, grouped by theme:

1. Alternative or Complementary Losses
Hierarchical Focal Loss
Extend focal loss to down-weight easy examples and respect the ICD code hierarchy (e.g. penalize mistakes between distant branches more).

Contrastive / Triplet Loss
Encourage embeddings of similar diagnoses (co‐occurring or semantically related) to be closer than unrelated ones.

Label-Smoothing / Confidence-Penalized Loss
Soften the hard one-hot targets to help the model generalize to rare codes.

Multi-Task Loss
Jointly predict diagnosis and comorbidity or procedure codes, with a weighted sum of per-task losses.

2. Richer Input Modalities
Laboratory & Vital Signs
Integrate temporally‐aligned lab results (LOINC codes) and vital measurements as additional node or feature channels.

Unstructured Text Beyond CUI’s
Fine-tune a clinical transformer (e.g. ClinicalBERT) on your notes and feed its outputs alongside your concept embeddings.

Medication Dosages & Timing
Encode dosage amount and administration timestamps to capture treatment intensity and trends.

Imaging or Waveform Data
If available, incorporate embeddings from chest X-rays or ECG signals into your patient graph.

3. Graph & Architecture Innovations
Relational GNN Layers
Use R-GCN or HGT to model different UMLS relation types explicitly (e.g. “treats,” “causes”).

Temporal Graph Networks
Treat each visit as a graph snapshot and apply a Temporal GNN (TGAT, EvolveGCN) to capture progression.

Graph Autoencoder Pretraining
Pretrain on link‐prediction within UMLS (or a larger biomedical KG) before fine‐tuning on your task.

Attention over Subgraphs
Learn to weight different semantic neighborhoods (e.g. Drug–Disease vs. Symptom–Disease paths) per patient.

4. Evaluation & Utility Enhancements
Explainability Module
Apply GNN explainer techniques to highlight which concepts or relations drove each prediction.

Calibration & Uncertainty Estimation
Use Bayesian layers or deep ensembles to produce confidence intervals—clinically valuable for risk stratification.

External Validation
Test on a different hospital’s MIMIC-IV or eICU dataset to assess generalization.

Active Learning Loop
Identify low-confidence or rare‐code cases to solicit expert annotations and iteratively improve performance.

5. Downstream & Clinical Integration
Multi-Outcome Prediction
Extend from diagnosis to predict length-of-stay, readmission risk, or mortality in a single unified model.

Treatment Recommendation
Couple your diagnosis embeddings with an outcome-driven policy network to suggest next best medication.

Population-Level Insights
Aggregate learned embeddings to cluster patients, identify subtypes, or discover novel disease cohorts.

Each of these could form a substantive extension—pick those that align best with your data availability and clinical goals!


What about others?




Search

Deep research

Create image



No file chosenNo file chosen
ChatGPT can make mistakes. Check important info.